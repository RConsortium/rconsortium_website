<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>R Consortium</title>
<link>https://r-consortium.org/blog/</link>
<atom:link href="https://r-consortium.org/blog/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.7.32</generator>
<lastBuildDate>Tue, 24 Jun 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>A Framework for Cohort Building in R - Nuria Mercade-Besora and Edward Burn</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/a-framework-for-cohort-building-in-r-nuria-mercade-besora-and-edward-burn/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/6-4diWLaE6w" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="cohortconstructor-simplifying-patient-cohort-management-with-r" class="level1">
<h1>CohortConstructor: Simplifying Patient Cohort Management with R</h1>
<p>In the ever-evolving field of healthcare data analysis, managing patient cohorts efficiently is crucial. The CohortConstructor package in R provides an innovative solution for building and managing patient cohorts using real-world health data mapped to the OMOP Common Data Model (CDM). Developed by Nuria Mercade-Besora and Edward Burn, this package streamlines the process, allowing users to apply both common and complex inclusion criteria, combine cohorts, update cohort entry and exit dates, and track groups of patients—all without the need to write complicated code.</p>
<section id="background-and-need-for-a-common-data-model" class="level2">
<h2 class="anchored" data-anchor-id="background-and-need-for-a-common-data-model">Background and Need for a Common Data Model</h2>
<p>In the realm of healthcare data, researchers often face the challenge of transforming disparate sources of data into reliable evidence. Traditional processes involve lengthy data transformations to make data research-ready, followed by the application of statistical methods. The introduction of a common data model, such as the OMOP CDM, simplifies this process by standardizing healthcare data. This allows researchers to focus on querying the standardized data to generate reliable evidence, without needing to handle the intricacies of data transformation themselves.</p>
<section id="key-benefits-of-the-omop-common-data-model" class="level3">
<h3 class="anchored" data-anchor-id="key-benefits-of-the-omop-common-data-model">Key Benefits of the OMOP Common Data Model:</h3>
<ul>
<li><strong>Standardization:</strong> Different source systems can be converted to the same common data model, enabling the use of consistent pipelines across various databases.</li>
<li><strong>Interoperability:</strong> The OMOP CDM allows for international network studies, where analytic code is distributed to data partners who run it locally, keeping patient data secure and aggregated results are shared.</li>
<li><strong>Reproducibility:</strong> The common vocabulary and standardized format ensure that studies are reproducible across different geographies and healthcare systems.</li>
</ul>
</section>
</section>
<section id="the-cohortconstructor-package" class="level2">
<h2 class="anchored" data-anchor-id="the-cohortconstructor-package">The CohortConstructor Package</h2>
<p>CohortConstructor is designed to make cohort work more transparent and reproducible. It offers a comprehensive set of tools for cohort curation, all within a tidyverse-style framework. The package does not require users to have expertise in the OMOP CDM, making it accessible to anyone working with healthcare data in R.</p>
<section id="core-features-of-cohortconstructor" class="level3">
<h3 class="anchored" data-anchor-id="core-features-of-cohortconstructor">Core Features of CohortConstructor:</h3>
<ol type="1">
<li><p><strong>Base Cohort Creation:</strong></p>
<ul>
<li><strong>Demographic Cohorts:</strong> Define cohorts based on age, sex, and observation periods.</li>
<li><strong>Concept and Measurement Cohorts:</strong> Use clinical concepts and measurements to define cohorts.</li>
<li><strong>Death Cohorts:</strong> Create cohorts based on recorded deaths in the database.</li>
</ul></li>
<li><p><strong>Cohort Curation Tools:</strong></p>
<ul>
<li>Apply inclusion criteria based on demographics and other factors.</li>
<li>Update cohort entry and exit dates using pre-defined functions.</li>
<li>Transform and combine cohorts, allowing for complex cohort constructions such as intersections and unions.</li>
</ul></li>
<li><p><strong>Reproducibility and Transparency:</strong></p>
<ul>
<li><strong>Cohort Settings:</strong> Associate each cohort with a name and variables for easy reference.</li>
<li><strong>Attrition Tracking:</strong> Document the inclusion criteria and the impact of each criterion on the cohort size.</li>
<li><strong>Cohort Code List:</strong> Maintain a record of the clinical codes used to define each cohort.</li>
</ul></li>
</ol>
</section>
</section>
<section id="practical-demonstration-and-use-cases" class="level2">
<h2 class="anchored" data-anchor-id="practical-demonstration-and-use-cases">Practical Demonstration and Use Cases</h2>
<p>The webinar, presented by Nuria Mercade-Besora and Edward Burn, provided practical examples of how the CohortConstructor package can be used. From creating base cohorts to applying complex inclusion criteria, the session demonstrated the package’s versatility in handling healthcare data.</p>
<section id="example-workflow" class="level3">
<h3 class="anchored" data-anchor-id="example-workflow">Example Workflow:</h3>
<ol type="1">
<li><strong>Base Cohort Creation:</strong> Using concepts and demographics to create initial cohorts.</li>
<li><strong>Applying Inclusion Criteria:</strong> Filtering cohorts based on required demographic and clinical criteria.</li>
<li><strong>Transforming and Combining Cohorts:</strong> Using functions to intersect and merge cohorts, creating complex patient groupings tailored to specific research questions.</li>
</ol>
</section>
</section>
<section id="additional-resources" class="level2">
<h2 class="anchored" data-anchor-id="additional-resources">Additional Resources</h2>
<p>For those interested in exploring the CohortConstructor package further, the full abstract, setup instructions, and demo slides are available on the <a href="https://github.com/OHDSI/CohortConstructor">GitHub page</a>. The package is also available on <a href="https://cran.r-project.org/web/packages/CohortConstructor/index.html">CRAN</a> for easy installation.</p>
<p>Ed Burn has also authored a book on programming with the OMOP Common Data Model in R, which is freely available online. This resource provides a deeper dive into working with databases in R, beyond just the OMOP CDM.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The CohortConstructor package represents a significant advancement in the management of patient cohorts using R. By simplifying the process and making it accessible to a wider audience, it empowers researchers to focus on deriving insights from healthcare data, rather than being bogged down by data management tasks. Whether you are a seasoned data scientist or a healthcare professional delving into data analysis, CohortConstructor offers the tools needed to streamline your workflow and enhance the reproducibility of your research.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Healthcare</category>
  <category>Epidemiology</category>
  <category>Software Development</category>
  <guid>https://r-consortium.org/posts/a-framework-for-cohort-building-in-r-nuria-mercade-besora-and-edward-burn/</guid>
  <pubDate>Tue, 24 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/a-framework-for-cohort-building-in-r-nuria-mercade-besora-and-edward-burn/thumbnail-cohortconstructor-burn-and-mercade-bosora.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>In the Nix of Time: Creating a reproducible analytical environment with Nix and {rix}</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/in-the-nix-of-time-creating-a-reproducible-analytical-environment-with-nix-and-rix/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/-NARVwViEwA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="turbocharging-shiny-app-development-with-nix-and-rix" class="level3">
<h3 class="anchored" data-anchor-id="turbocharging-shiny-app-development-with-nix-and-rix">Turbocharging Shiny App Development with Nix and {rix}</h3>
<p>In the realm of data science, particularly for medical research, the integration of open-source tools has dramatically accelerated the development and deployment of sophisticated analytical applications. Among these tools, R has established itself as a powerhouse for statistical computing and data visualization, especially with its ability to create interactive web applications using Shiny. However, as developers push the boundaries of what’s possible, ensuring reproducibility and handling complex dependencies become crucial challenges. This is where the Nix package manager and the {rix} R package, authored by Bruno Rodrigues and Philipp Baumann, come into play.</p>
<p>Eric Nantz, a seasoned statistician, developer, and the host of the R-Podcast, recently shared his insights at the R/Medicine 2025 conference on how these tools have revolutionized his workflow in developing robust production-quality Shiny applications. His demonstration highlighted the synergy between Nix and Shiny, presenting a compelling case for their combined use in data science projects.</p>
<section id="the-reproducibility-challenge" class="level4">
<h4 class="anchored" data-anchor-id="the-reproducibility-challenge">The Reproducibility Challenge</h4>
<p>Shiny applications, while powerful, often require a myriad of dependencies. These range from R packages like {reactable} for table visualizations to system-level dependencies and external services such as APIs. Traditionally, tools like {renv} have been used to manage R package dependencies, while Docker has been employed to handle system-level dependencies. However, this combination can sometimes fall short, particularly in complex environments or when deploying across different systems.</p>
</section>
<section id="enter-nix-and-rix" class="level4">
<h4 class="anchored" data-anchor-id="enter-nix-and-rix">Enter Nix and {rix}</h4>
<p>The Nix package manager offers a comprehensive solution by managing the full dependency stack of software projects. It provides a sandboxed environment where dependencies can be installed and managed without interfering with the host system. This is particularly advantageous for Shiny applications, which can have intricate dependencies spread across different languages and systems.</p>
<p>The {rix} package simplifies the integration of Nix with R projects. It allows developers to create project-specific sandboxes that include both R packages and system dependencies, all managed via Nix. This ensures that all team members can work in a consistent environment, reducing the “it works on my machine” syndrome.</p>
</section>
<section id="key-features-of-nix-and-rix" class="level4">
<h4 class="anchored" data-anchor-id="key-features-of-nix-and-rix">Key Features of Nix and {rix}</h4>
<ul>
<li><p><strong>Comprehensive Package Management</strong>: Nix manages over 120,000 software packages, ensuring that all dependencies, including system libraries, are automatically resolved and installed.</p></li>
<li><p><strong>Immutable and Reproducible Environments</strong>: By creating an immutable file system, Nix ensures that the development environment remains consistent, preventing accidental changes that can affect reproducibility.</p></li>
<li><p><strong>Cross-Platform Compatibility</strong>: Nix can be installed on Linux, macOS, and Windows (via Windows Subsystem for Linux), making it accessible to a wide range of users.</p></li>
<li><p><strong>Integration with {rix}</strong>: The {rix} package allows easy configuration of R-specific environments, including access to CRAN and Bioconductor packages, as well as GitHub-hosted packages.</p></li>
</ul>
</section>
<section id="shiny-and-nix-a-perfect-match" class="level4">
<h4 class="anchored" data-anchor-id="shiny-and-nix-a-perfect-match">Shiny and Nix: A Perfect Match</h4>
<p>Eric Nantz’s demonstration showcased a Shiny application developed using the Nix and {rix} workflow. The app, which explored data from the National Health and Nutrition Examination Survey, was developed using the Golem package to convert the Shiny app into a package format—a best practice that enhances maintainability and scalability.</p>
<p>One of the standout features of this workflow was the ability to run a Shiny application with all its dependencies managed by Nix, without R being installed on the host system. This was possible through the use of a Nix shell, which provided a sandboxed environment where R and its packages were available. This not only ensured a consistent development environment but also facilitated easy deployment via Docker for hosting on cloud platforms.</p>
</section>
<section id="overcoming-challenges" class="level4">
<h4 class="anchored" data-anchor-id="overcoming-challenges">Overcoming Challenges</h4>
<p>While the benefits of using Nix and {rix} are substantial, there are some challenges to be aware of. The learning curve associated with Nix’s domain-specific language can be steep, and managing storage space for the Nix store is necessary. Additionally, some packages that write to temporary directories may not play well with Nix’s immutable file system. However, the advantages of reproducibility and ease of deployment often outweigh these hurdles.</p>
</section>
<section id="conclusion" class="level4">
<h4 class="anchored" data-anchor-id="conclusion">Conclusion</h4>
<p>The integration of Nix and {rix} into Shiny app development represents a significant advancement in creating reproducible and scalable data science projects. By leveraging Nix’s powerful package management capabilities and {rix}’s seamless integration with R, developers can ensure their applications are robust, consistent, and easy to deploy across various environments.</p>
<p>Eric Nantz’s insights and demonstration provide a valuable resource for R developers looking to enhance their workflow with these cutting-edge tools. As the R community continues to innovate and evolve, the combination of Shiny, Nix, and {rix} is poised to play a pivotal role in shaping the future of data science applications.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Clinical Research</category>
  <category>Software Development</category>
  <guid>https://r-consortium.org/posts/in-the-nix-of-time-creating-a-reproducible-analytical-environment-with-nix-and-rix/</guid>
  <pubDate>Tue, 24 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/in-the-nix-of-time-creating-a-reproducible-analytical-environment-with-nix-and-rix/thumbnail-rix-and-nix-nantz.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>REDCapSync and RosyREDCap for standardized data pipelines and exploratory data analysis</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/redcapsync-and-rosyredcap-for-standardized-data-pipelines-and-exploratory-data-analysis/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/rkT6SParLO0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="enhancing-clinical-research-with-redcapsync-and-rosyredcap" class="level2">
<h2 class="anchored" data-anchor-id="enhancing-clinical-research-with-redcapsync-and-rosyredcap">Enhancing Clinical Research with REDCapSync and RosyREDCap</h2>
<p>In the realm of clinical research and trials, the synergy between R and REDCap is transforming data management and analysis. REDCapSync and RosyREDCap, two innovative R packages, are at the forefront of this transformation, offering a streamlined approach to data pipelines and exploratory data analysis using the REDCap API.</p>
<section id="understanding-redcapsync-rosyredcap" class="level3">
<h3 class="anchored" data-anchor-id="understanding-redcapsync-rosyredcap">Understanding REDCapSync &amp; RosyREDCap</h3>
<p><strong>REDCapSync</strong> is a robust R package designed to facilitate the synchronization of data from one or multiple REDCap projects. The package introduces several core functions, such as <code>setup_project</code> and <code>sync_project</code>, which simplify the process of metadata and data extraction. By leveraging a cache of the last project save, a designated file directory, and the REDCap log, REDCapSync efficiently updates only the data that has changed since the last API call. This functionality is crucial, especially for large datasets, ensuring that users do not repeatedly download the entire dataset, thereby saving time and server load.</p>
<p>Key Features of REDCapSync:</p>
<ul>
<li><strong>Efficient Data Updates:</strong> Utilizes REDCap logs to update only modified records, reducing server load and improving speed.</li>
<li><strong>Standardized R List Objects:</strong> Maintains data as R list objects for easy downstream processing.</li>
<li><strong>Experimental Functions:</strong> Includes features for adding derived variables, merging forms, and generating data subsets that refresh with <code>sync_project</code>.</li>
<li><strong>Upload Capabilities:</strong> Allows for uploading labeled data via the API, a feature not available through the REDCap website.</li>
</ul>
<p><strong>RosyREDCap</strong>, on the other hand, is an exploratory data analysis tool. It is a Shiny application designed to load previous projects set up with REDCapSync. Users can navigate between projects, anonymize data, and perform ad-hoc visualizations effortlessly.</p>
<p>Core Features of RosyREDCap:</p>
<ul>
<li><strong>Interactive Interface:</strong> Provides a user-friendly interface for toggling between projects, ideal for exploratory data analysis.</li>
<li><strong>Data De-identification:</strong> Ensures confidentiality by allowing data de-identification within the application.</li>
<li><strong>Visualization Tools:</strong> Facilitates data visualization through intuitive tools, enhancing data interpretation and presentation.</li>
</ul>
</section>
<section id="the-power-of-r-and-redcap-in-clinical-research" class="level3">
<h3 class="anchored" data-anchor-id="the-power-of-r-and-redcap-in-clinical-research">The Power of R and REDCap in Clinical Research</h3>
<p>The integration of R with REDCap through these packages offers multiple advantages for researchers:</p>
<ul>
<li><strong>Reproducibility and Efficiency:</strong> By maintaining a standardized data structure and using efficient synchronization methods, researchers can ensure reproducibility and save time.</li>
<li><strong>Enhanced Data Analysis:</strong> With functions tailored for clinical data, users can perform complex analyses and visualizations, leading to more insightful research outcomes.</li>
<li><strong>Accessibility for New Users:</strong> Even those new to R can leverage the power of REDCapSync and RosyREDCap without needing in-depth knowledge of the REDCap API.</li>
</ul>
</section>
<section id="real-world-applications-and-future-directions" class="level3">
<h3 class="anchored" data-anchor-id="real-world-applications-and-future-directions">Real-World Applications and Future Directions</h3>
<p>Dr.&nbsp;Brandon Rose, developer of these packages, brings extensive experience in hematology, oncology, and clinical informatics. His work on the Fire Fighter Cancer Cohort Study and UK BioBank research underscores the practical applications of these tools in real-world scenarios.</p>
<p>At the R Medicine conference, Dr.&nbsp;Rose plans to showcase various examples and use cases of REDCapSync and RosyREDCap. These demonstrations will highlight how the combined strengths of R and REDCap can streamline clinical data pipelines, ultimately enhancing research quality and patient care.</p>
<p>For those interested in the development and application of these packages, Dr.&nbsp;Rose offers support and collaboration opportunities. By generalizing code and sharing tools, researchers can contribute to a community of practice that enhances data management and analysis across various domains.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>REDCapSync and RosyREDCap represent significant strides in the integration of R and REDCap, providing powerful tools for clinical data management and analysis. These packages not only simplify the technical aspects of data synchronization and analysis but also empower researchers to focus on the insights and outcomes of their work. As these tools continue to evolve, they hold the promise of further transforming the landscape of clinical research and healthcare data management.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Clinical Research</category>
  <category>Software Development</category>
  <category>Healthcare</category>
  <guid>https://r-consortium.org/posts/redcapsync-and-rosyredcap-for-standardized-data-pipelines-and-exploratory-data-analysis/</guid>
  <pubDate>Tue, 24 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/redcapsync-and-rosyredcap-for-standardized-data-pipelines-and-exploratory-data-analysis/thumbnail-redcap-rose.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>The power of {targets} package for reproducible data science</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/the-power-of-targets-package-for-reproducible-data-science/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/2BtXjPRLGkQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="enhancing-reproducible-data-science-with-the-targets-package-in-r" class="level1">
<h1>Enhancing Reproducible Data Science with the <code>targets</code> Package in R</h1>
<p>Reproducibility is a non-negotiable pillar in the realm of data science, ensuring that analyses can be reliably replicated and shared. This commitment to reproducibility is vital for building trust and credibility in the outcomes of data-driven projects. In the R ecosystem, the <code>targets</code> package stands out as a powerful tool designed to streamline and enhance reproducibility in data science workflows.</p>
<section id="the-power-of-targets" class="level2">
<h2 class="anchored" data-anchor-id="the-power-of-targets">The Power of <code>targets</code></h2>
<p>The <code>targets</code> package in R offers a robust framework for pipeline management, enabling efficient dependency tracking, automated pipeline execution, and clear documentation of the entire data analysis process. This package ensures that complex pipelines execute consistently in isolated environments.</p>
<p>When combined with tools like <code>{renv}</code> and Docker, <code>targets</code> eliminates the common “it works on my machine” problem. This synergy fosters reproducibility across diverse computational environments, empowering data scientists to create scalable and maintainable projects.</p>
</section>
<section id="workshop-overview" class="level2">
<h2 class="anchored" data-anchor-id="workshop-overview">Workshop Overview</h2>
<p>The workshop, led by Rahul Sangole, a Senior Data Science Manager at Apple, was designed for data scientists and analysts eager to enhance their pipeline management skills. Through hands-on exercises and real-world examples, attendees learned how to leverage <code>targets</code> to build reproducible data science workflows.</p>
<section id="key-topics-covered" class="level3">
<h3 class="anchored" data-anchor-id="key-topics-covered">Key Topics Covered</h3>
<ol type="1">
<li><strong>Pipeline Basics and Functions</strong>:
<ul>
<li>Transitioning from a script-based approach to a <code>targets</code> pipeline involves converting each variable into a “target.”</li>
<li>Using functions to keep pipelines clean and maintainable.</li>
<li>Visualizing pipelines to understand dependencies and execution flow.</li>
</ul></li>
<li><strong>Handling Files</strong>:
<ul>
<li>Managing input and output files within a <code>targets</code> pipeline.</li>
<li>Utilizing <code>format = "file"</code> to track file changes and ensure pipeline validity.</li>
<li>Incorporating Quarto documents for literate programming within pipelines.</li>
</ul></li>
<li><strong>Parallel Computing</strong>:
<ul>
<li>Accelerating pipelines by utilizing multiple workers with the <code>crew</code> package.</li>
<li>Monitoring pipeline execution in real-time with <code>tar_watch()</code>.</li>
</ul></li>
<li><strong>Dynamic Branching</strong>:
<ul>
<li>Utilizing dynamic branching to handle scenarios where the number of models or data sets is not known beforehand.</li>
<li>Implementing <code>pattern = map</code> and <code>pattern = cross</code> to create flexible and scalable pipelines.</li>
</ul></li>
<li><strong>Database Integration</strong>:
<ul>
<li>Connecting to databases, querying data, and writing results back to databases within a <code>targets</code> pipeline.</li>
<li>Using <code>withr</code> for clean database connections and disconnections.</li>
</ul></li>
<li><strong>Comprehensive Example</strong>:
<ul>
<li>Organizing code into clear, maintainable structures with separate functions and pipelines.</li>
<li>Building a full-fledged example of a machine learning pipeline using <code>targets</code> to manage data cleaning, modeling, and reporting.</li>
</ul></li>
</ol>
</section>
<section id="resources" class="level3">
<h3 class="anchored" data-anchor-id="resources">Resources</h3>
<ul>
<li><strong>Setup Instructions</strong> for the workshop: <a href="https://rsangole.netlify.app/talks/2025-06-09_rmedicine-targets/doc.html#setup-instructions">Workshop Setup</a></li>
</ul>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The <code>targets</code> package is an invaluable asset for data scientists committed to reproducibility. By adopting <code>targets</code>, data professionals can build transparent, trustable, and reproducible data workflows. The workshop provided a comprehensive introduction to <code>targets</code>, equipping participants with the knowledge and skills to implement these practices in their projects.</p>
<p><strong>Rahul Sangole’s</strong> expertise and passion for reproducible data science were evident throughout the session, offering participants a deep dive into the capabilities of <code>targets</code>. This workshop is a stepping stone for data scientists to enhance their pipeline management and scale their analytical workflows effectively.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <guid>https://r-consortium.org/posts/the-power-of-targets-package-for-reproducible-data-science/</guid>
  <pubDate>Tue, 24 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/the-power-of-targets-package-for-reproducible-data-science/thumbnail-targets-rahul.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>First Steps with SQL in R: Making Data Talk</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/first-steps-with-sql-in-r-making-data-talk/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/1t0DvqGD9nM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="unlocking-sqls-potential-in-r-a-beginners-guide" class="level1">
<h1>Unlocking SQL’s Potential in R: A Beginner’s Guide</h1>
<p>In the ever-evolving landscape of data science and analytics, SQL (Structured Query Language) remains a steadfast tool for extracting and organizing data. For those entrenched in the world of R, particularly within clinical research and healthcare settings, integrating SQL skills can be a game changer. The recent workshop hosted by Chris Battiston, a seasoned REDCap administrator and research data analyst at Women’s College Hospital, Toronto, underscored the immense potential of SQL when used alongside R, especially for those managing complex data sets in clinical environments.</p>
<section id="why-sql" class="level2">
<h2 class="anchored" data-anchor-id="why-sql">Why SQL?</h2>
<p>SQL’s longevity and widespread use across industries underscore its utility. Developed in the 1970s, SQL was designed for querying relational databases, which makes it perfect for managing structured data. In clinical research, where data integrity and accessibility are paramount, SQL acts as a bridge between raw data and meaningful insights.</p>
<p>SQL is particularly beneficial for:</p>
<ul>
<li><strong>Data Extraction and Transformation</strong>: SQL efficiently handles large datasets, enabling the extraction of specific data points without overwhelming memory resources.</li>
<li><strong>Relational Data Handling</strong>: Ideal for linking tables and datasets, SQL simplifies the process of combining disparate data sources for a comprehensive analysis.</li>
<li><strong>Portability and Familiarity</strong>: As a universal language for data queries, SQL skills are transferable across various platforms and systems, making it a valuable addition to any data analyst’s toolkit.</li>
</ul>
</section>
<section id="sql-in-the-r-environment" class="level2">
<h2 class="anchored" data-anchor-id="sql-in-the-r-environment">SQL in the R Environment</h2>
<p>Chris Battiston’s workshop, “First Steps in SQL with R: Making Data Talk,” offered an in-depth look at how the <code>sqldf</code> package in R can be used to run SQL queries directly on R data frames. This integration allows R users to take advantage of SQL’s strengths without leaving the R environment, streamlining workflows and enhancing productivity.</p>
<p>Key learnings from the workshop included:</p>
<ul>
<li><strong>SQL Basics</strong>: Understanding SQL syntax, including commands like SELECT, FROM, WHERE, ORDER BY, GROUP BY, and JOIN.</li>
<li><strong>Comparative Analysis</strong>: Using SQL alongside <code>dplyr</code> for common data tasks, highlighting scenarios where SQL might offer a more efficient or intuitive solution.</li>
<li><strong>Hands-on Practice</strong>: Participants engaged in live coding exercises, writing SQL queries to filter, sort, group, and join data frames in R.</li>
</ul>
</section>
<section id="practical-applications" class="level2">
<h2 class="anchored" data-anchor-id="practical-applications">Practical Applications</h2>
<p>The workshop provided practical examples using real-world data from New York City hospitals, demonstrating how SQL queries can surface valuable insights quickly. For instance, participants learned to:</p>
<ul>
<li>Identify the top hospitals by procedure type using SQL’s GROUP BY and ORDER BY clauses.</li>
<li>Analyze demographic variations in healthcare charges across different counties.</li>
<li>Understand the nuances of joins, such as inner joins and left joins, to merge datasets effectively.</li>
</ul>
<p>These examples showcased SQL’s ability to handle complex queries and provide actionable insights, essential for clinical data managers and researchers.</p>
</section>
<section id="complementary-tools" class="level2">
<h2 class="anchored" data-anchor-id="complementary-tools">Complementary Tools</h2>
<p>While SQL excels in data extraction and organization, R shines in statistical analysis and visualization. The workshop encouraged participants to think of SQL and R as complementary tools rather than competing ones. For instance, SQL can be used to preprocess and clean data, which can then be fed into R for advanced modeling and visualization.</p>
</section>
<section id="conclusion-and-next-steps" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-and-next-steps">Conclusion and Next Steps</h2>
<p>By the end of the workshop, participants gained confidence in using SQL within R, learning to write queries that enhance data analysis workflows. The session emphasized that while SQL is a powerful tool, its true potential is realized when used to tell a story with data. In clinical research, this means transforming raw data into narratives that drive understanding and inform decision-making.</p>
<p>For those looking to deepen their SQL skills within R, Chris Battiston provided a wealth of resources, including practice queries, cheat sheets, and access to the Spark dataset from New York State. These tools offer a solid foundation for further exploration and mastery of SQL in R.</p>
<p>As the data landscape continues to evolve, the ability to integrate SQL into R workflows will undoubtedly remain a valuable skill, opening doors to more efficient data management and richer insights.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <guid>https://r-consortium.org/posts/first-steps-with-sql-in-r-making-data-talk/</guid>
  <pubDate>Mon, 23 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/first-steps-with-sql-in-r-making-data-talk/thumbnail-SQL-in-R-chris.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Quarto Dashboards: from zero to publish in one hour</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/quarto-dashboards-from-zero-to-publish-in-one-hour/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/NoOU_nzeAGk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="from-zero-to-quarto-dashboards-a-comprehensive-guide" class="level1">
<h1>From Zero to Quarto Dashboards: A Comprehensive Guide</h1>
<p>In the ever-evolving landscape of data science and statistical analysis, Quarto offers a novel approach to creating and sharing interactive dashboards. Quarto Dashboards provide an intuitive platform that is both elegant and efficient, making it easier than ever to present data in a visually appealing format. Professor Mine Çetinkaya-Rundel from Duke University recently demonstrated the power of Quarto Dashboards at the R/Medicine 2025 conference, showcasing how to build and publish a dashboard from scratch in just an hour.</p>
<section id="the-essence-of-quarto-dashboards" class="level2">
<h2 class="anchored" data-anchor-id="the-essence-of-quarto-dashboards">The Essence of Quarto Dashboards</h2>
<p>Quarto is an open-source scientific and technical publishing system designed to enhance the process of creating and collaborating on data projects. It seamlessly combines narrative text and code to produce outputs such as HTML, PDF, Word documents, and dashboards. Quarto Dashboards, in particular, have been available since version 1.4, with the latest pre-release version being 1.8. These dashboards are built using a variety of components, including static graphics, interactive widgets, and tabular data. They are responsive, ensuring that they look great on devices of all sizes.</p>
<section id="key-features-of-quarto-dashboards" class="level3">
<h3 class="anchored" data-anchor-id="key-features-of-quarto-dashboards">Key Features of Quarto Dashboards</h3>
<ol type="1">
<li><p><strong>Component Flexibility</strong>: Users can incorporate static graphics, interactive widgets, tabular data, value boxes, and text annotations into their dashboards, offering a comprehensive range of visualization tools.</p></li>
<li><p><strong>Responsive Design</strong>: With intelligent resizing, Quarto Dashboards offer optimal viewing experiences on any device, whether it’s a smartphone, tablet, or desktop.</p></li>
<li><p><strong>Markdown Comfort</strong>: The ability to author dashboards in plain text markdown with any text editor makes Quarto Dashboards accessible for users familiar with markdown syntax.</p></li>
<li><p><strong>Cross-Language Compatibility</strong>: While the demo primarily used R, Quarto Dashboards support computations in Python and Julia as well, with only minor syntax adjustments needed for code cells.</p></li>
<li><p><strong>Theming and Customization</strong>: Users can define custom themes, including light and dark modes, by utilizing brand YAML files and SCSS rules, allowing for extensive personalization.</p></li>
</ol>
</section>
</section>
<section id="building-a-quarto-dashboard-a-step-by-step-approach" class="level2">
<h2 class="anchored" data-anchor-id="building-a-quarto-dashboard-a-step-by-step-approach">Building a Quarto Dashboard: A Step-by-Step Approach</h2>
<section id="starting-point" class="level3">
<h3 class="anchored" data-anchor-id="starting-point">Starting Point</h3>
<p>The process begins with a basic Quarto document. This document is then transformed into a dashboard by changing the document format to <code>dashboard</code>. The output of each R code cell becomes a card within the dashboard. The organization of these cards can be customized through rows and columns, offering a structured way to present data.</p>
</section>
<section id="creating-value-boxes" class="level3">
<h3 class="anchored" data-anchor-id="creating-value-boxes">Creating Value Boxes</h3>
<p>Value boxes are a key feature of Quarto Dashboards, providing a succinct way to display summary statistics. In the demo, value boxes were used to present the number of keynotes, tutorials, and other session types. Each value box is created by setting the content of a code cell to <code>value-box</code>, with customizable titles, icons, and colors.</p>
</section>
<section id="plot-sizing" class="level3">
<h3 class="anchored" data-anchor-id="plot-sizing">Plot Sizing</h3>
<p>An important aspect of dashboard design is ensuring that plots are appropriately sized. Quarto Dashboards allow users to specify figure heights and widths, ensuring plots are legible and well-integrated into the dashboard layout.</p>
</section>
<section id="light-and-dark-themes" class="level3">
<h3 class="anchored" data-anchor-id="light-and-dark-themes">Light and Dark Themes</h3>
<p>A standout feature of Quarto Dashboards is the ability to switch between light and dark themes. This is achieved through the use of brand YAML files, which define the color palettes for different components. Additionally, users can specify separate plot renderings for each theme, ensuring visual consistency.</p>
</section>
<section id="tab-sets-and-tables" class="level3">
<h3 class="anchored" data-anchor-id="tab-sets-and-tables">Tab Sets and Tables</h3>
<p>For dashboards that require the presentation of multiple tables or plots, Quarto Dashboards support the use of tab sets. This feature allows users to organize content into tabs, providing a cleaner and more interactive user experience. Tables can also be customized with light and dark themes, with helper functions available to streamline the process.</p>
</section>
</section>
<section id="the-future-of-quarto-dashboards" class="level2">
<h2 class="anchored" data-anchor-id="the-future-of-quarto-dashboards">The Future of Quarto Dashboards</h2>
<p>Quarto Dashboards represent a significant advancement in data visualization within the R community. Their flexibility, ease of use, and cross-language compatibility make them a valuable tool for data scientists, educators, and researchers alike. As the Quarto platform continues to evolve, we can expect even more features and enhancements that will further streamline the process of creating interactive and visually appealing dashboards.</p>
<p>Professor Mine Çetinkaya-Rundel’s demonstration at the R/Medicine 2025 conference highlighted the practical applications of Quarto Dashboards, providing attendees with the knowledge and tools needed to create their own dashboards. Her work in statistics and data science pedagogy, particularly her focus on open-source education and student-centered learning, continues to inspire and empower the R community.</p>
<p>For those interested in exploring Quarto Dashboards further, resources and examples are available through the R Consortium and related projects. Whether you’re looking to enhance your teaching materials, share research findings, or simply present data in a new way, Quarto Dashboards offer a robust and versatile solution.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The introduction of Quarto Dashboards marks a new era in data visualization and reporting. By combining the power of R with the flexibility of Quarto, users can create dashboards that are not only informative but also visually stunning. As the R community continues to innovate and collaborate, tools like Quarto Dashboards will undoubtedly play a pivotal role in shaping the future of data science.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <guid>https://r-consortium.org/posts/quarto-dashboards-from-zero-to-publish-in-one-hour/</guid>
  <pubDate>Mon, 23 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/quarto-dashboards-from-zero-to-publish-in-one-hour/thumbnail-quarto-mine.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Supporting R learners on the job during interesting times: A panel of R educators</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/supporting-r-learners-on-the-job-during-interesting-times-a-panel-of-r-educators/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/KKRbA2VYOb4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="navigating-the-seas-of-r-education-insights-from-rmedicine-2025" class="level1">
<h1>Navigating the Seas of R Education: Insights from R/Medicine 2025</h1>
<p>In an ever-evolving world where technology and data science are at the forefront of innovation, the need for effective education in tools like R is more critical than ever. The recent panel discussion at R/Medicine 2025 brought together a group of seasoned educators and practitioners who shared their insights on the challenges and opportunities in teaching R, especially outside the traditional classroom setting. This post delves into the key takeaways from the panel, offering a glimpse into the future of R education and how we can collectively support learners during these interesting times.</p>
<section id="meet-the-panelists" class="level2">
<h2 class="anchored" data-anchor-id="meet-the-panelists">Meet the Panelists</h2>
<p>The panel was composed of esteemed educators with diverse backgrounds, each bringing unique perspectives to the table:</p>
<ul>
<li><p><strong>Ray Balise</strong>: An Associate Professor of Biostatistics and Bioinformatics at the University of Miami, Ray has a long history of teaching statistical programming and is passionate about teaching smart people to think about data.</p></li>
<li><p><strong>Silvia Canelón</strong>: From Penn Medicine Center for Health Justice, Silvia is an advocate for data literacy, communication, and accessibility, with a focus on supporting others in their data-driven journeys.</p></li>
<li><p><strong>Meghan Santiago Harris</strong>: Known for her work in public health and data science education, Meghan emphasizes the importance of self-teaching and mentoring in her field.</p></li>
<li><p><strong>Ted Laderas</strong>: As the Director of Training and Community for the Office of the Chief Data Officer at Fred Hutch Cancer Center, Ted is focused on changing data culture through education and training.</p></li>
<li><p><strong>Joy Payton</strong>: A dedicated data science educator in biomedicine, Joy has been instrumental in building capacity and fostering community among learners.</p></li>
</ul>
</section>
<section id="the-challenge-of-interesting-times" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge-of-interesting-times">The Challenge of Interesting Times</h2>
<p>Joy Payton opened the discussion by acknowledging the “interesting times” we are living in. With cognitive load at an all-time high due to various global and local challenges, learners and educators alike must navigate these turbulent waters. The panelists echoed this sentiment, emphasizing the need for flexibility, adaptability, and a supportive community to help learners thrive.</p>
</section>
<section id="the-role-of-ai-and-llms-in-r-education" class="level2">
<h2 class="anchored" data-anchor-id="the-role-of-ai-and-llms-in-r-education">The Role of AI and LLMs in R Education</h2>
<p>One of the hot topics discussed was the role of AI and large language models (LLMs) in learning and coding. While AI tools like ChatGPT and Claude can assist learners by handling syntax and providing quick answers, the panelists stressed the importance of understanding the underlying mental models and logic behind coding. They agreed that while LLMs are here to stay, educators must teach students how to use these tools effectively without compromising foundational skills.</p>
<p>Ted Laderas highlighted the importance of cognitive load theory and psychological safety in education, advocating for a balance between leveraging AI tools and maintaining the integrity of the learning process. Ray Balise added that teaching prompt engineering and proper use of LLMs should be integrated into the curriculum to ensure students are equipped for the modern workforce.</p>
</section>
<section id="supporting-learners-in-uncertain-times" class="level2">
<h2 class="anchored" data-anchor-id="supporting-learners-in-uncertain-times">Supporting Learners in Uncertain Times</h2>
<p>The panelists shared their strategies for supporting learners amidst uncertainty. Meghan Santiago Harris emphasized the value of integrating R into existing work or projects that learners are passionate about. This approach not only aids retention but also makes the learning process more enjoyable and relevant.</p>
<p>Silvia Canelón pointed out the importance of community and collaboration, noting that much of her learning came from engaging with others in the R community. For educators, fostering an environment where learners can share their experiences and learn from each other is crucial.</p>
</section>
<section id="resources-and-tools-for-r-learners" class="level2">
<h2 class="anchored" data-anchor-id="resources-and-tools-for-r-learners">Resources and Tools for R Learners</h2>
<p>The panelists also discussed their favorite resources for teaching and learning R:</p>
<ul>
<li><strong>Big Book of R</strong>: A comprehensive collection of online R books covering various topics.</li>
<li><strong>R OpenSci Trainings</strong>: Focused on research contexts, these resources are invaluable for developing R skills.</li>
<li><strong>Our Ladies YouTube Channel</strong>: Offers a wealth of tutorials and workshops on different R topics.</li>
<li><strong>Urban Institute’s Do No Harm Project</strong>: Provides guidance on equity and accessibility in data visualization.</li>
</ul>
<p>Joy Payton introduced <strong>Bent</strong>, a data education navigator tool, and <strong>Leoscript</strong>, a markdown renderer for creating educational resources. These tools exemplify the innovative approaches educators are taking to enhance the learning experience.</p>
</section>
<section id="building-a-community-of-r-educators" class="level2">
<h2 class="anchored" data-anchor-id="building-a-community-of-r-educators">Building a Community of R Educators</h2>
<p>In closing, Joy Payton urged attendees to be gate openers, highlighting that everyone has something to contribute to the R community. Whether you’re a seasoned educator or a beginner, sharing your knowledge and experiences can help build a robust network of engaged and supportive R learners and educators.</p>
<p>As we look to the future, the insights from R/Medicine 2025 remind us of the importance of adaptability, community, and thoughtful integration of technology in education. By embracing these principles, we can empower the next generation of R users to navigate the complexities of data science with confidence and creativity.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <guid>https://r-consortium.org/posts/supporting-r-learners-on-the-job-during-interesting-times-a-panel-of-r-educators/</guid>
  <pubDate>Mon, 23 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/supporting-r-learners-on-the-job-during-interesting-times-a-panel-of-r-educators/supporting-r-learners-panel.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Using R shiny to perform and automate decision-analytic modeling for cost-effectiveness analysis</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/using-r-shiny-to-perform-and-automate-decision-analytic-modeling-for-cost-effectiveness-analysis/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/UMdN9SgQbLA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="leveraging-r-shiny-for-automated-decision-analytic-modeling-in-cost-effectiveness-analysis" class="level1">
<h1>Leveraging R Shiny for Automated Decision-Analytic Modeling in Cost-Effectiveness Analysis</h1>
<p>The landscape of health economic evaluation is increasingly influenced by computational tools, and R is playing a pivotal role in this transformation. Mahip Acharya, an assistant professor at the University of Arkansas for Medical Sciences, has showcased the power of R and R Shiny in automating decision-analytic modeling for cost-effectiveness analysis. This innovative approach provides a flexible and efficient method of evaluating economic outcomes, particularly in the healthcare sector.</p>
<section id="understanding-cost-effectiveness-analysis" class="level2">
<h2 class="anchored" data-anchor-id="understanding-cost-effectiveness-analysis">Understanding Cost-Effectiveness Analysis</h2>
<p>Cost-effectiveness analysis (CEA) is a form of economic evaluation that compares the relative costs and outcomes (effects) of different courses of action. It is commonly used in healthcare to determine the value of interventions, such as medications or surgical procedures, by assessing how much benefit they provide relative to their cost. The benefits are typically measured in terms of life years gained or quality-adjusted life years (QALYs).</p>
<p>In CEA, the incremental cost-effectiveness ratio (ICER) is a crucial metric. It represents the additional cost per additional unit of benefit (e.g., per QALY). Decision-makers often compare the ICER to a willingness-to-pay threshold to determine if an intervention is cost-effective. For instance, in the United States, thresholds can range from $50,000 to $150,000 per QALY.</p>
</section>
<section id="the-role-of-markov-models" class="level2">
<h2 class="anchored" data-anchor-id="the-role-of-markov-models">The Role of Markov Models</h2>
<p>Markov models are frequently employed in CEA to simulate the progression of diseases over time. They are particularly useful for modeling chronic conditions where the disease state changes over a long period. In a Markov model, patients transition between different health states at specified probabilities, which can be influenced by interventions.</p>
<p>In his demonstration, Mahip Acharya utilized a Markov model to evaluate the cost-effectiveness of a treatment for hypertension. The model included various health states, such as myocardial infarction, heart failure, stroke, and death. Patients transition between these states based on predefined probabilities, and the model calculates the associated costs and QALYs.</p>
</section>
<section id="automating-the-process-with-r-shiny" class="level2">
<h2 class="anchored" data-anchor-id="automating-the-process-with-r-shiny">Automating the Process with R Shiny</h2>
<p>R Shiny offers an interactive platform for building web applications directly from R. Mahip leveraged Shiny to automate the process of decision-analytic modeling, making it accessible for users without extensive programming expertise. The application allows users to input various parameters, such as transition probabilities, costs, and health utilities, through a user-friendly interface.</p>
<section id="key-features-of-the-shiny-application" class="level3">
<h3 class="anchored" data-anchor-id="key-features-of-the-shiny-application">Key Features of the Shiny Application</h3>
<ul>
<li><p><strong>Model Structure Visualization</strong>: Users can upload a PDF depicting the Markov model structure. The application reads the state names directly from the file, eliminating the need for manual input.</p></li>
<li><p><strong>Data Input Flexibility</strong>: The app accepts multiple data inputs, including transition rates, hazard ratios, treatment costs, and health utilities. These inputs can be adjusted to explore different scenarios.</p></li>
<li><p><strong>Interactive Output</strong>: The application calculates and displays key outputs, such as total costs, total QALYs, and ICERs. Users can adjust the model parameters, such as the discount rate and model cycle length, to see how these changes affect the results.</p></li>
<li><p><strong>Generalizability and Adaptability</strong>: While the current model focuses on hypertension, the framework can be adapted to other chronic conditions by modifying the disease states and transition probabilities.</p></li>
</ul>
</section>
</section>
<section id="challenges-and-future-work" class="level2">
<h2 class="anchored" data-anchor-id="challenges-and-future-work">Challenges and Future Work</h2>
<p>While the application serves as a robust proof of concept, Mahip acknowledges several areas for future improvement:</p>
<ul>
<li><p><strong>Expanding File Format Support</strong>: Currently, the model structure must be uploaded as a PDF. Adding support for other image formats like PNG or JPEG would enhance usability.</p></li>
<li><p><strong>Incorporating Sensitivity Analyses</strong>: Implementing deterministic and probabilistic sensitivity analyses would provide deeper insights into the model’s robustness.</p></li>
<li><p><strong>Enhancing User Inputs</strong>: Allowing users to specify different cycle lengths (e.g., annual or monthly) and starting cohorts would increase the model’s flexibility.</p></li>
<li><p><strong>Supporting Multiple Treatment Groups</strong>: The current version of the app supports only one treatment group. Expanding to multiple groups would accommodate more complex analyses.</p></li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The integration of R Shiny with decision-analytic modeling marks a significant advancement in the field of health economic evaluation. By automating processes and providing a user-friendly interface, Mahip Acharya’s approach empowers researchers and decision-makers to conduct comprehensive cost-effectiveness analyses with greater efficiency and precision. As the application evolves, it promises to become an invaluable tool in the evaluation of healthcare interventions, ultimately contributing to more informed and cost-effective healthcare decisions.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Healthcare</category>
  <category>Clinical Research</category>
  <category>Software Development</category>
  <guid>https://r-consortium.org/posts/using-r-shiny-to-perform-and-automate-decision-analytic-modeling-for-cost-effectiveness-analysis/</guid>
  <pubDate>Mon, 23 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/using-r-shiny-to-perform-and-automate-decision-analytic-modeling-for-cost-effectiveness-analysis/thumbnail-using-r-shiny-mahip.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Visualising data for patients: create accessible charts</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/visualising-data-for-patients-create-accessible-charts/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/j6iwGeYj-xU" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="creating-accessible-and-clear-charts-in-r-a-guide-to-inclusion" class="level3">
<h3 class="anchored" data-anchor-id="creating-accessible-and-clear-charts-in-r-a-guide-to-inclusion">Creating Accessible and Clear Charts in R: A Guide to Inclusion</h3>
<p>In today’s world, patients can access a plethora of health data through various applications. However, visualizing this data in a manner that is understandable and inclusive remains a challenge. Dr.&nbsp;Rita Giordano from Visual Data Studio / Clarum presented a compelling demonstration on how to create charts in R that are not only clear but also accessible to a wider audience, including those with visual impairments like color blindness and autism.</p>
<section id="key-takeaways-from-the-demonstration" class="level4">
<h4 class="anchored" data-anchor-id="key-takeaways-from-the-demonstration">Key Takeaways from the Demonstration:</h4>
<ul>
<li><p><strong>Decluttering and Accessibility</strong>: The importance of decluttering charts and making them accessible by choosing the right color palettes and fonts was emphasized. This ensures that even those without a scientific or medical background can understand the data presented to them.</p></li>
<li><p><strong>Colorblind-Friendly Palettes</strong>: Attendees learned how to create colorblind-friendly palettes in R. Using packages like <code>RColorBrewer</code>, <code>colorspace</code>, and <code>colorblindr</code>, one can quickly generate palettes that are inclusive to those with color vision deficiencies.</p></li>
<li><p><strong>Autism-Friendly Color Schemes</strong>: Dr.&nbsp;Giordano highlighted the significance of using autism-friendly colors, which are calming shades of green and blue. The choice of colors can significantly impact how individuals on the autism spectrum perceive and interact with data.</p></li>
<li><p><strong>Font Readability</strong>: Selecting fonts with high readability is crucial, especially for those with visual impairments. Google Fonts such as Roboto, Lexend, and OpenDyslexic were recommended for their legibility and accessibility.</p></li>
</ul>
</section>
<section id="practical-demonstration" class="level4">
<h4 class="anchored" data-anchor-id="practical-demonstration">Practical Demonstration:</h4>
<p>Dr.&nbsp;Giordano provided a hands-on demonstration using R packages to create accessible charts:</p>
<ul>
<li><p><strong>Using RColorBrewer and Colorblindr</strong>: The session showcased how to apply colorblind-friendly palettes to a dataset using <code>ggplot2</code>. The <code>RColorBrewer</code> package was highlighted for its wide range of palettes that cater to different types of visual impairments.</p></li>
<li><p><strong>Color Contrast Testing</strong>: Ensuring sufficient contrast between text and background colors is essential. Tools like <code>coloratio</code> and <code>colorspace</code> were demonstrated to test and adjust color contrasts, ensuring they meet accessibility standards.</p></li>
<li><p><strong>Recolorize Package</strong>: For those needing to adjust brand colors to be more inclusive, the <code>recolorize</code> package was introduced. By tweaking saturation and brightness, existing palettes can be modified to become colorblind-friendly without losing brand identity.</p></li>
<li><p><strong>Interactive Tools for Palette Selection</strong>: The <code>colorspace</code> package includes a GUI tool that allows users to select palettes and instantly see how they appear to individuals with different types of color blindness.</p></li>
</ul>
</section>
<section id="importance-of-accessibility" class="level4">
<h4 class="anchored" data-anchor-id="importance-of-accessibility">Importance of Accessibility:</h4>
<p>The session underscored the moral and ethical responsibility of data scientists to make data accessible to everyone. Whether it’s a report for an elderly arthritis patient or a dashboard for diabetic monitoring, accessibility should be a primary consideration.</p>
<p>In conclusion, creating accessible visualizations is not merely about compliance but about inclusivity and empathy towards all data consumers. Dr.&nbsp;Giordano’s demonstration serves as a vital reminder and guide on how this can be achieved effectively in R.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <guid>https://r-consortium.org/posts/visualising-data-for-patients-create-accessible-charts/</guid>
  <pubDate>Mon, 23 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/visualising-data-for-patients-create-accessible-charts/thumbnail-accessible-charts-giordano.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Advanced Distance Metrics for High-Dimensional Clustering: introducing ‘distanceHD’ R-package</title>
  <dc:creator>R Consortium, Software Development</dc:creator>
  <link>https://r-consortium.org/posts/advanced-distance-metrics-for-high-dimensional-clustering-introducing-distancehd-r-package/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/EC_vTG-_XCQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="distancehd-a-new-frontier-in-high-dimensional-clustering" class="level1">
<h1>DistanceHD: A New Frontier in High-Dimensional Clustering</h1>
<p>Greetings, data enthusiasts and R aficionados! Today, we delve into development in the realm of high-dimensional clustering with the introduction of the <code>distanceHD</code> package. This tool, crafted by Jung Ae Lee, Assistant Professor of Biostatistics at the University of Massachusetts Chan Medical School, addresses the need for robust, scalable, and statistically sound distance measures tailored specifically for high-dimensional settings.</p>
<section id="why-distancehd" class="level2">
<h2 class="anchored" data-anchor-id="why-distancehd">Why <code>distanceHD</code>?</h2>
<p>Traditional distance metrics such as Euclidean or Mahalanobis distances often falter in high-dimensional spaces. These conventional methods, while effective in lower dimensions, struggle to detect meaningful clusters or outliers when faced with the complexity of high-dimensional data. This gap is particularly evident in fields like genomics, where the number of features (variables) often exceeds the number of samples.</p>
<p>The <code>distanceHD</code> package introduces three innovative distance metrics designed for high-dimensional clustering and outlier detection: the centroid distance, ridge Mahalanobis distance, and maximal data piling (MDP) distance. Each of these metrics offers unique benefits, making them invaluable tools in the data scientist’s arsenal.</p>
</section>
<section id="the-three-pillars-of-distancehd" class="level2">
<h2 class="anchored" data-anchor-id="the-three-pillars-of-distancehd">The Three Pillars of <code>distanceHD</code></h2>
<section id="centroid-distance" class="level3">
<h3 class="anchored" data-anchor-id="centroid-distance">1. Centroid Distance</h3>
<p>Centroid distance, also known as Euclidean distance, calculates the distance between the centers of two groups. It is a straightforward metric but can be limited in high-dimensional spaces with correlated variables. However, it remains effective when variables are uncorrelated.</p>
</section>
<section id="ridge-mahalanobis-distance" class="level3">
<h3 class="anchored" data-anchor-id="ridge-mahalanobis-distance">2. Ridge Mahalanobis Distance</h3>
<p>The ridge Mahalanobis distance introduces a ridge correction constant, alpha, to ensure the covariance matrix is invertible — a common issue in high-dimensional analysis due to singularity problems. This adjustment allows for a more stable calculation of distances, bridging the gap between the centroid and MDP distances. When alpha is large, the ridge Mahalanobis distance approximates the centroid distance, while a smaller alpha brings it closer to the MDP distance.</p>
</section>
<section id="maximal-data-piling-mdp-distance" class="level3">
<h3 class="anchored" data-anchor-id="maximal-data-piling-mdp-distance">3. Maximal Data Piling (MDP) Distance</h3>
<p>The MDP distance is perhaps the most novel of the three metrics. It computes the orthogonal distance between the affine spaces spanned by each class, offering a unique direction vector that maximizes the distance between class projections. This metric shines in situations with highly correlated variables, such as gene expression data, and is particularly effective for classification problems.</p>
</section>
</section>
<section id="practical-applications" class="level2">
<h2 class="anchored" data-anchor-id="practical-applications">Practical Applications</h2>
<p>The <code>distanceHD</code> package is not just a theoretical construct; it has real-world applications in clustering, classification, and outlier detection. For instance, in the context of outlier identification, the MDP distance can effectively discern outliers by maximizing the projection distance in a unique direction. This capability is demonstrated through sequential simulations, such as gene expression data with multiple features and patients, where traditional metrics may fall short.</p>
<p>In classification tasks, the MDP distance provides a high-dimensional, low-sample-size version of Fisher’s discriminant analysis, offering a powerful tool for binary predictions, such as disease status classification.</p>
</section>
<section id="future-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-directions">Future Directions</h2>
<p>While the <code>distanceHD</code> package is a significant leap forward, Jung Ae Lee plans to expand its functionalities further. Upcoming updates will focus on improving outlier detection processes and incorporating additional distance metrics to enhance the package’s versatility and applicability in various high-dimensional contexts.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The <code>distanceHD</code> package represents a significant advancement in the field of high-dimensional data analysis, offering robust tools for clustering, classification, and outlier detection. With its innovative metrics and practical applications, it is poised to become an essential resource for researchers and practitioners working with complex, high-dimensional datasets.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <guid>https://r-consortium.org/posts/advanced-distance-metrics-for-high-dimensional-clustering-introducing-distancehd-r-package/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/advanced-distance-metrics-for-high-dimensional-clustering-introducing-distancehd-r-package/thumbnail-advanced-distance-metrics-lee.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>An Accelerometry Biomarker Framework with Application in Vigilance in UK Biobank Data</title>
  <dc:creator>R Consortium, Healthcare</dc:creator>
  <link>https://r-consortium.org/posts/an-accelerometry-biomarker-framework-with-application-in-vigilance-in-uk-biobank-data/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/KiQAySB2rF4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="exploring-vigilance-with-accelerometry-insights-from-the-uk-biobank" class="level1">
<h1>Exploring Vigilance with Accelerometry: Insights from the UK Biobank</h1>
<p>Accelerometry data from wearable devices have opened new horizons in non-invasive health monitoring, providing a continuous and objective measure of physical activity. Michael Kane, MD Anderson Cancer Center, alongside Dmitri Wolson and Francesco Onarati at Takeda Pharmaceuticals, delves into using accelerometry as a biomarker for assessing vigilance, focusing on the potential to identify non-vigilant states such as excessive daytime sleepiness or narcolepsy-like symptoms. This analysis is rooted in data from the UK Biobank, a rich resource encompassing accelerometry, demographic, and lifestyle data of approximately 78,500 participants.</p>
<section id="understanding-the-data-and-its-challenges" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-data-and-its-challenges">Understanding the Data and Its Challenges</h2>
<p>The UK Biobank’s accelerometry data provides a wealth of information, with measurements taken at 100 Hz over a week for each participant. These data offer a high-resolution view of daily activity patterns, recorded across three axes (X, Y, Z) in milligravities. However, this study’s foundational challenge lies in its observational nature and reliance on self-reported outcomes to define vigilance states.</p>
<p>Vigilance and non-vigilance were distinguished using self-reported symptoms of narcolepsy and frequency of daytime naps. Non-vigilant participants reported narcolepsy symptoms often or always and took frequent naps, while vigilant participants did not report these symptoms or behaviors. Despite a robust overall dataset, the non-vigilant group comprised only 679 individuals, necessitating a careful matching process to ensure comparable analysis groups.</p>
</section>
<section id="propensity-score-matching-for-balanced-analysis" class="level2">
<h2 class="anchored" data-anchor-id="propensity-score-matching-for-balanced-analysis">Propensity Score Matching for Balanced Analysis</h2>
<p>To address imbalances and potential confounders in the observational data, propensity score matching was employed. This method allowed for the creation of matched pairs of vigilant and non-vigilant participants based on physical and lifestyle characteristics, including age, sex, ethnicity, BMI, smoking habits, alcohol use, and more. This rigorous matching resulted in 95 well-matched pairs, setting the stage for a focused exploration of accelerometry’s potential in assessing vigilance.</p>
</section>
<section id="transforming-accelerometry-data-into-spectral-images" class="level2">
<h2 class="anchored" data-anchor-id="transforming-accelerometry-data-into-spectral-images">Transforming Accelerometry Data into Spectral Images</h2>
<p>A critical step in the analysis involved transforming raw accelerometry data into a structured format conducive to machine learning. The data were downsampled to 33 Hz to focus on relevant daily movement frequencies. Subsequently, the data were segmented into five-minute blocks, and a Discrete Fourier Transform was applied to each block. This transformation yielded sorted spectral images, representing the energy expended at different frequencies without capturing the precise timing of activities within a day.</p>
</section>
<section id="convolutional-neural-network-for-classification" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-neural-network-for-classification">Convolutional Neural Network for Classification</h2>
<p>Inspired by the architecture of AlexNet, a simplified convolutional neural network (CNN) was developed to classify participants as vigilant or non-vigilant based on the spectral images. The CNN architecture included convolutional layers, max pooling, and dense layers with dropout to prevent overfitting. Training involved 20-fold cross-validation at the subject level, ensuring that predictions were genuinely out-of-sample.</p>
<p>The CNN yielded an out-of-sample F1 score of 0.576 and an AUC of 0.539 at the sample level for participants aged 65 or younger. At the subject level, the F1 score was 0.539, and the AUC was 0.564. While these results indicate a weak association between accelerometry-derived biomarkers and vigilance states, they underscore the potential for further refinement and application in broader contexts.</p>
</section>
<section id="potential-applications-and-future-directions" class="level2">
<h2 class="anchored" data-anchor-id="potential-applications-and-future-directions">Potential Applications and Future Directions</h2>
<p>The study highlights accelerometry’s promise as a non-invasive tool for assessing cognitive states and movement-related disorders. The association between accelerometry and vigilance, albeit modest, opens avenues for monitoring conditions where non-vigilance is a co-morbidity, such as sleep disorders, neurological conditions, and psychiatric disorders.</p>
<p>The findings also suggest potential applications in monitoring the effectiveness of treatments for conditions like narcolepsy, where stimulant medications may influence accelerometry patterns. Furthermore, the study indicates that stratifying by age could enhance the model’s predictive accuracy, given that younger participants tend to exhibit more movement.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Kane’s exploration into accelerometry as a biomarker for vigilance represents an exciting step forward in leveraging wearable technology for health monitoring. While the association between accelerometry and vigilance is currently weak, the study underscores the potential for accelerometry-derived insights to inform interventions across a range of conditions. The use of R for analysis and presentation further demonstrates the language’s versatility in handling complex datasets and machine learning models.</p>
<p>As the R community continues to evolve and embrace cutting-edge methodologies, studies like this exemplify the innovative applications of R in advancing healthcare research. The integration of accelerometry data into clinical and research settings promises to enhance our understanding of human physiology and behavior, paving the way for more personalized and effective health interventions.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Clinical Research</category>
  <guid>https://r-consortium.org/posts/an-accelerometry-biomarker-framework-with-application-in-vigilance-in-uk-biobank-data/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/an-accelerometry-biomarker-framework-with-application-in-vigilance-in-uk-biobank-data/thumbnail-kane-accelerometry.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Application of attention mechanism to improve performance of llm/mllm used across R/Medicine</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/application-of-attention-mechanism-to-improve-performance-of-llmmllm-used-across-rmedicine/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/aGvAE7Z1XJ0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="rmedicine-2025-enhancing-regulatory-submissions-with-attention-mechanisms" class="level1">
<h1>R/Medicine 2025: Enhancing Regulatory Submissions with Attention Mechanisms</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In the rapidly evolving field of medicine, the integration of technology and data science is ushering in transformative changes. At the R/Medicine 2025 conference, Robert Devine from Johnson &amp; Johnson Companies presented an insightful demonstration on the “Application of attention mechanism to improve performance of surveyed llm/mllm used across R/Medicine.” This session provided a deep dive into how attention mechanisms, a component of transformer architectures, can enhance the efficiency and accuracy of regulatory submissions in the medical field.</p>
</section>
<section id="the-role-of-attention-mechanisms" class="level2">
<h2 class="anchored" data-anchor-id="the-role-of-attention-mechanisms">The Role of Attention Mechanisms</h2>
<p>Attention mechanisms have been pivotal in the field of natural language processing (NLP) since their emergence in 2014. Initially used in neural machine translation tasks, they have since been refined and expanded, particularly following significant advancements by Google in 2017. In the context of medicine, attention mechanisms help improve the performance of large language models (LLMs) and multi-modal large language models (MLLMs), facilitating tasks such as the generation of descriptive vignettes for analytical datasets and the auto-generation of the Analysis Data Reviewer’s Guide (ADRG).</p>
<section id="transformer-architecture-in-rmedicine" class="level3">
<h3 class="anchored" data-anchor-id="transformer-architecture-in-rmedicine">Transformer Architecture in R/Medicine</h3>
<p>The session included a comprehensive overview of the transformer architecture, focusing on the attention mechanism’s role in R/Medicine. This architecture allows models to evaluate which parts of the input data are most relevant at each step of the process, thereby enhancing the model’s ability to generate accurate and contextually relevant outputs.</p>
</section>
</section>
<section id="demonstration-highlights" class="level2">
<h2 class="anchored" data-anchor-id="demonstration-highlights">Demonstration Highlights</h2>
<ol type="1">
<li><p><strong>Vignette Generation for Analysis Dataset Descriptions</strong>: The demonstration showcased how attention mechanisms can automate the creation of detailed vignettes for analysis datasets. These vignettes are crucial for providing context and understanding of safety and efficacy data used in the R Consortium Pilot Series with the FDA.</p></li>
<li><p><strong>Public Repository for Community Participation</strong>: A public repository was introduced to encourage community engagement. This resource allows participants to access and contribute to the development of working examples that hold clinical importance for analytics and regulatory submissions.</p></li>
<li><p><strong>Private-Public Partnerships</strong>: The session highlighted the ongoing collaboration between private entities and regulatory agencies to foster the adoption of mandated submission guidelines. This collaboration is crucial for aligning industry practices with regulatory requirements and accelerating the conformance to technical guidelines.</p></li>
</ol>
</section>
<section id="the-importance-of-r-consortium-pilot-series" class="level2">
<h2 class="anchored" data-anchor-id="the-importance-of-r-consortium-pilot-series">The Importance of R Consortium Pilot Series</h2>
<p>The R Consortium Pilot Series with the FDA plays a vital role in advancing the adoption of modern technical submission standards. These pilot studies focus on demonstrating the practical applications of LLMs/MLLMs in regulatory submissions, aiming to improve efficiency and accuracy while ensuring compliance with regulatory standards.</p>
<p>Reference: <a href="https://r-consortium.org/posts/submissions-wg-pilot5-pilot6-and-more/index.html#progress-reports-and-future-plans">R Submissions Working Group: Pilot 5 Launch and more!</a></p>
<section id="key-achievements-and-future-directions" class="level3">
<h3 class="anchored" data-anchor-id="key-achievements-and-future-directions">Key Achievements and Future Directions</h3>
<ul>
<li><p><strong>Pilot Studies</strong>: The pilot studies have successfully demonstrated the potential of LLMs/MLLMs in automating various aspects of regulatory submissions. These include generating vignettes, auto-generating ADRGs, and streamlining the overall submission process.</p></li>
<li><p><strong>Ongoing Developments</strong>: The session emphasized the need for continuous development and collaboration within the R community. By leveraging the public repository, participants can contribute to ongoing projects, ensuring that advancements in technology are effectively integrated into regulatory practices.</p></li>
</ul>
</section>
</section>
<section id="the-broader-implications-of-attention-mechanisms" class="level2">
<h2 class="anchored" data-anchor-id="the-broader-implications-of-attention-mechanisms">The Broader Implications of Attention Mechanisms</h2>
<p>The application of attention mechanisms extends beyond regulatory submissions. In clinical trials and patient engagement, these mechanisms enable more accurate data analysis and improved patient outcomes. For example, attention mechanisms can identify significant interactions in complex biological systems, such as protein folding, which are critical for understanding disease mechanisms and developing new treatments.</p>
<section id="interoperability-and-data-sharing" class="level3">
<h3 class="anchored" data-anchor-id="interoperability-and-data-sharing">Interoperability and Data Sharing</h3>
<p>The session also touched upon the importance of interoperability and data sharing in the medical field. The 21st Century Cures Act, which promotes interoperability between different technologies, was highlighted as a critical component for facilitating data sharing and enhancing patient care. The use of universal APIs allows patients to share their electronic health records seamlessly, promoting collaboration between clinicians, researchers, and pharmaceutical companies.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Robert Devine’s presentation at R/Medicine 2025 underscored the transformative potential of attention mechanisms in the field of regulatory submissions. By automating complex tasks and enhancing data analysis, these mechanisms pave the way for more efficient and accurate regulatory processes. The R Consortium’s ongoing collaboration with the FDA and industry sponsors is crucial for driving the adoption of these technologies and ensuring that regulatory practices keep pace with technological advancements.</p>
<p>As the R community continues to explore and develop these capabilities, the potential for improving patient outcomes and streamlining regulatory processes becomes increasingly tangible. By embracing these innovations, the medical field can look forward to a future where technology and data science work hand in hand to deliver better healthcare solutions.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Submissions</category>
  <guid>https://r-consortium.org/posts/application-of-attention-mechanism-to-improve-performance-of-llmmllm-used-across-rmedicine/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/application-of-attention-mechanism-to-improve-performance-of-llmmllm-used-across-rmedicine/thumbnail-devine-application-of-attention-mechanism.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Bedside to Bench - Reinventing medicine with AI</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/bedside-to-bench-reinventing-medicine-with-ai/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/OPib-OztZQc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="reinventing-medicine-with-ai-a-new-pathway-to-discovery" class="level1">
<h1>Reinventing Medicine with AI: A New Pathway to Discovery</h1>
<p>The landscape of medical research and discovery is ripe for a seismic shift, one that is being catalyzed by the integration of artificial intelligence (AI) into the healthcare domain. This paradigm shift was the core focus of Dr.&nbsp;Ziad Obermeyer’s keynote at R/Medicine 2025, where he explored the potential of AI to resurrect and revolutionize the “bedside to bench” pathway for medical discovery.</p>
<section id="from-bench-to-bedside-the-traditional-model" class="level2">
<h2 class="anchored" data-anchor-id="from-bench-to-bedside-the-traditional-model">From Bench to Bedside: The Traditional Model</h2>
<p>The traditional model of medical discovery often starts at the molecular level, focusing on genes, proteins, and signaling pathways. This approach has led to significant breakthroughs, particularly in areas like cancer and immunology, where targeted therapies have been transformative. However, this model is not without its limitations. As Dr.&nbsp;Obermeyer pointed out, many complex medical problems remain unsolved, and the traditional bench-to-bedside approach has largely overshadowed the alternative pathway — one that begins with observations at the bedside.</p>
</section>
<section id="ai-a-new-lens-for-medical-discovery" class="level2">
<h2 class="anchored" data-anchor-id="ai-a-new-lens-for-medical-discovery">AI: A New Lens for Medical Discovery</h2>
<p>AI, with its ability to process vast amounts of data and detect patterns invisible to the human eye, offers a powerful alternative. Dr.&nbsp;Obermeyer provided compelling examples of how AI can generate novel empirical observations from real-world data, thereby reinvigorating the bedside-to-bench pathway.</p>
<section id="the-case-of-knee-pain" class="level3">
<h3 class="anchored" data-anchor-id="the-case-of-knee-pain">The Case of Knee Pain</h3>
<p>One of the illustrative examples Dr.&nbsp;Obermeyer discussed was knee pain, a condition that has long eluded effective treatment through traditional molecular approaches. Historically, research on knee pain has zoomed in at a molecular level, focusing on inflammation markers and cartilage degradation. However, this approach has not significantly alleviated the widespread issue of knee pain, leading to an over-reliance on opioids as a treatment.</p>
<p>Dr.&nbsp;Obermeyer’s team leveraged AI to analyze knee X-rays, not to replicate human radiologist interpretations, but to predict patient-reported pain scores directly from the image data. This approach uncovered new insights into the anatomical and physiological factors contributing to knee pain, particularly among Black patients, thus addressing a known disparity in pain management and treatment outcomes.</p>
</section>
<section id="sudden-cardiac-death-predicting-the-unpredictable" class="level3">
<h3 class="anchored" data-anchor-id="sudden-cardiac-death-predicting-the-unpredictable">Sudden Cardiac Death: Predicting the Unpredictable</h3>
<p>Another poignant example involved the use of AI to predict sudden cardiac death, a condition notorious for its unpredictability. By analyzing ECG data linked to patient outcomes in Sweden, Dr.&nbsp;Obermeyer’s team developed a model that could identify individuals at high risk of sudden cardiac death with greater accuracy than traditional metrics. This predictive capability has the potential to optimize the allocation of defibrillators, ensuring they reach those most in need.</p>
</section>
</section>
<section id="implications-for-the-future-of-medicine" class="level2">
<h2 class="anchored" data-anchor-id="implications-for-the-future-of-medicine">Implications for the Future of Medicine</h2>
<p>The implications of these findings are profound. By turning complex medical images into actionable data, AI not only enhances diagnostic precision but also opens new avenues for therapeutic interventions. This approach allows for a re-examination of established medical knowledge, potentially leading to new standards of care.</p>
<section id="bridging-disciplines" class="level3">
<h3 class="anchored" data-anchor-id="bridging-disciplines">Bridging Disciplines</h3>
<p>The integration of AI into medical research also underscores the importance of interdisciplinary collaboration. As Dr.&nbsp;Obermeyer noted, insights from fields such as computer science, economics, and behavioral science can enrich our understanding of health and disease, leading to more holistic and effective healthcare solutions.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Dr.&nbsp;Obermeyer’s keynote at R/Medicine 2025 highlighted the transformative potential of AI in medicine. By enabling a new cycle of discovery that starts at the bedside, AI promises to uncover new abstractions and insights, ultimately improving patient care and outcomes. As the R community continues to explore these frontiers, the collaborative efforts between data scientists, clinicians, and researchers will be crucial in unlocking the full potential of AI in healthcare.</p>
<p>With the power of AI and the collective wisdom of diverse disciplines, the future of medical discovery has the potential for advancements that were once thought unattainable.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>AI</category>
  <guid>https://r-consortium.org/posts/bedside-to-bench-reinventing-medicine-with-ai/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/bedside-to-bench-reinventing-medicine-with-ai/thumbnail-ai-ziad.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Bootstrap inference made easy: p-values and confidence intervals in one line of code</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/bootstrap-inference-made-easy-p-values-and-confidence-intervals-in-one-line-of-code/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/EeAtvWF3twA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="simplifying-bootstrap-inference-in-r-with-the-boot.pval-package" class="level3">
<h3 class="anchored" data-anchor-id="simplifying-bootstrap-inference-in-r-with-the-boot.pval-package">Simplifying Bootstrap Inference in R with the boot.pval Package</h3>
<p>In the realm of statistical analysis, ensuring the reliability of p-values and confidence intervals is paramount, especially when classical assumptions about data distribution do not hold. This is where bootstrap methods come into play, offering a robust alternative by relying on data-driven, empirical distributions rather than theoretical assumptions. Despite their utility, bootstrap methods are often underutilized due to perceived complexities in implementation. However, with advancements in R packages like <code>boot.pval</code>, bootstrap inference has become more accessible than ever.</p>
<section id="the-bootstrap-approach" class="level4">
<h4 class="anchored" data-anchor-id="the-bootstrap-approach">The Bootstrap Approach</h4>
<p>Introduced by Bradley Efron in 1979, the bootstrap method revolutionizes statistical inference by focusing on the empirical distribution of the data itself. Unlike traditional methods that start with a distributional assumption (e.g., normality), bootstrap methods begin with the actual data distribution. By resampling from this empirical distribution and calculating the test statistic repeatedly, we obtain an accurate approximation of its distribution. This allows for the computation of p-values and confidence intervals without reliance on stringent assumptions about data normality.</p>
</section>
<section id="the-role-of-boot.pval" class="level4">
<h4 class="anchored" data-anchor-id="the-role-of-boot.pval">The Role of boot.pval</h4>
<p>The <code>boot.pval</code> package in R, developed by Måns Thulin from Thulin Consulting AB, simplifies the process of applying bootstrap methods to a variety of statistical tests and models. From t-tests to regression coefficients in linear models, GLMs, survival models, and mixed models, <code>boot.pval</code> enables users to compute bootstrap p-values and confidence intervals efficiently—often with just a single line of code.</p>
</section>
<section id="key-features-and-benefits" class="level4">
<h4 class="anchored" data-anchor-id="key-features-and-benefits">Key Features and Benefits</h4>
<ul>
<li><strong>Ease of Use</strong>: The package allows for straightforward computation of bootstrap p-values and confidence intervals without needing to write complex custom functions.</li>
<li><strong>Integration</strong>: Built on top of established R packages like <code>boot</code>, <code>car</code>, <code>lme4</code>, and <code>survival</code>, it ensures compatibility and extends functionality.</li>
<li><strong>Customizability</strong>: Users can create custom bootstrap tests for unique statistical measures.</li>
<li><strong>Consistency</strong>: It ensures that the derived p-values and confidence intervals are consistent, addressing discrepancies often found in other implementations.</li>
</ul>
</section>
<section id="practical-application-a-case-study" class="level4">
<h4 class="anchored" data-anchor-id="practical-application-a-case-study">Practical Application: A Case Study</h4>
<p>Using the sleep dataset in R, Thulin demonstrates how to replace a classic t-test with a bootstrap t-test using <code>boot.pval</code>. This approach not only simplifies the code but also enhances the robustness of the test against non-normal data distributions. The output from <code>boot.pval</code> mirrors that from traditional tests but is derived from a more reliable, data-centric approach.</p>
</section>
<section id="extending-beyond-simple-tests" class="level4">
<h4 class="anchored" data-anchor-id="extending-beyond-simple-tests">Extending Beyond Simple Tests</h4>
<p>The <code>boot.pval</code> package is not limited to simple t-tests; it supports complex models including linear regressions and survival models. By fitting a model using standard R functions like <code>lm()</code> or <code>glm()</code>, users can then apply <code>boot.summary()</code> from <code>boot.pval</code> to obtain detailed summaries including estimates, confidence intervals, and p-values—all bootstrapped for enhanced reliability.</p>
</section>
<section id="conclusion" class="level4">
<h4 class="anchored" data-anchor-id="conclusion">Conclusion</h4>
<p>Bootstrap methods provide a powerful tool for statistical inference when traditional assumptions do not hold. With packages like <code>boot.pval</code>, R users can integrate robust bootstrap techniques into their daily analysis workflows effortlessly. Whether dealing with straightforward comparisons or complex multivariable models, <code>boot.pval</code> offers a user-friendly yet powerful solution for making informed statistical decisions based on solid empirical evidence.</p>
<p>For those interested in delving deeper into bootstrap methods or seeking practical applications within R, exploring further resources such as <a href="https://www.modernstatisticswithr.com/">Thulin’s book “Modern Statistics with R”</a> or foundational texts on bootstrap methodology can be incredibly beneficial.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Clinical Research</category>
  <guid>https://r-consortium.org/posts/bootstrap-inference-made-easy-p-values-and-confidence-intervals-in-one-line-of-code/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/bootstrap-inference-made-easy-p-values-and-confidence-intervals-in-one-line-of-code/thumbnail-bootstrap-inference-thulin.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Co-occurrence analysis and knowledge graphs for suicide risk prediction</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/co-occurrence-analysis-and-knowledge-graphs-for-suicide-risk-prediction/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/zy6DD7-H_bg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="unraveling-the-complexities-of-mental-health-diagnosis-with-r-nlpembeds-and-kgraph" class="level1">
<h1>Unraveling the Complexities of Mental Health Diagnosis with R: <code>nlpembeds</code> and <code>kgraph</code></h1>
<p>In the ever-evolving landscape of mental health research, the integration of technology and data analytics has opened new horizons for understanding and diagnosing complex mental health disorders. At the forefront of this intersection is Thomas Charlon from Harvard Medical School, whose recent presentation at R/Medicine 2025 highlighted groundbreaking tools designed to tackle the intricacies of mental health diagnosis. These tools, encapsulated in the open-source R packages <code>nlpembeds</code> and <code>kgraph</code>, provide a novel approach to processing and analyzing electronic health records (EHR), especially unstructured text data such as clinician notes.</p>
<section id="the-challenge-of-mental-health-diagnosis" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge-of-mental-health-diagnosis">The Challenge of Mental Health Diagnosis</h2>
<p>Mental health disorders are notoriously complex, characterized by overlapping symptoms and subtle variations that can be difficult to discern through traditional diagnostic methods. Current diagnostic frameworks often fail to capture the full spectrum and gradients of these disorders, leading to underdiagnoses or misdiagnoses. This is where Natural Language Processing (NLP) comes into play, offering new opportunities to enhance diagnostic accuracy by leveraging the rich, unstructured data found in clinician notes.</p>
</section>
<section id="the-role-of-celehs-and-the-csrp-project" class="level2">
<h2 class="anchored" data-anchor-id="the-role-of-celehs-and-the-csrp-project">The Role of CELEHS and the CSRP Project</h2>
<p>Charlon’s work is anchored in the CELEHS laboratory at Harvard, which is part of the Center for Suicide Research and Prevention (CSRP) project led by Massachusetts General Hospital. This initiative aims to develop tools that help clinicians assess suicide risk more accurately. A significant challenge addressed by the project is the limitations of the International Classification of Diseases (ICD) in identifying suicide attempts, which often results in underestimations of their prevalence.</p>
</section>
<section id="two-pronged-approach-survival-models-and-nlp-techniques" class="level2">
<h2 class="anchored" data-anchor-id="two-pronged-approach-survival-models-and-nlp-techniques">Two-Pronged Approach: Survival Models and NLP Techniques</h2>
<p>The CELEHS team’s contribution to the CSRP project is twofold. First, they focus on developing robust survival models on codified data that can be transferred between institutions like MGH and Cambridge Health Alliance. Second, they leverage advanced NLP techniques, including name-entity recognition, co-occurrence analysis, and knowledge graphs, to extract deeper insights from clinician notes.</p>
<p>Charlon’s presentation underscored the analysis of cohorts comprising approximately 5,000 teenage patients admitted to psychiatric departments. This analysis is pivotal in understanding the nuances of mental health disorders among adolescents, a demographic that is particularly vulnerable to mental health challenges.</p>
</section>
<section id="introducing-nlpembeds-and-kgraph" class="level2">
<h2 class="anchored" data-anchor-id="introducing-nlpembeds-and-kgraph">Introducing <code>nlpembeds</code> and <code>kgraph</code></h2>
<p>The tools developed by Charlon and his team, <code>nlpembeds</code> and <code>kgraph</code>, are both available on CRAN, the comprehensive R archive network. These packages were introduced in Charlon’s previous talk at R/Medicine 2024, where he discussed “Word embeddings in mental health.” The methods underlying these packages enable the efficient analysis of large volumes of EHR data, both codified and unstructured.</p>
<section id="nlpembeds-harnessing-the-power-of-nlp" class="level3">
<h3 class="anchored" data-anchor-id="nlpembeds-harnessing-the-power-of-nlp"><code>nlpembeds</code>: Harnessing the Power of NLP</h3>
<p>The <code>nlpembeds</code> package focuses on transforming unstructured text data into structured insights. By utilizing techniques like word embeddings, which are akin to the Word2Vec algorithm, the package allows researchers to analyze the relationships between different medical concepts as documented in clinician notes. This transformation helps in identifying patterns and correlations that might not be evident in codified data alone.</p>
</section>
<section id="kgraph-visualizing-complex-data-relationships" class="level3">
<h3 class="anchored" data-anchor-id="kgraph-visualizing-complex-data-relationships"><code>kgraph</code>: Visualizing Complex Data Relationships</h3>
<p>Complementing <code>nlpembeds</code>, the <code>kgraph</code> package offers powerful visualization capabilities for the data relationships uncovered through NLP analysis. By constructing knowledge graphs, researchers can visually explore the connections between various medical concepts, enhancing their ability to interpret complex data sets. This is particularly useful in mental health research, where understanding the interplay between different symptoms and diagnoses is crucial.</p>
</section>
</section>
<section id="real-world-applications-and-insights" class="level2">
<h2 class="anchored" data-anchor-id="real-world-applications-and-insights">Real-World Applications and Insights</h2>
<p>Charlon shared compelling examples of how these tools can be applied in real-world settings. One notable case involved the interpretation of acetaminophen’s role in predicting suicide attempts. Initially perceived as a spurious correlation, further analysis using NLP techniques revealed its association with overdose and intoxication in clinician notes, suggesting a genuine predictive value.</p>
<p>Such insights underscore the importance of considering both codified and unstructured data in mental health research. By integrating these two data types, the tools developed by Charlon and his team provide a more comprehensive view of patient health, enabling more accurate risk assessments and ultimately, better patient outcomes.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Thomas Charlon’s work demonstrates the power of R and open-source tools in unraveling the complexities of mental health diagnosis. The <code>nlpembeds</code> and <code>kgraph</code> packages are not only a testament to the potential of NLP in healthcare but also a call to the R community to continue pushing the boundaries of what is possible with data analytics.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <category>Healthcare</category>
  <guid>https://r-consortium.org/posts/co-occurrence-analysis-and-knowledge-graphs-for-suicide-risk-prediction/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/co-occurrence-analysis-and-knowledge-graphs-for-suicide-risk-prediction/thumbnail-charlon-co-occurence-analysis.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Dengue Forecasting Addressing the Interrupted Effect from COVID-19 Cases</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/dengue-forecasting-addressing-the-interrupted-effect-from-covid-19-cases/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/TNRH2WxA3J0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><em>Note: This project is <a href="http://localhost:4215/posts/empowering-dengue-research-through-the-dengue-data-hub/index.html">funded by the R Consortium</a></em></p>
<section id="navigating-interruption-forecasting-dengue-cases-amidst-covid-19" class="level1">
<h1>Navigating Interruption: Forecasting Dengue Cases Amidst COVID-19</h1>
<p>Dengue fever, a mosquito-borne disease, remains a significant health concern in tropical regions, particularly in countries near the equator. The dengue virus, primarily transmitted by Aedes mosquitoes, thrives in warm, humid conditions with regular rainfall—conditions that are prevalent in these regions. However, the onset of the COVID-19 pandemic introduced an unprecedented interruption in the usual dengue case patterns. This blog post delves into a study that explores how to accurately forecast dengue cases amidst the interruptions caused by the COVID-19 pandemic, using Sri Lanka as a case study.</p>
<p><strong>Understanding the Interruption</strong></p>
<p>During the COVID-19 pandemic, several factors contributed to an unusual pattern in dengue case reporting. These include:</p>
<ul>
<li>Misclassification of dengue as COVID-19 due to overlapping symptoms such as fever, headache, and fatigue.</li>
<li>Underreporting or delayed reporting due to lockdowns and mobility restrictions.</li>
<li>Reduced human-mosquito contact due to people spending more time indoors.</li>
<li>School and workplace closures, which are common sites for dengue transmission.</li>
<li>Travel restrictions, which reduced the spread of dengue to new areas.</li>
</ul>
<p>This period, referred to as the “interrupted period,” significantly affected the usual seasonal and annual patterns of dengue cases.</p>
<p><strong>Forecasting Strategies</strong></p>
<p>The study, presented by Thiyanga S. Talagala from the Department of Statistics at the University of Sri Jayewardenepura, Sri Lanka, investigates three modeling strategies to address this interruption in dengue case forecasting:</p>
<ol type="1">
<li><p><strong>Excluding the Interrupted Period</strong>: This approach involves using only post-COVID-19 data for model training, effectively ignoring the data from the interrupted period.</p></li>
<li><p><strong>Forecasting the Interrupted Period First</strong>: This method involves forecasting the interrupted period based on data up to 2019, then using the updated time series for model training.</p></li>
<li><p><strong>Down-Weighting the Interrupted Period</strong>: This strategy assigns lower weights to data points in the interrupted period, giving higher weights to uninterrupted periods.</p></li>
</ol>
<p>Data from 2007 to 2024 were used for model fitting, and data for 2025 served as the test set to evaluate the performance of these methods across 25 districts in Sri Lanka.</p>
<p><strong>Evaluating the Methods</strong></p>
<p>The study employed various accuracy measures, including Root Mean Square Error (RMSE) and Mean Absolute Error (MAE), to compare the forecasting performance of each approach. The findings revealed that no single method outperformed across all districts. Instead, the effectiveness of each approach depended on the specific characteristics and historical patterns of each district.</p>
<p><strong>Insights and Implications</strong></p>
<p>The study’s insights underscore the importance of tailoring forecasting methods to the unique characteristics of each region. For instance, the down-weighting approach proved effective in areas where the usual dengue patterns persisted despite the interruption. In contrast, excluding the interrupted period worked best in districts that had shifted to a new normal post-COVID-19.</p>
<p>Furthermore, the study highlighted the influence of weather patterns on dengue transmission. Districts affected by specific monsoon periods or characterized by unique weather conditions showed distinct forecasting patterns.</p>
<p><strong>Conclusion</strong></p>
<p>Forecasting dengue cases amidst interruptions like COVID-19 is a complex task that requires adaptive approaches. The study by Talagala emphasizes that understanding the local context, including weather patterns and historical data, is crucial for accurate forecasting. This research not only contributes to improving dengue preparedness but also offers valuable insights for handling future public health interruptions.</p>
<p>For more detailed insights and to explore the methodologies used, you can access <a href="https://github.com/thiyangt/denguedatahub">the project on GitHub</a> and <a href="https://denguedatahub.netlify.app/">the main Dengue Data Hub site</a>.</p>


</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Epidemiology</category>
  <category>Healthcare</category>
  <guid>https://r-consortium.org/posts/dengue-forecasting-addressing-the-interrupted-effect-from-covid-19-cases/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/dengue-forecasting-addressing-the-interrupted-effect-from-covid-19-cases/thumbnail-dengue.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Ethical Considerations of Contrasts in Statistical Modeling of Medical Equity</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/ethical-considerations-of-contrasts-in-statistical-modeling-of-medical-equity/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/pAx92roI3VE" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="ethical-choices-in-regression-analysis-a-case-study-from-seattle-childrens-hospital" class="level2">
<h2 class="anchored" data-anchor-id="ethical-choices-in-regression-analysis-a-case-study-from-seattle-childrens-hospital">Ethical Choices in Regression Analysis: A Case Study from Seattle Children’s Hospital</h2>
<p>In the world of statistical modeling, the choice of coding schemes for categorical variables is not merely a technical consideration but a decision laden with ethical implications. This was the focus of a recent presentation during the R/Medicine 2025 conference by Dwight Barry, Principal Data Scientist at Seattle Children’s Hospital, and Nicole Chicoine, DO, MPH, also at Seattle Children’s Hospital. The talk revolved around how these choices can influence the inferences drawn from regression analyses and ultimately impact healthcare delivery and research.</p>
<section id="the-hospitals-language-diversity" class="level3">
<h3 class="anchored" data-anchor-id="the-hospitals-language-diversity">The Hospital’s Language Diversity</h3>
<p>Seattle Children’s Hospital is a bustling 400-bed facility that handles over half a million patient visits annually. With such a diverse patient base, the hospital encounters more than 130 languages, with Spanish being the most common after English. To accommodate this linguistic diversity, the hospital has initiated its first all-Spanish speaking operating room, ensuring equitable care for non-English speaking patients. This commitment to inclusivity is mirrored in their research methodologies, where the focus is on equitable outcomes across different patient groups.</p>
</section>
<section id="understanding-coding-schemes" class="level3">
<h3 class="anchored" data-anchor-id="understanding-coding-schemes">Understanding Coding Schemes</h3>
<p>The choice of coding schemes in regression models is crucial as it can shape the conclusions drawn from the data. Barry highlighted three coding schemes used in R: treatment contrast, sum contrast, and weighted effect coding. Each scheme presents categorical variables in different ways, affecting the interpretation of the data.</p>
<ol type="1">
<li><p><strong>Treatment Contrast</strong>: This is the default coding in R, where one category is used as a reference against which others are compared. In a clinical setting, this could inadvertently privilege a particular group, often aligning with the English-speaking, white demographic, which can skew the narrative towards existing inequities.</p></li>
<li><p><strong>Sum Contrast</strong>: Here, the grand mean of all categories is used as the reference point. This approach decouples any single category from being the norm, promoting a more balanced view. In the context of healthcare, it shifts the focus from a single dominant group to an aggregate understanding, which can be crucial in addressing biases.</p></li>
<li><p><strong>Weighted Effect Coding</strong>: This method is a variant of the sum contrast, where each category level is weighted by its sample size. Although not a built-in feature in base R, the <code>wec</code> package facilitates its implementation. This approach provides a nuanced view by factoring in the prevalence of each category, which can be especially useful in diverse patient populations.</p></li>
</ol>
</section>
<section id="implications-for-healthcare-research" class="level3">
<h3 class="anchored" data-anchor-id="implications-for-healthcare-research">Implications for Healthcare Research</h3>
<p>The choice of coding scheme is not just a statistical decision but an ethical one, as it affects how healthcare equity is perceived and addressed. Barry’s presentation underscored that while odds ratios may differ across coding schemes, the marginal effects remain consistent, suggesting that predictions are unaffected by these choices. However, the ethical ramifications are significant, as they influence which group is centered in the analysis.</p>
<p>In healthcare research, where categorical exposures like language group, race, or ethnicity lack a natural order, choosing the right coding scheme is vital. Sum contrasts, for instance, provide a means to avoid privileging any group, thereby promoting equity.</p>
</section>
<section id="broader-implications-and-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="broader-implications-and-recommendations">Broader Implications and Recommendations</h3>
<p>Barry’s insights extend beyond surgery to any healthcare condition where equity is a concern. The presentation emphasized the importance of presenting marginal effects alongside regression coefficients or odds ratios to provide a comprehensive view of the data. By decentering from a single reference point, researchers can challenge the dominant social narratives and highlight systemic inequities, paving the way for more inclusive and equitable healthcare practices.</p>
<p>In conclusion, the ethical dimensions of statistical modeling are as crucial as the statistical ones. As healthcare becomes increasingly data-driven, recognizing and addressing these ethical considerations is essential. By adopting coding schemes that promote equity, researchers and healthcare providers can ensure that their work contributes to a more just and equitable healthcare system.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Healthcare</category>
  <guid>https://r-consortium.org/posts/ethical-considerations-of-contrasts-in-statistical-modeling-of-medical-equity/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/ethical-considerations-of-contrasts-in-statistical-modeling-of-medical-equity/thumbnail-ethical-considerations.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Improving Reproducibility of Medical Research with Controlled Vocabularies</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/improving-reproducibility-of-medical-research-with-controlled-vocabularies/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/bf8NZTnz6NA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="enhancing-reproducibility-in-medical-research-with-controlled-vocabularies" class="level1">
<h1>Enhancing Reproducibility in Medical Research with Controlled Vocabularies</h1>
<p>Hello R community! Today, let’s delve into an intriguing topic presented by Jonathan Pearce, a Senior Analyst at the Analysis Group, at the R/Medicine 2025 conference. Jonathan’s insightful presentation focused on improving the reproducibility of medical research through the use of controlled vocabularies for variable naming. Reproducibility in medical research is crucial, and Jonathan’s approach offers a structured methodology to enhance clarity and efficiency in data analysis.</p>
<section id="the-need-for-reproducibility" class="level2">
<h2 class="anchored" data-anchor-id="the-need-for-reproducibility">The Need for Reproducibility</h2>
<p>The reproducibility of medical research has been a growing concern. Successful replication of studies depends on various factors such as code correctness, thorough documentation, and clear communication of the methods used. While much emphasis is often placed on technical aspects like coding environments and data access, the crux of reproducibility lies in the precise and consistent implementation of the original work.</p>
</section>
<section id="introducing-controlled-vocabularies" class="level2">
<h2 class="anchored" data-anchor-id="introducing-controlled-vocabularies">Introducing Controlled Vocabularies</h2>
<p>Jonathan introduced the concept of controlled vocabularies for variable naming as a means to bolster reproducibility. This involves using a schema to label variables in a dataset consistently. This systematic approach encodes metadata directly into variable names, providing immediate context and enhancing the clarity of the data.</p>
<section id="example-of-controlled-vocabulary" class="level3">
<h3 class="anchored" data-anchor-id="example-of-controlled-vocabulary">Example of Controlled Vocabulary</h3>
<p>Consider a scenario where you have a dataset with multiple tables, each containing various types of data. For instance, if you’re tracking whether patients had an eGFR lab test during a baseline period, a variable name following a controlled vocabulary might be <code>labs_eGFR_baseline_ind</code>, where <code>ind</code> stands for indicator. This descriptive naming convention helps users instantly understand the data stored in the column.</p>
</section>
<section id="structured-naming-and-its-advantages" class="level3">
<h3 class="anchored" data-anchor-id="structured-naming-and-its-advantages">Structured Naming and Its Advantages</h3>
<p>Controlled vocabularies mandate a consistent format across the project, which can be crucial for downstream analyses. For example, a variable capturing the median value of an eGFR test might be named <code>labs_eGFR_baseline_median_value</code>, providing additional clarity such as the unit of measurement and the calculation method.</p>
<p>The benefits of controlled vocabularies extend to various facets of data analysis:</p>
<ul>
<li><strong>Regular Expressions:</strong> With consistently named variables, regular expressions can efficiently query subsets of data, facilitating tasks like data validation and report generation.</li>
<li><strong>Data Validation:</strong> By defining expected data types and constraints (e.g., numeric values for duration variables should be non-negative), controlled vocabularies simplify the validation process.</li>
<li><strong>Streamlined Workflows:</strong> Tasks such as creating summary tables, modeling, sensitivity analysis, and generating data dictionaries become more straightforward and reproducible.</li>
</ul>
</section>
</section>
<section id="practical-considerations" class="level2">
<h2 class="anchored" data-anchor-id="practical-considerations">Practical Considerations</h2>
<p>While controlled vocabularies offer significant advantages, there are practical considerations to keep in mind:</p>
<ul>
<li><strong>Balancing Detail and Brevity:</strong> It’s essential to avoid overly long variable names by using abbreviations judiciously.</li>
<li><strong>Initial Setup Effort:</strong> Defining a controlled vocabulary requires upfront work, but the long-term benefits often outweigh the initial investment.</li>
<li><strong>Avoiding Conflicts:</strong> Care must be taken with underscores and keywords to prevent conflicts during keyword searches in variable names.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Jonathan Pearce’s presentation highlighted the transformative potential of controlled vocabularies in enhancing the reproducibility of medical research. By embedding metadata directly within variable names, researchers can achieve greater clarity and consistency, ultimately leading to more reliable and efficient data analysis.</p>
<p>As we strive to make our work more sustainable and transparent, adopting practices like controlled vocabularies can help ensure that our research remains accessible and understandable, even months or years later.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <guid>https://r-consortium.org/posts/improving-reproducibility-of-medical-research-with-controlled-vocabularies/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/improving-reproducibility-of-medical-research-with-controlled-vocabularies/thumbnail-reproducibility-pearce.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Mix, Pour, Share: The rUM Cocktail for Biomedical Project Packaging</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/mix-pour-share-the-rum-cocktail-for-biomedical-project-packaging/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/03_5KrQA-mo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="unveiling-rum-a-game-changer-for-biomedical-reproducibility" class="level1">
<h1>Unveiling rUM: A Game-Changer for Biomedical Reproducibility</h1>
<p>The reproducibility crisis in biomedical research is a formidable challenge that demands innovative solutions. At the forefront of addressing this issue is the rUM package, developed at the University of Miami, which promises to revolutionize how biomedical research projects are documented, shared, and analyzed. Here, we delve into the capabilities of <a href="https://raymondbalise.github.io/rUM/">rUM version 2.2</a>, named “rUM Runner,” and its potential to streamline research processes while promoting reproducibility.</p>
<section id="meet-the-innovators" class="level2">
<h2 class="anchored" data-anchor-id="meet-the-innovators">Meet the Innovators</h2>
<p>Kyle Gealis and Dr.&nbsp;Raymond Balise from the University of Miami’s Department of Public Health Sciences are the developers of rUM. They aim to bridge the gap between complex research needs and user-friendly tools, ensuring that even researchers with minimal programming experience can produce high-quality, reproducible work.</p>
</section>
<section id="what-is-rum" class="level2">
<h2 class="anchored" data-anchor-id="what-is-rum">What is rUM?</h2>
<p>While “rum” might first bring to mind thoughts of a tropical cocktail, in the context of biomedical research, rUM is an acronym for an R package designed to make research reproducibility more accessible. It facilitates the creation of comprehensive, CRAN-ready research packages with minimal coding effort. The package allows researchers to bundle entire projects, complete with analyses, datasets, documentation, and presentations, into a single distributable package.</p>
</section>
<section id="key-features-of-rum" class="level2">
<h2 class="anchored" data-anchor-id="key-features-of-rum">Key Features of rUM</h2>
<ol type="1">
<li><p><strong>CRAN-Ready Package Structures</strong>: With a single function call, rUM creates package structures that adhere to CRAN standards, simplifying the package development process.</p></li>
<li><p><strong>Automated Templates</strong>: R Markdown and Quarto manuscripts can be seamlessly integrated as package vignettes, providing a cohesive documentation of research efforts.</p></li>
<li><p><strong>Enhanced Dataset Documentation</strong>: rUM includes tools for documenting datasets with comprehensive metadata, ensuring that datasets are well-described and easy to understand.</p></li>
<li><p><strong>Presentation Integration</strong>: With rUM, creating Quarto RevealJS presentations within packages is straightforward. This feature allows researchers to share their findings in a visually engaging manner.</p></li>
<li><p><strong>Slide Deck Display and Sharing</strong>: Researchers can easily display and share slide decks stored within their packages, enhancing collaborative communication.</p></li>
</ol>
</section>
<section id="addressing-the-reproducibility-crisis" class="level2">
<h2 class="anchored" data-anchor-id="addressing-the-reproducibility-crisis">Addressing the Reproducibility Crisis</h2>
<p>The reproducibility crisis highlights the inconsistencies often found when attempting to replicate research findings, either with new data or the original datasets. rUM addresses this by providing a systematic approach to project organization, ensuring that all elements of the research process are documented and reproducible.</p>
<section id="the-workflow-from-data-to-package" class="level3">
<h3 class="anchored" data-anchor-id="the-workflow-from-data-to-package">The Workflow: From Data to Package</h3>
<p>The rUM package facilitates a complete workflow:</p>
<ul>
<li><p><strong>Analyzing Data</strong>: Take, for example, the pharmacokinetic dataset (medicaldata::theophylline). rUM assists in analyzing such datasets with integrated tools.</p></li>
<li><p><strong>Documenting with Metadata</strong>: Datasets are documented with comprehensive metadata, making them easier to understand and use by others.</p></li>
<li><p><strong>Creating Presentations</strong>: Researchers can create presentation slides featuring their analysis visualizations, making it easier to communicate findings.</p></li>
<li><p><strong>Bundling into a Package</strong>: Finally, all elements are bundled into a single, distributable package that is discoverable and reusable, addressing critical elements of reproducibility in medical research.</p></li>
</ul>
</section>
</section>
<section id="hands-on-with-rum" class="level2">
<h2 class="anchored" data-anchor-id="hands-on-with-rum">Hands-On with rUM</h2>
<p>Kyle Gealis and Dr.&nbsp;Raymond Balise showcased the practical application of rUM during their presentation, guiding attendees through the steps of installing rUM, creating packages, and developing presentations. They demonstrated how to edit the R profile, initialize a package project, and navigate through various functionalities such as adding licenses, checking package integrity, and documenting datasets.</p>
<section id="creating-and-sharing-presentations" class="level3">
<h3 class="anchored" data-anchor-id="creating-and-sharing-presentations">Creating and Sharing Presentations</h3>
<p>One of the standout features of rUM is its ability to create and share presentations directly from the package. Researchers can build slide decks and integrate them into their packages, making it easier to disseminate research findings.</p>
</section>
</section>
<section id="new-features-in-rum-runner" class="level2">
<h2 class="anchored" data-anchor-id="new-features-in-rum-runner">New Features in rUM Runner</h2>
<p>The new version of rUM introduces several enhancements aimed at improving collaborative communication and documentation:</p>
<ul>
<li><strong>Write Notes</strong>: Create dated progress notes to keep track of project developments.</li>
<li><strong>Readme Templates</strong>: Generate structured readme files, guiding users through the project’s contents and processes.</li>
<li><strong>Quarto Documents and SCSS</strong>: Write Quarto documents and add custom SCSS for personalized styling.</li>
</ul>
</section>
<section id="an-invitation-to-the-r-community" class="level2">
<h2 class="anchored" data-anchor-id="an-invitation-to-the-r-community">An Invitation to the R Community</h2>
<p>The developers of rUM invite the R community to explore and contribute to the package. They encourage feedback, ideas for new templates, and even pull requests on GitHub. The goal is to continuously enhance rUM’s functionality, ensuring it meets the evolving needs of the biomedical research community.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>rUM Runner is more than just a tool; it’s a step towards a more reproducible future in biomedical research. By simplifying the process of creating comprehensive research packages, rUM empowers researchers to focus on science while ensuring their work is transparent, discoverable, and reusable. Whether you’re a seasoned programmer or new to R, rUM offers a pathway to enhancing the reproducibility and impact of your research.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <category>Healthcare</category>
  <guid>https://r-consortium.org/posts/mix-pour-share-the-rum-cocktail-for-biomedical-project-packaging/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/mix-pour-share-the-rum-cocktail-for-biomedical-project-packaging/thumbnail-rum-gealis-balise.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Model Evaluation: From Machine Learning to Generative AI</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/model-evaluation-from-machine-learning-to-generative-ai/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Lq568n0pClc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="exploring-the-future-of-ai-evaluation-with-dr.-erin-ledell-at-rmedicine-2025" class="level1">
<h1>Exploring the Future of AI Evaluation with Dr.&nbsp;Erin LeDell at R/Medicine 2025</h1>
<p>As artificial intelligence evolves, so must the methodologies used to evaluate these systems. Dr.&nbsp;Erin LeDell, a prominent figure in the AI and R communities, addressed this critical transition in her keynote at R/Medicine 2025. Dr.&nbsp;LeDell, the Chief Scientist at Distributional, Inc.&nbsp;and founder of DataScientific, Inc., brings her extensive expertise to the forefront as she guides us through the intricate world of AI evaluation, moving from deterministic machine learning models to the more complex generative AI systems.</p>
<section id="from-deterministic-to-generative-a-paradigm-shift" class="level2">
<h2 class="anchored" data-anchor-id="from-deterministic-to-generative-a-paradigm-shift">From Deterministic to Generative: A Paradigm Shift</h2>
<p>In traditional machine learning (ML), models are deterministic – given the same input, they produce the same output. This predictability provides a clear framework for evaluation metrics such as accuracy, precision, recall, and others familiar to statisticians and data scientists. However, with the rise of large language models (LLMs) and generative AI, this predictability is challenged. These systems introduce non-determinism, meaning outputs can vary even with identical inputs, necessitating new evaluation frameworks.</p>
<p>Dr.&nbsp;LeDell emphasized that traditional accuracy-based metrics fall short when assessing generative AI systems. Instead, evaluation must consider coherence, consistency, and bias, along with the challenges of reproducibility in probabilistic AI systems. This shift leads to questions about how to ensure AI models are reliable and function as expected in real-world scenarios.</p>
</section>
<section id="dr.-erin-ledells-journey-with-r-and-ai" class="level2">
<h2 class="anchored" data-anchor-id="dr.-erin-ledells-journey-with-r-and-ai">Dr.&nbsp;Erin LeDell’s Journey with R and AI</h2>
<p>Dr.&nbsp;LeDell shared her journey from machine learning to generative AI, highlighting her longstanding experience with R. Since 2008, she has been deeply involved in machine learning, contributing to various R packages such as SuperLearner, Subsemble, and H2O, the latter of which she worked on extensively during her tenure at H2O.ai. Her work in AutoML led to the creation of the AutoML benchmark, setting standards for algorithm evaluation.</p>
<p>Her transition into generative AI coincided with the advent of tools like ChatGPT, pushing her to explore new methods for evaluating these complex systems. Dr.&nbsp;LeDell’s passion for using AI in healthcare was evident as she discussed her collaborations with medical companies, applying machine learning to tackle various health-related problems.</p>
</section>
<section id="evaluating-generative-ai-new-approaches" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-generative-ai-new-approaches">Evaluating Generative AI: New Approaches</h2>
<p>The talk delved into the architectural differences between traditional ML systems and modern AI applications, particularly generative AI systems. Dr.&nbsp;LeDell outlined the multi-component nature of these systems, where changes in one part can affect the whole, and the importance of monitoring these changes over time. She addressed the non-stationary behavior of generative AI, noting how external factors and updates from third-party providers can alter system performance.</p>
<p>A significant challenge with generative AI is its inherent non-determinism. Unlike traditional ML models, generative AI requires novel evaluation metrics that account for variability in outputs. Dr.&nbsp;LeDell introduced several frameworks and tools aimed at assessing these systems, emphasizing the role of humans in the evaluation process. Humans provide essential oversight, creating “golden” datasets and evaluating outputs, though this process is not always scalable.</p>
</section>
<section id="llm-as-judge-a-new-standard" class="level2">
<h2 class="anchored" data-anchor-id="llm-as-judge-a-new-standard">LLM as Judge: A New Standard</h2>
<p>One innovative approach Dr.&nbsp;LeDell highlighted is using LLMs themselves to evaluate AI outputs. This method involves deploying LLMs as judges to assess whether responses are correct, helpful, or safe. While this technique is automated and widely used, it presents challenges, such as potential biases if the same model is used for generation and evaluation. Dr.&nbsp;LeDell recommended using specialized LLMs designed for evaluation to mitigate these issues.</p>
</section>
<section id="practical-applications-and-tools" class="level2">
<h2 class="anchored" data-anchor-id="practical-applications-and-tools">Practical Applications and Tools</h2>
<p>Dr.&nbsp;LeDell provided insights into practical applications of generative AI in healthcare, such as using LLMs for clinical note-taking and research acceleration. She described a retrieval-augmented generation (RAG) system for medical Q&amp;A, which combines traditional information retrieval with generative capabilities, enriching AI responses with context from specialized knowledge bases.</p>
<p>For those eager to explore AI evaluation further, Dr.&nbsp;LeDell pointed to several open-source tools, including the R package “vitals,” a port of the Python library “inspect,” developed by JJ Allaire. These tools provide a foundation for customizing evaluation metrics and integrating human oversight into the evaluation pipeline.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Dr.&nbsp;LeDell’s keynote at R/Medicine 2025 illuminated the evolving landscape of AI evaluation, underscoring the need for innovative methodologies to assess the next generation of AI models. Her insights into the intersection of AI and healthcare offer promising pathways for improving AI reliability and trustworthiness in critical applications.</p>
<p>As the R community continues to embrace these advancements, Dr.&nbsp;LeDell’s work encourages practitioners to think critically and creatively about AI evaluation.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>AI</category>
  <guid>https://r-consortium.org/posts/model-evaluation-from-machine-learning-to-generative-ai/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/model-evaluation-from-machine-learning-to-generative-ai/thumbnail-keynote-day2-model-evaluation-ledell.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
