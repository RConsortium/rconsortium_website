<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>R Consortium</title>
<link>https://r-consortium.org/blog/</link>
<atom:link href="https://r-consortium.org/blog/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.7.32</generator>
<lastBuildDate>Mon, 23 Jun 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>First Steps with SQL in R: Making Data Talk</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/first-steps-with-sql-in-r-making-data-talk/</link>
  <description><![CDATA[ 





<p></p><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/1t0DvqGD9nM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div> # Unlocking SQL’s Potential in R: A Beginner’s Guide<p></p>
<p>In the ever-evolving landscape of data science and analytics, SQL (Structured Query Language) remains a steadfast tool for extracting and organizing data. For those entrenched in the world of R, particularly within clinical research and healthcare settings, integrating SQL skills can be a game changer. The recent workshop hosted by Chris Battiston, a seasoned REDCap administrator and research data analyst at Women’s College Hospital, Toronto, underscored the immense potential of SQL when used alongside R, especially for those managing complex data sets in clinical environments.</p>
<section id="why-sql" class="level2">
<h2 class="anchored" data-anchor-id="why-sql">Why SQL?</h2>
<p>SQL’s longevity and widespread use across industries underscore its utility. Developed in the 1970s, SQL was designed for querying relational databases, which makes it perfect for managing structured data. In clinical research, where data integrity and accessibility are paramount, SQL acts as a bridge between raw data and meaningful insights.</p>
<p>SQL is particularly beneficial for:</p>
<ul>
<li><strong>Data Extraction and Transformation</strong>: SQL efficiently handles large datasets, enabling the extraction of specific data points without overwhelming memory resources.</li>
<li><strong>Relational Data Handling</strong>: Ideal for linking tables and datasets, SQL simplifies the process of combining disparate data sources for a comprehensive analysis.</li>
<li><strong>Portability and Familiarity</strong>: As a universal language for data queries, SQL skills are transferable across various platforms and systems, making it a valuable addition to any data analyst’s toolkit.</li>
</ul>
</section>
<section id="sql-in-the-r-environment" class="level2">
<h2 class="anchored" data-anchor-id="sql-in-the-r-environment">SQL in the R Environment</h2>
<p>Chris Battiston’s workshop, “First Steps in SQL with R: Making Data Talk,” offered an in-depth look at how the <code>sqldf</code> package in R can be used to run SQL queries directly on R data frames. This integration allows R users to take advantage of SQL’s strengths without leaving the R environment, streamlining workflows and enhancing productivity.</p>
<p>Key learnings from the workshop included:</p>
<ul>
<li><strong>SQL Basics</strong>: Understanding SQL syntax, including commands like SELECT, FROM, WHERE, ORDER BY, GROUP BY, and JOIN.</li>
<li><strong>Comparative Analysis</strong>: Using SQL alongside <code>dplyr</code> for common data tasks, highlighting scenarios where SQL might offer a more efficient or intuitive solution.</li>
<li><strong>Hands-on Practice</strong>: Participants engaged in live coding exercises, writing SQL queries to filter, sort, group, and join data frames in R.</li>
</ul>
</section>
<section id="practical-applications" class="level2">
<h2 class="anchored" data-anchor-id="practical-applications">Practical Applications</h2>
<p>The workshop provided practical examples using real-world data from New York City hospitals, demonstrating how SQL queries can surface valuable insights quickly. For instance, participants learned to:</p>
<ul>
<li>Identify the top hospitals by procedure type using SQL’s GROUP BY and ORDER BY clauses.</li>
<li>Analyze demographic variations in healthcare charges across different counties.</li>
<li>Understand the nuances of joins, such as inner joins and left joins, to merge datasets effectively.</li>
</ul>
<p>These examples showcased SQL’s ability to handle complex queries and provide actionable insights, essential for clinical data managers and researchers.</p>
</section>
<section id="complementary-tools" class="level2">
<h2 class="anchored" data-anchor-id="complementary-tools">Complementary Tools</h2>
<p>While SQL excels in data extraction and organization, R shines in statistical analysis and visualization. The workshop encouraged participants to think of SQL and R as complementary tools rather than competing ones. For instance, SQL can be used to preprocess and clean data, which can then be fed into R for advanced modeling and visualization.</p>
</section>
<section id="conclusion-and-next-steps" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-and-next-steps">Conclusion and Next Steps</h2>
<p>By the end of the workshop, participants gained confidence in using SQL within R, learning to write queries that enhance data analysis workflows. The session emphasized that while SQL is a powerful tool, its true potential is realized when used to tell a story with data. In clinical research, this means transforming raw data into narratives that drive understanding and inform decision-making.</p>
<p>For those looking to deepen their SQL skills within R, Chris Battiston provided a wealth of resources, including practice queries, cheat sheets, and access to the Spark dataset from New York State. These tools offer a solid foundation for further exploration and mastery of SQL in R.</p>
<p>As the data landscape continues to evolve, the ability to integrate SQL into R workflows will undoubtedly remain a valuable skill, opening doors to more efficient data management and richer insights.</p>


</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <guid>https://r-consortium.org/posts/first-steps-with-sql-in-r-making-data-talk/</guid>
  <pubDate>Mon, 23 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/first-steps-with-sql-in-r-making-data-talk/thumbnail-SQL-in-R-chris.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Advanced Distance Metrics for High-Dimensional Clustering: introducing ‘distanceHD’ R-package</title>
  <dc:creator>R Consortium, Software Development</dc:creator>
  <link>https://r-consortium.org/posts/advanced-distance-metrics-for-high-dimensional-clustering-introducing-distancehd-r-package/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/EC_vTG-_XCQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="distancehd-a-new-frontier-in-high-dimensional-clustering" class="level1">
<h1>DistanceHD: A New Frontier in High-Dimensional Clustering</h1>
<p>Greetings, data enthusiasts and R aficionados! Today, we delve into development in the realm of high-dimensional clustering with the introduction of the <code>distanceHD</code> package. This tool, crafted by Jung Ae Lee, Assistant Professor of Biostatistics at the University of Massachusetts Chan Medical School, addresses the need for robust, scalable, and statistically sound distance measures tailored specifically for high-dimensional settings.</p>
<section id="why-distancehd" class="level2">
<h2 class="anchored" data-anchor-id="why-distancehd">Why <code>distanceHD</code>?</h2>
<p>Traditional distance metrics such as Euclidean or Mahalanobis distances often falter in high-dimensional spaces. These conventional methods, while effective in lower dimensions, struggle to detect meaningful clusters or outliers when faced with the complexity of high-dimensional data. This gap is particularly evident in fields like genomics, where the number of features (variables) often exceeds the number of samples.</p>
<p>The <code>distanceHD</code> package introduces three innovative distance metrics designed for high-dimensional clustering and outlier detection: the centroid distance, ridge Mahalanobis distance, and maximal data piling (MDP) distance. Each of these metrics offers unique benefits, making them invaluable tools in the data scientist’s arsenal.</p>
</section>
<section id="the-three-pillars-of-distancehd" class="level2">
<h2 class="anchored" data-anchor-id="the-three-pillars-of-distancehd">The Three Pillars of <code>distanceHD</code></h2>
<section id="centroid-distance" class="level3">
<h3 class="anchored" data-anchor-id="centroid-distance">1. Centroid Distance</h3>
<p>Centroid distance, also known as Euclidean distance, calculates the distance between the centers of two groups. It is a straightforward metric but can be limited in high-dimensional spaces with correlated variables. However, it remains effective when variables are uncorrelated.</p>
</section>
<section id="ridge-mahalanobis-distance" class="level3">
<h3 class="anchored" data-anchor-id="ridge-mahalanobis-distance">2. Ridge Mahalanobis Distance</h3>
<p>The ridge Mahalanobis distance introduces a ridge correction constant, alpha, to ensure the covariance matrix is invertible — a common issue in high-dimensional analysis due to singularity problems. This adjustment allows for a more stable calculation of distances, bridging the gap between the centroid and MDP distances. When alpha is large, the ridge Mahalanobis distance approximates the centroid distance, while a smaller alpha brings it closer to the MDP distance.</p>
</section>
<section id="maximal-data-piling-mdp-distance" class="level3">
<h3 class="anchored" data-anchor-id="maximal-data-piling-mdp-distance">3. Maximal Data Piling (MDP) Distance</h3>
<p>The MDP distance is perhaps the most novel of the three metrics. It computes the orthogonal distance between the affine spaces spanned by each class, offering a unique direction vector that maximizes the distance between class projections. This metric shines in situations with highly correlated variables, such as gene expression data, and is particularly effective for classification problems.</p>
</section>
</section>
<section id="practical-applications" class="level2">
<h2 class="anchored" data-anchor-id="practical-applications">Practical Applications</h2>
<p>The <code>distanceHD</code> package is not just a theoretical construct; it has real-world applications in clustering, classification, and outlier detection. For instance, in the context of outlier identification, the MDP distance can effectively discern outliers by maximizing the projection distance in a unique direction. This capability is demonstrated through sequential simulations, such as gene expression data with multiple features and patients, where traditional metrics may fall short.</p>
<p>In classification tasks, the MDP distance provides a high-dimensional, low-sample-size version of Fisher’s discriminant analysis, offering a powerful tool for binary predictions, such as disease status classification.</p>
</section>
<section id="future-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-directions">Future Directions</h2>
<p>While the <code>distanceHD</code> package is a significant leap forward, Jung Ae Lee plans to expand its functionalities further. Upcoming updates will focus on improving outlier detection processes and incorporating additional distance metrics to enhance the package’s versatility and applicability in various high-dimensional contexts.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The <code>distanceHD</code> package represents a significant advancement in the field of high-dimensional data analysis, offering robust tools for clustering, classification, and outlier detection. With its innovative metrics and practical applications, it is poised to become an essential resource for researchers and practitioners working with complex, high-dimensional datasets.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <guid>https://r-consortium.org/posts/advanced-distance-metrics-for-high-dimensional-clustering-introducing-distancehd-r-package/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/advanced-distance-metrics-for-high-dimensional-clustering-introducing-distancehd-r-package/thumbnail-advanced-distance-metrics-lee.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>An Accelerometry Biomarker Framework with Application in Vigilance in UK Biobank Data</title>
  <dc:creator>R Consortium, Healthcare</dc:creator>
  <link>https://r-consortium.org/posts/an-accelerometry-biomarker-framework-with-application-in-vigilance-in-uk-biobank-data/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/KiQAySB2rF4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="exploring-vigilance-with-accelerometry-insights-from-the-uk-biobank" class="level1">
<h1>Exploring Vigilance with Accelerometry: Insights from the UK Biobank</h1>
<p>Accelerometry data from wearable devices have opened new horizons in non-invasive health monitoring, providing a continuous and objective measure of physical activity. Michael Kane, MD Anderson Cancer Center, alongside Dmitri Wolson and Francesco Onarati at Takeda Pharmaceuticals, delves into using accelerometry as a biomarker for assessing vigilance, focusing on the potential to identify non-vigilant states such as excessive daytime sleepiness or narcolepsy-like symptoms. This analysis is rooted in data from the UK Biobank, a rich resource encompassing accelerometry, demographic, and lifestyle data of approximately 78,500 participants.</p>
<section id="understanding-the-data-and-its-challenges" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-data-and-its-challenges">Understanding the Data and Its Challenges</h2>
<p>The UK Biobank’s accelerometry data provides a wealth of information, with measurements taken at 100 Hz over a week for each participant. These data offer a high-resolution view of daily activity patterns, recorded across three axes (X, Y, Z) in milligravities. However, this study’s foundational challenge lies in its observational nature and reliance on self-reported outcomes to define vigilance states.</p>
<p>Vigilance and non-vigilance were distinguished using self-reported symptoms of narcolepsy and frequency of daytime naps. Non-vigilant participants reported narcolepsy symptoms often or always and took frequent naps, while vigilant participants did not report these symptoms or behaviors. Despite a robust overall dataset, the non-vigilant group comprised only 679 individuals, necessitating a careful matching process to ensure comparable analysis groups.</p>
</section>
<section id="propensity-score-matching-for-balanced-analysis" class="level2">
<h2 class="anchored" data-anchor-id="propensity-score-matching-for-balanced-analysis">Propensity Score Matching for Balanced Analysis</h2>
<p>To address imbalances and potential confounders in the observational data, propensity score matching was employed. This method allowed for the creation of matched pairs of vigilant and non-vigilant participants based on physical and lifestyle characteristics, including age, sex, ethnicity, BMI, smoking habits, alcohol use, and more. This rigorous matching resulted in 95 well-matched pairs, setting the stage for a focused exploration of accelerometry’s potential in assessing vigilance.</p>
</section>
<section id="transforming-accelerometry-data-into-spectral-images" class="level2">
<h2 class="anchored" data-anchor-id="transforming-accelerometry-data-into-spectral-images">Transforming Accelerometry Data into Spectral Images</h2>
<p>A critical step in the analysis involved transforming raw accelerometry data into a structured format conducive to machine learning. The data were downsampled to 33 Hz to focus on relevant daily movement frequencies. Subsequently, the data were segmented into five-minute blocks, and a Discrete Fourier Transform was applied to each block. This transformation yielded sorted spectral images, representing the energy expended at different frequencies without capturing the precise timing of activities within a day.</p>
</section>
<section id="convolutional-neural-network-for-classification" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-neural-network-for-classification">Convolutional Neural Network for Classification</h2>
<p>Inspired by the architecture of AlexNet, a simplified convolutional neural network (CNN) was developed to classify participants as vigilant or non-vigilant based on the spectral images. The CNN architecture included convolutional layers, max pooling, and dense layers with dropout to prevent overfitting. Training involved 20-fold cross-validation at the subject level, ensuring that predictions were genuinely out-of-sample.</p>
<p>The CNN yielded an out-of-sample F1 score of 0.576 and an AUC of 0.539 at the sample level for participants aged 65 or younger. At the subject level, the F1 score was 0.539, and the AUC was 0.564. While these results indicate a weak association between accelerometry-derived biomarkers and vigilance states, they underscore the potential for further refinement and application in broader contexts.</p>
</section>
<section id="potential-applications-and-future-directions" class="level2">
<h2 class="anchored" data-anchor-id="potential-applications-and-future-directions">Potential Applications and Future Directions</h2>
<p>The study highlights accelerometry’s promise as a non-invasive tool for assessing cognitive states and movement-related disorders. The association between accelerometry and vigilance, albeit modest, opens avenues for monitoring conditions where non-vigilance is a co-morbidity, such as sleep disorders, neurological conditions, and psychiatric disorders.</p>
<p>The findings also suggest potential applications in monitoring the effectiveness of treatments for conditions like narcolepsy, where stimulant medications may influence accelerometry patterns. Furthermore, the study indicates that stratifying by age could enhance the model’s predictive accuracy, given that younger participants tend to exhibit more movement.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Kane’s exploration into accelerometry as a biomarker for vigilance represents an exciting step forward in leveraging wearable technology for health monitoring. While the association between accelerometry and vigilance is currently weak, the study underscores the potential for accelerometry-derived insights to inform interventions across a range of conditions. The use of R for analysis and presentation further demonstrates the language’s versatility in handling complex datasets and machine learning models.</p>
<p>As the R community continues to evolve and embrace cutting-edge methodologies, studies like this exemplify the innovative applications of R in advancing healthcare research. The integration of accelerometry data into clinical and research settings promises to enhance our understanding of human physiology and behavior, paving the way for more personalized and effective health interventions.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Clinical Research</category>
  <guid>https://r-consortium.org/posts/an-accelerometry-biomarker-framework-with-application-in-vigilance-in-uk-biobank-data/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/an-accelerometry-biomarker-framework-with-application-in-vigilance-in-uk-biobank-data/thumbnail-kane-accelerometry.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Application of attention mechanism to improve performance of llm/mllm used across R/Medicine</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/application-of-attention-mechanism-to-improve-performance-of-llmmllm-used-across-rmedicine/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/aGvAE7Z1XJ0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="rmedicine-2025-enhancing-regulatory-submissions-with-attention-mechanisms" class="level1">
<h1>R/Medicine 2025: Enhancing Regulatory Submissions with Attention Mechanisms</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In the rapidly evolving field of medicine, the integration of technology and data science is ushering in transformative changes. At the R/Medicine 2025 conference, Robert Devine from Johnson &amp; Johnson Companies presented an insightful demonstration on the “Application of attention mechanism to improve performance of surveyed llm/mllm used across R/Medicine.” This session provided a deep dive into how attention mechanisms, a component of transformer architectures, can enhance the efficiency and accuracy of regulatory submissions in the medical field.</p>
</section>
<section id="the-role-of-attention-mechanisms" class="level2">
<h2 class="anchored" data-anchor-id="the-role-of-attention-mechanisms">The Role of Attention Mechanisms</h2>
<p>Attention mechanisms have been pivotal in the field of natural language processing (NLP) since their emergence in 2014. Initially used in neural machine translation tasks, they have since been refined and expanded, particularly following significant advancements by Google in 2017. In the context of medicine, attention mechanisms help improve the performance of large language models (LLMs) and multi-modal large language models (MLLMs), facilitating tasks such as the generation of descriptive vignettes for analytical datasets and the auto-generation of the Analysis Data Reviewer’s Guide (ADRG).</p>
<section id="transformer-architecture-in-rmedicine" class="level3">
<h3 class="anchored" data-anchor-id="transformer-architecture-in-rmedicine">Transformer Architecture in R/Medicine</h3>
<p>The session included a comprehensive overview of the transformer architecture, focusing on the attention mechanism’s role in R/Medicine. This architecture allows models to evaluate which parts of the input data are most relevant at each step of the process, thereby enhancing the model’s ability to generate accurate and contextually relevant outputs.</p>
</section>
</section>
<section id="demonstration-highlights" class="level2">
<h2 class="anchored" data-anchor-id="demonstration-highlights">Demonstration Highlights</h2>
<ol type="1">
<li><p><strong>Vignette Generation for Analysis Dataset Descriptions</strong>: The demonstration showcased how attention mechanisms can automate the creation of detailed vignettes for analysis datasets. These vignettes are crucial for providing context and understanding of safety and efficacy data used in the R Consortium Pilot Series with the FDA.</p></li>
<li><p><strong>Public Repository for Community Participation</strong>: A public repository was introduced to encourage community engagement. This resource allows participants to access and contribute to the development of working examples that hold clinical importance for analytics and regulatory submissions.</p></li>
<li><p><strong>Private-Public Partnerships</strong>: The session highlighted the ongoing collaboration between private entities and regulatory agencies to foster the adoption of mandated submission guidelines. This collaboration is crucial for aligning industry practices with regulatory requirements and accelerating the conformance to technical guidelines.</p></li>
</ol>
</section>
<section id="the-importance-of-r-consortium-pilot-series" class="level2">
<h2 class="anchored" data-anchor-id="the-importance-of-r-consortium-pilot-series">The Importance of R Consortium Pilot Series</h2>
<p>The R Consortium Pilot Series with the FDA plays a vital role in advancing the adoption of modern technical submission standards. These pilot studies focus on demonstrating the practical applications of LLMs/MLLMs in regulatory submissions, aiming to improve efficiency and accuracy while ensuring compliance with regulatory standards.</p>
<p>Reference: <a href="https://r-consortium.org/posts/submissions-wg-pilot5-pilot6-and-more/index.html#progress-reports-and-future-plans">R Submissions Working Group: Pilot 5 Launch and more!</a></p>
<section id="key-achievements-and-future-directions" class="level3">
<h3 class="anchored" data-anchor-id="key-achievements-and-future-directions">Key Achievements and Future Directions</h3>
<ul>
<li><p><strong>Pilot Studies</strong>: The pilot studies have successfully demonstrated the potential of LLMs/MLLMs in automating various aspects of regulatory submissions. These include generating vignettes, auto-generating ADRGs, and streamlining the overall submission process.</p></li>
<li><p><strong>Ongoing Developments</strong>: The session emphasized the need for continuous development and collaboration within the R community. By leveraging the public repository, participants can contribute to ongoing projects, ensuring that advancements in technology are effectively integrated into regulatory practices.</p></li>
</ul>
</section>
</section>
<section id="the-broader-implications-of-attention-mechanisms" class="level2">
<h2 class="anchored" data-anchor-id="the-broader-implications-of-attention-mechanisms">The Broader Implications of Attention Mechanisms</h2>
<p>The application of attention mechanisms extends beyond regulatory submissions. In clinical trials and patient engagement, these mechanisms enable more accurate data analysis and improved patient outcomes. For example, attention mechanisms can identify significant interactions in complex biological systems, such as protein folding, which are critical for understanding disease mechanisms and developing new treatments.</p>
<section id="interoperability-and-data-sharing" class="level3">
<h3 class="anchored" data-anchor-id="interoperability-and-data-sharing">Interoperability and Data Sharing</h3>
<p>The session also touched upon the importance of interoperability and data sharing in the medical field. The 21st Century Cures Act, which promotes interoperability between different technologies, was highlighted as a critical component for facilitating data sharing and enhancing patient care. The use of universal APIs allows patients to share their electronic health records seamlessly, promoting collaboration between clinicians, researchers, and pharmaceutical companies.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Robert Devine’s presentation at R/Medicine 2025 underscored the transformative potential of attention mechanisms in the field of regulatory submissions. By automating complex tasks and enhancing data analysis, these mechanisms pave the way for more efficient and accurate regulatory processes. The R Consortium’s ongoing collaboration with the FDA and industry sponsors is crucial for driving the adoption of these technologies and ensuring that regulatory practices keep pace with technological advancements.</p>
<p>As the R community continues to explore and develop these capabilities, the potential for improving patient outcomes and streamlining regulatory processes becomes increasingly tangible. By embracing these innovations, the medical field can look forward to a future where technology and data science work hand in hand to deliver better healthcare solutions.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Submissions</category>
  <guid>https://r-consortium.org/posts/application-of-attention-mechanism-to-improve-performance-of-llmmllm-used-across-rmedicine/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/application-of-attention-mechanism-to-improve-performance-of-llmmllm-used-across-rmedicine/thumbnail-devine-application-of-attention-mechanism.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Bedside to Bench - Reinventing medicine with AI</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/bedside-to-bench-reinventing-medicine-with-ai/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/OPib-OztZQc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="reinventing-medicine-with-ai-a-new-pathway-to-discovery" class="level1">
<h1>Reinventing Medicine with AI: A New Pathway to Discovery</h1>
<p>The landscape of medical research and discovery is ripe for a seismic shift, one that is being catalyzed by the integration of artificial intelligence (AI) into the healthcare domain. This paradigm shift was the core focus of Dr.&nbsp;Ziad Obermeyer’s keynote at R/Medicine 2025, where he explored the potential of AI to resurrect and revolutionize the “bedside to bench” pathway for medical discovery.</p>
<section id="from-bench-to-bedside-the-traditional-model" class="level2">
<h2 class="anchored" data-anchor-id="from-bench-to-bedside-the-traditional-model">From Bench to Bedside: The Traditional Model</h2>
<p>The traditional model of medical discovery often starts at the molecular level, focusing on genes, proteins, and signaling pathways. This approach has led to significant breakthroughs, particularly in areas like cancer and immunology, where targeted therapies have been transformative. However, this model is not without its limitations. As Dr.&nbsp;Obermeyer pointed out, many complex medical problems remain unsolved, and the traditional bench-to-bedside approach has largely overshadowed the alternative pathway — one that begins with observations at the bedside.</p>
</section>
<section id="ai-a-new-lens-for-medical-discovery" class="level2">
<h2 class="anchored" data-anchor-id="ai-a-new-lens-for-medical-discovery">AI: A New Lens for Medical Discovery</h2>
<p>AI, with its ability to process vast amounts of data and detect patterns invisible to the human eye, offers a powerful alternative. Dr.&nbsp;Obermeyer provided compelling examples of how AI can generate novel empirical observations from real-world data, thereby reinvigorating the bedside-to-bench pathway.</p>
<section id="the-case-of-knee-pain" class="level3">
<h3 class="anchored" data-anchor-id="the-case-of-knee-pain">The Case of Knee Pain</h3>
<p>One of the illustrative examples Dr.&nbsp;Obermeyer discussed was knee pain, a condition that has long eluded effective treatment through traditional molecular approaches. Historically, research on knee pain has zoomed in at a molecular level, focusing on inflammation markers and cartilage degradation. However, this approach has not significantly alleviated the widespread issue of knee pain, leading to an over-reliance on opioids as a treatment.</p>
<p>Dr.&nbsp;Obermeyer’s team leveraged AI to analyze knee X-rays, not to replicate human radiologist interpretations, but to predict patient-reported pain scores directly from the image data. This approach uncovered new insights into the anatomical and physiological factors contributing to knee pain, particularly among Black patients, thus addressing a known disparity in pain management and treatment outcomes.</p>
</section>
<section id="sudden-cardiac-death-predicting-the-unpredictable" class="level3">
<h3 class="anchored" data-anchor-id="sudden-cardiac-death-predicting-the-unpredictable">Sudden Cardiac Death: Predicting the Unpredictable</h3>
<p>Another poignant example involved the use of AI to predict sudden cardiac death, a condition notorious for its unpredictability. By analyzing ECG data linked to patient outcomes in Sweden, Dr.&nbsp;Obermeyer’s team developed a model that could identify individuals at high risk of sudden cardiac death with greater accuracy than traditional metrics. This predictive capability has the potential to optimize the allocation of defibrillators, ensuring they reach those most in need.</p>
</section>
</section>
<section id="implications-for-the-future-of-medicine" class="level2">
<h2 class="anchored" data-anchor-id="implications-for-the-future-of-medicine">Implications for the Future of Medicine</h2>
<p>The implications of these findings are profound. By turning complex medical images into actionable data, AI not only enhances diagnostic precision but also opens new avenues for therapeutic interventions. This approach allows for a re-examination of established medical knowledge, potentially leading to new standards of care.</p>
<section id="bridging-disciplines" class="level3">
<h3 class="anchored" data-anchor-id="bridging-disciplines">Bridging Disciplines</h3>
<p>The integration of AI into medical research also underscores the importance of interdisciplinary collaboration. As Dr.&nbsp;Obermeyer noted, insights from fields such as computer science, economics, and behavioral science can enrich our understanding of health and disease, leading to more holistic and effective healthcare solutions.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Dr.&nbsp;Obermeyer’s keynote at R/Medicine 2025 highlighted the transformative potential of AI in medicine. By enabling a new cycle of discovery that starts at the bedside, AI promises to uncover new abstractions and insights, ultimately improving patient care and outcomes. As the R community continues to explore these frontiers, the collaborative efforts between data scientists, clinicians, and researchers will be crucial in unlocking the full potential of AI in healthcare.</p>
<p>With the power of AI and the collective wisdom of diverse disciplines, the future of medical discovery has the potential for advancements that were once thought unattainable.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>AI</category>
  <guid>https://r-consortium.org/posts/bedside-to-bench-reinventing-medicine-with-ai/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/bedside-to-bench-reinventing-medicine-with-ai/thumbnail-ai-ziad.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Bootstrap inference made easy: p-values and confidence intervals in one line of code</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/bootstrap-inference-made-easy-p-values-and-confidence-intervals-in-one-line-of-code/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/EeAtvWF3twA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="simplifying-bootstrap-inference-in-r-with-the-boot.pval-package" class="level3">
<h3 class="anchored" data-anchor-id="simplifying-bootstrap-inference-in-r-with-the-boot.pval-package">Simplifying Bootstrap Inference in R with the boot.pval Package</h3>
<p>In the realm of statistical analysis, ensuring the reliability of p-values and confidence intervals is paramount, especially when classical assumptions about data distribution do not hold. This is where bootstrap methods come into play, offering a robust alternative by relying on data-driven, empirical distributions rather than theoretical assumptions. Despite their utility, bootstrap methods are often underutilized due to perceived complexities in implementation. However, with advancements in R packages like <code>boot.pval</code>, bootstrap inference has become more accessible than ever.</p>
<section id="the-bootstrap-approach" class="level4">
<h4 class="anchored" data-anchor-id="the-bootstrap-approach">The Bootstrap Approach</h4>
<p>Introduced by Bradley Efron in 1979, the bootstrap method revolutionizes statistical inference by focusing on the empirical distribution of the data itself. Unlike traditional methods that start with a distributional assumption (e.g., normality), bootstrap methods begin with the actual data distribution. By resampling from this empirical distribution and calculating the test statistic repeatedly, we obtain an accurate approximation of its distribution. This allows for the computation of p-values and confidence intervals without reliance on stringent assumptions about data normality.</p>
</section>
<section id="the-role-of-boot.pval" class="level4">
<h4 class="anchored" data-anchor-id="the-role-of-boot.pval">The Role of boot.pval</h4>
<p>The <code>boot.pval</code> package in R, developed by Måns Thulin from Thulin Consulting AB, simplifies the process of applying bootstrap methods to a variety of statistical tests and models. From t-tests to regression coefficients in linear models, GLMs, survival models, and mixed models, <code>boot.pval</code> enables users to compute bootstrap p-values and confidence intervals efficiently—often with just a single line of code.</p>
</section>
<section id="key-features-and-benefits" class="level4">
<h4 class="anchored" data-anchor-id="key-features-and-benefits">Key Features and Benefits</h4>
<ul>
<li><strong>Ease of Use</strong>: The package allows for straightforward computation of bootstrap p-values and confidence intervals without needing to write complex custom functions.</li>
<li><strong>Integration</strong>: Built on top of established R packages like <code>boot</code>, <code>car</code>, <code>lme4</code>, and <code>survival</code>, it ensures compatibility and extends functionality.</li>
<li><strong>Customizability</strong>: Users can create custom bootstrap tests for unique statistical measures.</li>
<li><strong>Consistency</strong>: It ensures that the derived p-values and confidence intervals are consistent, addressing discrepancies often found in other implementations.</li>
</ul>
</section>
<section id="practical-application-a-case-study" class="level4">
<h4 class="anchored" data-anchor-id="practical-application-a-case-study">Practical Application: A Case Study</h4>
<p>Using the sleep dataset in R, Thulin demonstrates how to replace a classic t-test with a bootstrap t-test using <code>boot.pval</code>. This approach not only simplifies the code but also enhances the robustness of the test against non-normal data distributions. The output from <code>boot.pval</code> mirrors that from traditional tests but is derived from a more reliable, data-centric approach.</p>
</section>
<section id="extending-beyond-simple-tests" class="level4">
<h4 class="anchored" data-anchor-id="extending-beyond-simple-tests">Extending Beyond Simple Tests</h4>
<p>The <code>boot.pval</code> package is not limited to simple t-tests; it supports complex models including linear regressions and survival models. By fitting a model using standard R functions like <code>lm()</code> or <code>glm()</code>, users can then apply <code>boot.summary()</code> from <code>boot.pval</code> to obtain detailed summaries including estimates, confidence intervals, and p-values—all bootstrapped for enhanced reliability.</p>
</section>
<section id="conclusion" class="level4">
<h4 class="anchored" data-anchor-id="conclusion">Conclusion</h4>
<p>Bootstrap methods provide a powerful tool for statistical inference when traditional assumptions do not hold. With packages like <code>boot.pval</code>, R users can integrate robust bootstrap techniques into their daily analysis workflows effortlessly. Whether dealing with straightforward comparisons or complex multivariable models, <code>boot.pval</code> offers a user-friendly yet powerful solution for making informed statistical decisions based on solid empirical evidence.</p>
<p>For those interested in delving deeper into bootstrap methods or seeking practical applications within R, exploring further resources such as <a href="https://www.modernstatisticswithr.com/">Thulin’s book “Modern Statistics with R”</a> or foundational texts on bootstrap methodology can be incredibly beneficial.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Clinical Research</category>
  <guid>https://r-consortium.org/posts/bootstrap-inference-made-easy-p-values-and-confidence-intervals-in-one-line-of-code/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/bootstrap-inference-made-easy-p-values-and-confidence-intervals-in-one-line-of-code/thumbnail-bootstrap-inference-thulin.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Co-occurrence analysis and knowledge graphs for suicide risk prediction</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/co-occurrence-analysis-and-knowledge-graphs-for-suicide-risk-prediction/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/zy6DD7-H_bg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="unraveling-the-complexities-of-mental-health-diagnosis-with-r-nlpembeds-and-kgraph" class="level1">
<h1>Unraveling the Complexities of Mental Health Diagnosis with R: <code>nlpembeds</code> and <code>kgraph</code></h1>
<p>In the ever-evolving landscape of mental health research, the integration of technology and data analytics has opened new horizons for understanding and diagnosing complex mental health disorders. At the forefront of this intersection is Thomas Charlon from Harvard Medical School, whose recent presentation at R/Medicine 2025 highlighted groundbreaking tools designed to tackle the intricacies of mental health diagnosis. These tools, encapsulated in the open-source R packages <code>nlpembeds</code> and <code>kgraph</code>, provide a novel approach to processing and analyzing electronic health records (EHR), especially unstructured text data such as clinician notes.</p>
<section id="the-challenge-of-mental-health-diagnosis" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge-of-mental-health-diagnosis">The Challenge of Mental Health Diagnosis</h2>
<p>Mental health disorders are notoriously complex, characterized by overlapping symptoms and subtle variations that can be difficult to discern through traditional diagnostic methods. Current diagnostic frameworks often fail to capture the full spectrum and gradients of these disorders, leading to underdiagnoses or misdiagnoses. This is where Natural Language Processing (NLP) comes into play, offering new opportunities to enhance diagnostic accuracy by leveraging the rich, unstructured data found in clinician notes.</p>
</section>
<section id="the-role-of-celehs-and-the-csrp-project" class="level2">
<h2 class="anchored" data-anchor-id="the-role-of-celehs-and-the-csrp-project">The Role of CELEHS and the CSRP Project</h2>
<p>Charlon’s work is anchored in the CELEHS laboratory at Harvard, which is part of the Center for Suicide Research and Prevention (CSRP) project led by Massachusetts General Hospital. This initiative aims to develop tools that help clinicians assess suicide risk more accurately. A significant challenge addressed by the project is the limitations of the International Classification of Diseases (ICD) in identifying suicide attempts, which often results in underestimations of their prevalence.</p>
</section>
<section id="two-pronged-approach-survival-models-and-nlp-techniques" class="level2">
<h2 class="anchored" data-anchor-id="two-pronged-approach-survival-models-and-nlp-techniques">Two-Pronged Approach: Survival Models and NLP Techniques</h2>
<p>The CELEHS team’s contribution to the CSRP project is twofold. First, they focus on developing robust survival models on codified data that can be transferred between institutions like MGH and Cambridge Health Alliance. Second, they leverage advanced NLP techniques, including name-entity recognition, co-occurrence analysis, and knowledge graphs, to extract deeper insights from clinician notes.</p>
<p>Charlon’s presentation underscored the analysis of cohorts comprising approximately 5,000 teenage patients admitted to psychiatric departments. This analysis is pivotal in understanding the nuances of mental health disorders among adolescents, a demographic that is particularly vulnerable to mental health challenges.</p>
</section>
<section id="introducing-nlpembeds-and-kgraph" class="level2">
<h2 class="anchored" data-anchor-id="introducing-nlpembeds-and-kgraph">Introducing <code>nlpembeds</code> and <code>kgraph</code></h2>
<p>The tools developed by Charlon and his team, <code>nlpembeds</code> and <code>kgraph</code>, are both available on CRAN, the comprehensive R archive network. These packages were introduced in Charlon’s previous talk at R/Medicine 2024, where he discussed “Word embeddings in mental health.” The methods underlying these packages enable the efficient analysis of large volumes of EHR data, both codified and unstructured.</p>
<section id="nlpembeds-harnessing-the-power-of-nlp" class="level3">
<h3 class="anchored" data-anchor-id="nlpembeds-harnessing-the-power-of-nlp"><code>nlpembeds</code>: Harnessing the Power of NLP</h3>
<p>The <code>nlpembeds</code> package focuses on transforming unstructured text data into structured insights. By utilizing techniques like word embeddings, which are akin to the Word2Vec algorithm, the package allows researchers to analyze the relationships between different medical concepts as documented in clinician notes. This transformation helps in identifying patterns and correlations that might not be evident in codified data alone.</p>
</section>
<section id="kgraph-visualizing-complex-data-relationships" class="level3">
<h3 class="anchored" data-anchor-id="kgraph-visualizing-complex-data-relationships"><code>kgraph</code>: Visualizing Complex Data Relationships</h3>
<p>Complementing <code>nlpembeds</code>, the <code>kgraph</code> package offers powerful visualization capabilities for the data relationships uncovered through NLP analysis. By constructing knowledge graphs, researchers can visually explore the connections between various medical concepts, enhancing their ability to interpret complex data sets. This is particularly useful in mental health research, where understanding the interplay between different symptoms and diagnoses is crucial.</p>
</section>
</section>
<section id="real-world-applications-and-insights" class="level2">
<h2 class="anchored" data-anchor-id="real-world-applications-and-insights">Real-World Applications and Insights</h2>
<p>Charlon shared compelling examples of how these tools can be applied in real-world settings. One notable case involved the interpretation of acetaminophen’s role in predicting suicide attempts. Initially perceived as a spurious correlation, further analysis using NLP techniques revealed its association with overdose and intoxication in clinician notes, suggesting a genuine predictive value.</p>
<p>Such insights underscore the importance of considering both codified and unstructured data in mental health research. By integrating these two data types, the tools developed by Charlon and his team provide a more comprehensive view of patient health, enabling more accurate risk assessments and ultimately, better patient outcomes.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Thomas Charlon’s work demonstrates the power of R and open-source tools in unraveling the complexities of mental health diagnosis. The <code>nlpembeds</code> and <code>kgraph</code> packages are not only a testament to the potential of NLP in healthcare but also a call to the R community to continue pushing the boundaries of what is possible with data analytics.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <category>Healthcare</category>
  <guid>https://r-consortium.org/posts/co-occurrence-analysis-and-knowledge-graphs-for-suicide-risk-prediction/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/co-occurrence-analysis-and-knowledge-graphs-for-suicide-risk-prediction/thumbnail-charlon-co-occurence-analysis.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Dengue Forecasting Addressing the Interrupted Effect from COVID-19 Cases</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/dengue-forecasting-addressing-the-interrupted-effect-from-covid-19-cases/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/TNRH2WxA3J0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><em>Note: This project is <a href="http://localhost:4215/posts/empowering-dengue-research-through-the-dengue-data-hub/index.html">funded by the R Consortium</a></em></p>
<section id="navigating-interruption-forecasting-dengue-cases-amidst-covid-19" class="level1">
<h1>Navigating Interruption: Forecasting Dengue Cases Amidst COVID-19</h1>
<p>Dengue fever, a mosquito-borne disease, remains a significant health concern in tropical regions, particularly in countries near the equator. The dengue virus, primarily transmitted by Aedes mosquitoes, thrives in warm, humid conditions with regular rainfall—conditions that are prevalent in these regions. However, the onset of the COVID-19 pandemic introduced an unprecedented interruption in the usual dengue case patterns. This blog post delves into a study that explores how to accurately forecast dengue cases amidst the interruptions caused by the COVID-19 pandemic, using Sri Lanka as a case study.</p>
<p><strong>Understanding the Interruption</strong></p>
<p>During the COVID-19 pandemic, several factors contributed to an unusual pattern in dengue case reporting. These include:</p>
<ul>
<li>Misclassification of dengue as COVID-19 due to overlapping symptoms such as fever, headache, and fatigue.</li>
<li>Underreporting or delayed reporting due to lockdowns and mobility restrictions.</li>
<li>Reduced human-mosquito contact due to people spending more time indoors.</li>
<li>School and workplace closures, which are common sites for dengue transmission.</li>
<li>Travel restrictions, which reduced the spread of dengue to new areas.</li>
</ul>
<p>This period, referred to as the “interrupted period,” significantly affected the usual seasonal and annual patterns of dengue cases.</p>
<p><strong>Forecasting Strategies</strong></p>
<p>The study, presented by Thiyanga S. Talagala from the Department of Statistics at the University of Sri Jayewardenepura, Sri Lanka, investigates three modeling strategies to address this interruption in dengue case forecasting:</p>
<ol type="1">
<li><p><strong>Excluding the Interrupted Period</strong>: This approach involves using only post-COVID-19 data for model training, effectively ignoring the data from the interrupted period.</p></li>
<li><p><strong>Forecasting the Interrupted Period First</strong>: This method involves forecasting the interrupted period based on data up to 2019, then using the updated time series for model training.</p></li>
<li><p><strong>Down-Weighting the Interrupted Period</strong>: This strategy assigns lower weights to data points in the interrupted period, giving higher weights to uninterrupted periods.</p></li>
</ol>
<p>Data from 2007 to 2024 were used for model fitting, and data for 2025 served as the test set to evaluate the performance of these methods across 25 districts in Sri Lanka.</p>
<p><strong>Evaluating the Methods</strong></p>
<p>The study employed various accuracy measures, including Root Mean Square Error (RMSE) and Mean Absolute Error (MAE), to compare the forecasting performance of each approach. The findings revealed that no single method outperformed across all districts. Instead, the effectiveness of each approach depended on the specific characteristics and historical patterns of each district.</p>
<p><strong>Insights and Implications</strong></p>
<p>The study’s insights underscore the importance of tailoring forecasting methods to the unique characteristics of each region. For instance, the down-weighting approach proved effective in areas where the usual dengue patterns persisted despite the interruption. In contrast, excluding the interrupted period worked best in districts that had shifted to a new normal post-COVID-19.</p>
<p>Furthermore, the study highlighted the influence of weather patterns on dengue transmission. Districts affected by specific monsoon periods or characterized by unique weather conditions showed distinct forecasting patterns.</p>
<p><strong>Conclusion</strong></p>
<p>Forecasting dengue cases amidst interruptions like COVID-19 is a complex task that requires adaptive approaches. The study by Talagala emphasizes that understanding the local context, including weather patterns and historical data, is crucial for accurate forecasting. This research not only contributes to improving dengue preparedness but also offers valuable insights for handling future public health interruptions.</p>
<p>For more detailed insights and to explore the methodologies used, you can access <a href="https://github.com/thiyangt/denguedatahub">the project on GitHub</a> and <a href="https://denguedatahub.netlify.app/">the main Dengue Data Hub site</a>.</p>


</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Epidemiology</category>
  <category>Healthcare</category>
  <guid>https://r-consortium.org/posts/dengue-forecasting-addressing-the-interrupted-effect-from-covid-19-cases/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/dengue-forecasting-addressing-the-interrupted-effect-from-covid-19-cases/thumbnail-dengue.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Ethical Considerations of Contrasts in Statistical Modeling of Medical Equity</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/ethical-considerations-of-contrasts-in-statistical-modeling-of-medical-equity/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/pAx92roI3VE" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="ethical-choices-in-regression-analysis-a-case-study-from-seattle-childrens-hospital" class="level2">
<h2 class="anchored" data-anchor-id="ethical-choices-in-regression-analysis-a-case-study-from-seattle-childrens-hospital">Ethical Choices in Regression Analysis: A Case Study from Seattle Children’s Hospital</h2>
<p>In the world of statistical modeling, the choice of coding schemes for categorical variables is not merely a technical consideration but a decision laden with ethical implications. This was the focus of a recent presentation during the R/Medicine 2025 conference by Dwight Barry, Principal Data Scientist at Seattle Children’s Hospital, and Nicole Chicoine, DO, MPH, also at Seattle Children’s Hospital. The talk revolved around how these choices can influence the inferences drawn from regression analyses and ultimately impact healthcare delivery and research.</p>
<section id="the-hospitals-language-diversity" class="level3">
<h3 class="anchored" data-anchor-id="the-hospitals-language-diversity">The Hospital’s Language Diversity</h3>
<p>Seattle Children’s Hospital is a bustling 400-bed facility that handles over half a million patient visits annually. With such a diverse patient base, the hospital encounters more than 130 languages, with Spanish being the most common after English. To accommodate this linguistic diversity, the hospital has initiated its first all-Spanish speaking operating room, ensuring equitable care for non-English speaking patients. This commitment to inclusivity is mirrored in their research methodologies, where the focus is on equitable outcomes across different patient groups.</p>
</section>
<section id="understanding-coding-schemes" class="level3">
<h3 class="anchored" data-anchor-id="understanding-coding-schemes">Understanding Coding Schemes</h3>
<p>The choice of coding schemes in regression models is crucial as it can shape the conclusions drawn from the data. Barry highlighted three coding schemes used in R: treatment contrast, sum contrast, and weighted effect coding. Each scheme presents categorical variables in different ways, affecting the interpretation of the data.</p>
<ol type="1">
<li><p><strong>Treatment Contrast</strong>: This is the default coding in R, where one category is used as a reference against which others are compared. In a clinical setting, this could inadvertently privilege a particular group, often aligning with the English-speaking, white demographic, which can skew the narrative towards existing inequities.</p></li>
<li><p><strong>Sum Contrast</strong>: Here, the grand mean of all categories is used as the reference point. This approach decouples any single category from being the norm, promoting a more balanced view. In the context of healthcare, it shifts the focus from a single dominant group to an aggregate understanding, which can be crucial in addressing biases.</p></li>
<li><p><strong>Weighted Effect Coding</strong>: This method is a variant of the sum contrast, where each category level is weighted by its sample size. Although not a built-in feature in base R, the <code>wec</code> package facilitates its implementation. This approach provides a nuanced view by factoring in the prevalence of each category, which can be especially useful in diverse patient populations.</p></li>
</ol>
</section>
<section id="implications-for-healthcare-research" class="level3">
<h3 class="anchored" data-anchor-id="implications-for-healthcare-research">Implications for Healthcare Research</h3>
<p>The choice of coding scheme is not just a statistical decision but an ethical one, as it affects how healthcare equity is perceived and addressed. Barry’s presentation underscored that while odds ratios may differ across coding schemes, the marginal effects remain consistent, suggesting that predictions are unaffected by these choices. However, the ethical ramifications are significant, as they influence which group is centered in the analysis.</p>
<p>In healthcare research, where categorical exposures like language group, race, or ethnicity lack a natural order, choosing the right coding scheme is vital. Sum contrasts, for instance, provide a means to avoid privileging any group, thereby promoting equity.</p>
</section>
<section id="broader-implications-and-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="broader-implications-and-recommendations">Broader Implications and Recommendations</h3>
<p>Barry’s insights extend beyond surgery to any healthcare condition where equity is a concern. The presentation emphasized the importance of presenting marginal effects alongside regression coefficients or odds ratios to provide a comprehensive view of the data. By decentering from a single reference point, researchers can challenge the dominant social narratives and highlight systemic inequities, paving the way for more inclusive and equitable healthcare practices.</p>
<p>In conclusion, the ethical dimensions of statistical modeling are as crucial as the statistical ones. As healthcare becomes increasingly data-driven, recognizing and addressing these ethical considerations is essential. By adopting coding schemes that promote equity, researchers and healthcare providers can ensure that their work contributes to a more just and equitable healthcare system.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Healthcare</category>
  <guid>https://r-consortium.org/posts/ethical-considerations-of-contrasts-in-statistical-modeling-of-medical-equity/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/ethical-considerations-of-contrasts-in-statistical-modeling-of-medical-equity/thumbnail-ethical-considerations.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Improving Reproducibility of Medical Research with Controlled Vocabularies</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/improving-reproducibility-of-medical-research-with-controlled-vocabularies/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/bf8NZTnz6NA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="enhancing-reproducibility-in-medical-research-with-controlled-vocabularies" class="level1">
<h1>Enhancing Reproducibility in Medical Research with Controlled Vocabularies</h1>
<p>Hello R community! Today, let’s delve into an intriguing topic presented by Jonathan Pearce, a Senior Analyst at the Analysis Group, at the R/Medicine 2025 conference. Jonathan’s insightful presentation focused on improving the reproducibility of medical research through the use of controlled vocabularies for variable naming. Reproducibility in medical research is crucial, and Jonathan’s approach offers a structured methodology to enhance clarity and efficiency in data analysis.</p>
<section id="the-need-for-reproducibility" class="level2">
<h2 class="anchored" data-anchor-id="the-need-for-reproducibility">The Need for Reproducibility</h2>
<p>The reproducibility of medical research has been a growing concern. Successful replication of studies depends on various factors such as code correctness, thorough documentation, and clear communication of the methods used. While much emphasis is often placed on technical aspects like coding environments and data access, the crux of reproducibility lies in the precise and consistent implementation of the original work.</p>
</section>
<section id="introducing-controlled-vocabularies" class="level2">
<h2 class="anchored" data-anchor-id="introducing-controlled-vocabularies">Introducing Controlled Vocabularies</h2>
<p>Jonathan introduced the concept of controlled vocabularies for variable naming as a means to bolster reproducibility. This involves using a schema to label variables in a dataset consistently. This systematic approach encodes metadata directly into variable names, providing immediate context and enhancing the clarity of the data.</p>
<section id="example-of-controlled-vocabulary" class="level3">
<h3 class="anchored" data-anchor-id="example-of-controlled-vocabulary">Example of Controlled Vocabulary</h3>
<p>Consider a scenario where you have a dataset with multiple tables, each containing various types of data. For instance, if you’re tracking whether patients had an eGFR lab test during a baseline period, a variable name following a controlled vocabulary might be <code>labs_eGFR_baseline_ind</code>, where <code>ind</code> stands for indicator. This descriptive naming convention helps users instantly understand the data stored in the column.</p>
</section>
<section id="structured-naming-and-its-advantages" class="level3">
<h3 class="anchored" data-anchor-id="structured-naming-and-its-advantages">Structured Naming and Its Advantages</h3>
<p>Controlled vocabularies mandate a consistent format across the project, which can be crucial for downstream analyses. For example, a variable capturing the median value of an eGFR test might be named <code>labs_eGFR_baseline_median_value</code>, providing additional clarity such as the unit of measurement and the calculation method.</p>
<p>The benefits of controlled vocabularies extend to various facets of data analysis:</p>
<ul>
<li><strong>Regular Expressions:</strong> With consistently named variables, regular expressions can efficiently query subsets of data, facilitating tasks like data validation and report generation.</li>
<li><strong>Data Validation:</strong> By defining expected data types and constraints (e.g., numeric values for duration variables should be non-negative), controlled vocabularies simplify the validation process.</li>
<li><strong>Streamlined Workflows:</strong> Tasks such as creating summary tables, modeling, sensitivity analysis, and generating data dictionaries become more straightforward and reproducible.</li>
</ul>
</section>
</section>
<section id="practical-considerations" class="level2">
<h2 class="anchored" data-anchor-id="practical-considerations">Practical Considerations</h2>
<p>While controlled vocabularies offer significant advantages, there are practical considerations to keep in mind:</p>
<ul>
<li><strong>Balancing Detail and Brevity:</strong> It’s essential to avoid overly long variable names by using abbreviations judiciously.</li>
<li><strong>Initial Setup Effort:</strong> Defining a controlled vocabulary requires upfront work, but the long-term benefits often outweigh the initial investment.</li>
<li><strong>Avoiding Conflicts:</strong> Care must be taken with underscores and keywords to prevent conflicts during keyword searches in variable names.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Jonathan Pearce’s presentation highlighted the transformative potential of controlled vocabularies in enhancing the reproducibility of medical research. By embedding metadata directly within variable names, researchers can achieve greater clarity and consistency, ultimately leading to more reliable and efficient data analysis.</p>
<p>As we strive to make our work more sustainable and transparent, adopting practices like controlled vocabularies can help ensure that our research remains accessible and understandable, even months or years later.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <guid>https://r-consortium.org/posts/improving-reproducibility-of-medical-research-with-controlled-vocabularies/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/improving-reproducibility-of-medical-research-with-controlled-vocabularies/thumbnail-reproducibility-pearce.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Mix, Pour, Share: The rUM Cocktail for Biomedical Project Packaging</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/mix-pour-share-the-rum-cocktail-for-biomedical-project-packaging/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/03_5KrQA-mo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="unveiling-rum-a-game-changer-for-biomedical-reproducibility" class="level1">
<h1>Unveiling rUM: A Game-Changer for Biomedical Reproducibility</h1>
<p>The reproducibility crisis in biomedical research is a formidable challenge that demands innovative solutions. At the forefront of addressing this issue is the rUM package, developed at the University of Miami, which promises to revolutionize how biomedical research projects are documented, shared, and analyzed. Here, we delve into the capabilities of <a href="https://raymondbalise.github.io/rUM/">rUM version 2.2</a>, named “rUM Runner,” and its potential to streamline research processes while promoting reproducibility.</p>
<section id="meet-the-innovators" class="level2">
<h2 class="anchored" data-anchor-id="meet-the-innovators">Meet the Innovators</h2>
<p>Kyle Gealis and Dr.&nbsp;Raymond Balise from the University of Miami’s Department of Public Health Sciences are the developers of rUM. They aim to bridge the gap between complex research needs and user-friendly tools, ensuring that even researchers with minimal programming experience can produce high-quality, reproducible work.</p>
</section>
<section id="what-is-rum" class="level2">
<h2 class="anchored" data-anchor-id="what-is-rum">What is rUM?</h2>
<p>While “rum” might first bring to mind thoughts of a tropical cocktail, in the context of biomedical research, rUM is an acronym for an R package designed to make research reproducibility more accessible. It facilitates the creation of comprehensive, CRAN-ready research packages with minimal coding effort. The package allows researchers to bundle entire projects, complete with analyses, datasets, documentation, and presentations, into a single distributable package.</p>
</section>
<section id="key-features-of-rum" class="level2">
<h2 class="anchored" data-anchor-id="key-features-of-rum">Key Features of rUM</h2>
<ol type="1">
<li><p><strong>CRAN-Ready Package Structures</strong>: With a single function call, rUM creates package structures that adhere to CRAN standards, simplifying the package development process.</p></li>
<li><p><strong>Automated Templates</strong>: R Markdown and Quarto manuscripts can be seamlessly integrated as package vignettes, providing a cohesive documentation of research efforts.</p></li>
<li><p><strong>Enhanced Dataset Documentation</strong>: rUM includes tools for documenting datasets with comprehensive metadata, ensuring that datasets are well-described and easy to understand.</p></li>
<li><p><strong>Presentation Integration</strong>: With rUM, creating Quarto RevealJS presentations within packages is straightforward. This feature allows researchers to share their findings in a visually engaging manner.</p></li>
<li><p><strong>Slide Deck Display and Sharing</strong>: Researchers can easily display and share slide decks stored within their packages, enhancing collaborative communication.</p></li>
</ol>
</section>
<section id="addressing-the-reproducibility-crisis" class="level2">
<h2 class="anchored" data-anchor-id="addressing-the-reproducibility-crisis">Addressing the Reproducibility Crisis</h2>
<p>The reproducibility crisis highlights the inconsistencies often found when attempting to replicate research findings, either with new data or the original datasets. rUM addresses this by providing a systematic approach to project organization, ensuring that all elements of the research process are documented and reproducible.</p>
<section id="the-workflow-from-data-to-package" class="level3">
<h3 class="anchored" data-anchor-id="the-workflow-from-data-to-package">The Workflow: From Data to Package</h3>
<p>The rUM package facilitates a complete workflow:</p>
<ul>
<li><p><strong>Analyzing Data</strong>: Take, for example, the pharmacokinetic dataset (medicaldata::theophylline). rUM assists in analyzing such datasets with integrated tools.</p></li>
<li><p><strong>Documenting with Metadata</strong>: Datasets are documented with comprehensive metadata, making them easier to understand and use by others.</p></li>
<li><p><strong>Creating Presentations</strong>: Researchers can create presentation slides featuring their analysis visualizations, making it easier to communicate findings.</p></li>
<li><p><strong>Bundling into a Package</strong>: Finally, all elements are bundled into a single, distributable package that is discoverable and reusable, addressing critical elements of reproducibility in medical research.</p></li>
</ul>
</section>
</section>
<section id="hands-on-with-rum" class="level2">
<h2 class="anchored" data-anchor-id="hands-on-with-rum">Hands-On with rUM</h2>
<p>Kyle Gealis and Dr.&nbsp;Raymond Balise showcased the practical application of rUM during their presentation, guiding attendees through the steps of installing rUM, creating packages, and developing presentations. They demonstrated how to edit the R profile, initialize a package project, and navigate through various functionalities such as adding licenses, checking package integrity, and documenting datasets.</p>
<section id="creating-and-sharing-presentations" class="level3">
<h3 class="anchored" data-anchor-id="creating-and-sharing-presentations">Creating and Sharing Presentations</h3>
<p>One of the standout features of rUM is its ability to create and share presentations directly from the package. Researchers can build slide decks and integrate them into their packages, making it easier to disseminate research findings.</p>
</section>
</section>
<section id="new-features-in-rum-runner" class="level2">
<h2 class="anchored" data-anchor-id="new-features-in-rum-runner">New Features in rUM Runner</h2>
<p>The new version of rUM introduces several enhancements aimed at improving collaborative communication and documentation:</p>
<ul>
<li><strong>Write Notes</strong>: Create dated progress notes to keep track of project developments.</li>
<li><strong>Readme Templates</strong>: Generate structured readme files, guiding users through the project’s contents and processes.</li>
<li><strong>Quarto Documents and SCSS</strong>: Write Quarto documents and add custom SCSS for personalized styling.</li>
</ul>
</section>
<section id="an-invitation-to-the-r-community" class="level2">
<h2 class="anchored" data-anchor-id="an-invitation-to-the-r-community">An Invitation to the R Community</h2>
<p>The developers of rUM invite the R community to explore and contribute to the package. They encourage feedback, ideas for new templates, and even pull requests on GitHub. The goal is to continuously enhance rUM’s functionality, ensuring it meets the evolving needs of the biomedical research community.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>rUM Runner is more than just a tool; it’s a step towards a more reproducible future in biomedical research. By simplifying the process of creating comprehensive research packages, rUM empowers researchers to focus on science while ensuring their work is transparent, discoverable, and reusable. Whether you’re a seasoned programmer or new to R, rUM offers a pathway to enhancing the reproducibility and impact of your research.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <category>Healthcare</category>
  <guid>https://r-consortium.org/posts/mix-pour-share-the-rum-cocktail-for-biomedical-project-packaging/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/mix-pour-share-the-rum-cocktail-for-biomedical-project-packaging/thumbnail-rum-gealis-balise.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Model Evaluation: From Machine Learning to Generative AI</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/model-evaluation-from-machine-learning-to-generative-ai/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Lq568n0pClc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="exploring-the-future-of-ai-evaluation-with-dr.-erin-ledell-at-rmedicine-2025" class="level1">
<h1>Exploring the Future of AI Evaluation with Dr.&nbsp;Erin LeDell at R/Medicine 2025</h1>
<p>As artificial intelligence evolves, so must the methodologies used to evaluate these systems. Dr.&nbsp;Erin LeDell, a prominent figure in the AI and R communities, addressed this critical transition in her keynote at R/Medicine 2025. Dr.&nbsp;LeDell, the Chief Scientist at Distributional, Inc.&nbsp;and founder of DataScientific, Inc., brings her extensive expertise to the forefront as she guides us through the intricate world of AI evaluation, moving from deterministic machine learning models to the more complex generative AI systems.</p>
<section id="from-deterministic-to-generative-a-paradigm-shift" class="level2">
<h2 class="anchored" data-anchor-id="from-deterministic-to-generative-a-paradigm-shift">From Deterministic to Generative: A Paradigm Shift</h2>
<p>In traditional machine learning (ML), models are deterministic – given the same input, they produce the same output. This predictability provides a clear framework for evaluation metrics such as accuracy, precision, recall, and others familiar to statisticians and data scientists. However, with the rise of large language models (LLMs) and generative AI, this predictability is challenged. These systems introduce non-determinism, meaning outputs can vary even with identical inputs, necessitating new evaluation frameworks.</p>
<p>Dr.&nbsp;LeDell emphasized that traditional accuracy-based metrics fall short when assessing generative AI systems. Instead, evaluation must consider coherence, consistency, and bias, along with the challenges of reproducibility in probabilistic AI systems. This shift leads to questions about how to ensure AI models are reliable and function as expected in real-world scenarios.</p>
</section>
<section id="dr.-erin-ledells-journey-with-r-and-ai" class="level2">
<h2 class="anchored" data-anchor-id="dr.-erin-ledells-journey-with-r-and-ai">Dr.&nbsp;Erin LeDell’s Journey with R and AI</h2>
<p>Dr.&nbsp;LeDell shared her journey from machine learning to generative AI, highlighting her longstanding experience with R. Since 2008, she has been deeply involved in machine learning, contributing to various R packages such as SuperLearner, Subsemble, and H2O, the latter of which she worked on extensively during her tenure at H2O.ai. Her work in AutoML led to the creation of the AutoML benchmark, setting standards for algorithm evaluation.</p>
<p>Her transition into generative AI coincided with the advent of tools like ChatGPT, pushing her to explore new methods for evaluating these complex systems. Dr.&nbsp;LeDell’s passion for using AI in healthcare was evident as she discussed her collaborations with medical companies, applying machine learning to tackle various health-related problems.</p>
</section>
<section id="evaluating-generative-ai-new-approaches" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-generative-ai-new-approaches">Evaluating Generative AI: New Approaches</h2>
<p>The talk delved into the architectural differences between traditional ML systems and modern AI applications, particularly generative AI systems. Dr.&nbsp;LeDell outlined the multi-component nature of these systems, where changes in one part can affect the whole, and the importance of monitoring these changes over time. She addressed the non-stationary behavior of generative AI, noting how external factors and updates from third-party providers can alter system performance.</p>
<p>A significant challenge with generative AI is its inherent non-determinism. Unlike traditional ML models, generative AI requires novel evaluation metrics that account for variability in outputs. Dr.&nbsp;LeDell introduced several frameworks and tools aimed at assessing these systems, emphasizing the role of humans in the evaluation process. Humans provide essential oversight, creating “golden” datasets and evaluating outputs, though this process is not always scalable.</p>
</section>
<section id="llm-as-judge-a-new-standard" class="level2">
<h2 class="anchored" data-anchor-id="llm-as-judge-a-new-standard">LLM as Judge: A New Standard</h2>
<p>One innovative approach Dr.&nbsp;LeDell highlighted is using LLMs themselves to evaluate AI outputs. This method involves deploying LLMs as judges to assess whether responses are correct, helpful, or safe. While this technique is automated and widely used, it presents challenges, such as potential biases if the same model is used for generation and evaluation. Dr.&nbsp;LeDell recommended using specialized LLMs designed for evaluation to mitigate these issues.</p>
</section>
<section id="practical-applications-and-tools" class="level2">
<h2 class="anchored" data-anchor-id="practical-applications-and-tools">Practical Applications and Tools</h2>
<p>Dr.&nbsp;LeDell provided insights into practical applications of generative AI in healthcare, such as using LLMs for clinical note-taking and research acceleration. She described a retrieval-augmented generation (RAG) system for medical Q&amp;A, which combines traditional information retrieval with generative capabilities, enriching AI responses with context from specialized knowledge bases.</p>
<p>For those eager to explore AI evaluation further, Dr.&nbsp;LeDell pointed to several open-source tools, including the R package “vitals,” a port of the Python library “inspect,” developed by JJ Allaire. These tools provide a foundation for customizing evaluation metrics and integrating human oversight into the evaluation pipeline.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Dr.&nbsp;LeDell’s keynote at R/Medicine 2025 illuminated the evolving landscape of AI evaluation, underscoring the need for innovative methodologies to assess the next generation of AI models. Her insights into the intersection of AI and healthcare offer promising pathways for improving AI reliability and trustworthiness in critical applications.</p>
<p>As the R community continues to embrace these advancements, Dr.&nbsp;LeDell’s work encourages practitioners to think critically and creatively about AI evaluation.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>AI</category>
  <guid>https://r-consortium.org/posts/model-evaluation-from-machine-learning-to-generative-ai/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/model-evaluation-from-machine-learning-to-generative-ai/thumbnail-keynote-day2-model-evaluation-ledell.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>No More Copy-Paste: Automating Patient Inquiry Tracking in Pharma with Shiny</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/no-more-copy-paste-automating-patient-inquiry-tracking-in-pharma-with-shiny/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/sRNxjGXZ0_0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="automating-patient-inquiry-tracking-in-pharma-with-shiny-a-game-changer-in-specialty-pharma-coordination" class="level1">
<h1>Automating Patient Inquiry Tracking in Pharma with Shiny: A Game-Changer in Specialty Pharma Coordination</h1>
<p>In the fast-paced world of specialty pharma coordination, the seamless flow of information and timely delivery of medication can be a matter of life and death for patients with rare diseases. The challenge? Managing patient support and prescription workflows often involves disparate tools, manual data wrangling, and time-consuming reporting. Tanya Cashorali, the founder of TCB Analytics, addressed this critical issue in her presentation at the R/Medicine conference, showcasing how an R Shiny application revolutionized these processes.</p>
<section id="meet-the-speaker-tanya-cashorali" class="level2">
<h2 class="anchored" data-anchor-id="meet-the-speaker-tanya-cashorali">Meet the Speaker: Tanya Cashorali</h2>
<p>Tanya Cashorali is the founder of TCB Analytics, a Boston-based data science consultancy with a strong focus on bio and pharma. She has an impressive background in biotech, having started her career analyzing genetic data at Children’s Hospital in 2005. Tanya has since contributed to building molecular applications at Dana Farber, conducted modeling work at GNS Healthcare to understand causal mechanisms of disease, and helped launch TCB Analytics after her tenure at Biogen.</p>
</section>
<section id="the-problem-manual-mayhem-in-specialty-pharma-coordination" class="level2">
<h2 class="anchored" data-anchor-id="the-problem-manual-mayhem-in-specialty-pharma-coordination">The Problem: Manual Mayhem in Specialty Pharma Coordination</h2>
<p>For bio and pharma companies treating rare diseases, ensuring patients receive their medications on time is crucial. However, the process often involves multiple manual steps:</p>
<ul>
<li>Physicians write prescriptions that go to specialty pharmacies.</li>
<li>Biopharma companies ensure the availability of drugs.</li>
<li>Specialty pharmacies ship the drugs to patients.</li>
</ul>
<p>This workflow was previously managed through Excel sheets and lengthy email chains. The result? Manual mayhem, with room for errors, slow case preparation, and time-consuming weekly status meetings.</p>
</section>
<section id="the-solution-an-r-shiny-application" class="level2">
<h2 class="anchored" data-anchor-id="the-solution-an-r-shiny-application">The Solution: An R Shiny Application</h2>
<p>To tackle this challenge, Tanya and her team developed an R Shiny application that serves as a dynamic operational hub. This application integrates prescription fulfillment data with Smartsheet-tracked patient inquiries, providing a unified, interactive dashboard for users. Key features include:</p>
<ul>
<li><strong>Real-time Data Integration:</strong> The application merges daily Excel-based prescription data with live Smartsheet API feeds.</li>
<li><strong>Interactive Dashboard:</strong> Users can filter, sort, and export patient records while tracking key support metrics such as shipment history and open ticket status.</li>
<li><strong>Ticket Creation and Management:</strong> Users can create and manage tickets directly from the application, which syncs with Smartsheet to streamline communication between the market access team and specialty pharmacies.</li>
</ul>
</section>
<section id="impact-and-efficiency-gains" class="level2">
<h2 class="anchored" data-anchor-id="impact-and-efficiency-gains">Impact and Efficiency Gains</h2>
<p>The impact of this R Shiny application has been significant:</p>
<ul>
<li><strong>Streamlined Communication:</strong> The application replaces five daily emails, reduces meeting times from 30 to 10 minutes, and supports over 36 users, including sales teams and market access teams.</li>
<li><strong>Real-time Insights:</strong> The application provides a single source of real-time truth, enabling more productive case tracking and management.</li>
<li><strong>Data-Driven Decisions:</strong> With 400 patients and two drug products tracked, the application offers valuable data insights, such as time to resolution and issue frequency across specialty pharmacies.</li>
</ul>
</section>
<section id="lessons-learned" class="level2">
<h2 class="anchored" data-anchor-id="lessons-learned">Lessons Learned</h2>
<p>Tanya shared valuable lessons learned from the project:</p>
<ul>
<li><strong>Start Simple:</strong> Begin with sketches and iterate based on user feedback.</li>
<li><strong>Build Dashboards with People, Not Just for Them:</strong> Engage users early and often to ensure the solution meets their needs.</li>
<li><strong>Leverage Power Users:</strong> Identify and collaborate with power users who can provide valuable feedback and drive adoption.</li>
<li><strong>Avoid Overengineering:</strong> Use existing tools like R Shiny, Smartsheet, and pins to create efficient solutions without complex infrastructure.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The R Shiny application developed by TCB Analytics exemplifies the power of R in transforming workflows and improving patient outcomes in the pharma industry. By automating patient inquiry tracking, the application not only streamlines operations but also ensures that patients receive their medications promptly, ultimately saving lives.</p>
<p>As Tanya summarized, this project demonstrates that you don’t need a big vendor platform or a six-month roadmap to make a significant impact. With the right tools and a focus on user needs, R can drive meaningful change in healthcare and beyond.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Pharma</category>
  <category>Healthcare</category>
  <guid>https://r-consortium.org/posts/no-more-copy-paste-automating-patient-inquiry-tracking-in-pharma-with-shiny/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/no-more-copy-paste-automating-patient-inquiry-tracking-in-pharma-with-shiny/thumbnail-no-more-copy-paste-tanya.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>nonprobsvy – An R package for modern methods for non-probability survey</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/nonprobsvy-an-r-package-for-modern-methods-for-non-probability-survey/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/MnEZlFcpmCE" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="unveiling-the-nonprobsvy-package-a-leap-in-non-probability-sample-inference-in-r" class="level1">
<h1>Unveiling the <code>nonprobsvy</code> Package: A Leap in Non-Probability Sample Inference in R</h1>
<p>Greetings R community! Today, we’re thrilled to delve into the details of <code>nonprobsvy</code>, an R package crafted for inference based on non-probability samples. Presented by Maciej Beręsewicz from the Poznań University of Economics and Business and the Statistical Office in Poznań, this package is a tool for statisticians dealing with the challenges of non-probability samples in various research domains.</p>
<section id="addressing-non-probability-sample-challenges" class="level2">
<h2 class="anchored" data-anchor-id="addressing-non-probability-sample-challenges">Addressing Non-Probability Sample Challenges</h2>
<p>The core motivation behind the development of <code>nonprobsvy</code> stems from the limitations often encountered in official statistics due to declining response rates and the growing reliance on non-probability surveys for population inference. Such surveys, including big data, opt-in web panels, and social media data, often introduce selection bias, complicating accurate population characteristic estimations.</p>
<p>Beręsewicz, deeply rooted in survey sampling and methodology, recognized these challenges through his work at the university and the Statistical Office in Poznań. This package is a testament to his commitment to providing robust statistical methods that correct selection bias, making it a resource for researchers worldwide.</p>
</section>
<section id="the-power-of-nonprobsvy" class="level2">
<h2 class="anchored" data-anchor-id="the-power-of-nonprobsvy">The Power of <code>nonprobsvy</code></h2>
<p><code>nonprobsvy</code> integrates with the popular <code>survey</code> package in R, offering a toolkit for addressing non-probability sample biases. It categorizes its approaches into three main groups: prediction-based approach, inverse probability weighting, and doubly robust approach. Here’s a closer look at what it provides:</p>
<ol type="1">
<li><strong>Inverse Probability Weighting (IPW):</strong> Allows for correction of selection bias using known population totals or survey designs.</li>
<li><strong>Mass Imputation Estimators:</strong> Employs methods such as regression imputation and nearest neighbors to estimate missing data.</li>
<li><strong>Doubly Robust Estimators:</strong> Combines the strengths of both IPW and outcome modeling for improved estimations.</li>
<li><strong>High-Dimensional Data Handling:</strong> Features variable selection using techniques like SCAD, LASSO, or MCP, crucial for handling administrative data or surveys with extensive questionnaires.</li>
</ol>
</section>
<section id="a-user-friendly-package" class="level2">
<h2 class="anchored" data-anchor-id="a-user-friendly-package">A User-Friendly Package</h2>
<p>The <code>nonprobsvy</code> package is designed with user-friendliness in mind. Its main function, <code>nonprop()</code>, mimics existing R functions by utilizing formulas, ensuring a smooth transition for users familiar with R’s ecosystem. The package supports both analytical and bootstrap variance estimators, providing flexibility and robustness in variance estimation.</p>
<section id="unique-features" class="level3">
<h3 class="anchored" data-anchor-id="unique-features">Unique Features</h3>
<ul>
<li><strong>Full Integration with the Survey Package:</strong> Ensures compatibility and extends the capabilities of existing survey methods.</li>
<li><strong>Advanced Estimators:</strong> Implements state-of-the-art methods, including those recently accepted for publication, ensuring users have access to the latest developments in survey sampling.</li>
<li><strong>Extensive Documentation:</strong> Provides detailed explanations, equations, and use cases, guiding users through implementation and interpretation.</li>
</ul>
</section>
</section>
<section id="looking-ahead-future-developments" class="level2">
<h2 class="anchored" data-anchor-id="looking-ahead-future-developments">Looking Ahead: Future Developments</h2>
<p>Beręsewicz and his team have ambitious plans for the future of <code>nonprobsvy</code>. They aim to incorporate overlapping samples, replicate weights, and expand mass imputation methods beyond parametric approaches. There is also a focus on developing inference methods for quantiles and integrating mixed-mode data handling, reflecting the dynamic needs of contemporary research.</p>
<section id="community-involvement" class="level3">
<h3 class="anchored" data-anchor-id="community-involvement">Community Involvement</h3>
<p>The development team encourages feedback and suggestions from the community. By engaging with users on GitHub, they aim to continuously refine and enhance the package. If you have innovative ideas or encounter challenges, they’re eager to hear from you.</p>
</section>
</section>
<section id="call-to-action" class="level2">
<h2 class="anchored" data-anchor-id="call-to-action">Call to Action</h2>
<p><code>nonprobsvy</code> is available on CRAN, and its development version is hosted on GitHub. We invite you to explore this powerful package, test its capabilities, and contribute to its evolution. Your insights and experiences are invaluable in shaping its future.</p>
<p>For researchers, statisticians, and R enthusiasts, <code>nonprobsvy</code> offers a robust solution to the complexities of non-probability samples. Whether you’re tackling big data, social media analytics, or opt-in web panels, this package equips you with the tools needed to derive accurate, reliable inferences.</p>
<p>To learn more about the package, visit the <a href="https://cran.r-project.org/package=nonprobsvy">CRAN page</a> or <a href="http://github.com/ncn-foreigners/nonprobsvy">GitHub repository</a>. Let’s work together in advancing statistical methodologies and making impactful contributions to the R community.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Clinical Research</category>
  <category>Software Development</category>
  <guid>https://r-consortium.org/posts/nonprobsvy-an-r-package-for-modern-methods-for-non-probability-survey/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/nonprobsvy-an-r-package-for-modern-methods-for-non-probability-survey/thumbnail-nonprobsvy.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Opening Remarks - Day 2</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/opening-remarks-day-2/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/HWJH9RiGFsc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="rmedicine-2025-day-2-a-celebration-of-r-in-medicine" class="level1">
<h1>R/Medicine 2025 Day 2: A Celebration of R in Medicine</h1>
<p>The R/Medicine 2025 conference, a flagship event brought to the community by the R Consortium, continues to be a focus of innovation and collaboration between R programming and medicine. With participants joining from diverse corners of the globe, including, just to name a few read off by Zabor during the talk, the Netherlands, Romania, London, Germany, and from US states like Florida, California, Minnesota, and Pennsylvania, the event shines as a testament to the universal appeal and utility of R in medical research and practice.</p>
<section id="acknowledgments-and-gratitude" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgments-and-gratitude">Acknowledgments and Gratitude</h2>
<p>The second day of the conference commenced with a welcome extended by Emily Zabor, the event’s chair and a dedicated biostatistician at the Cleveland Clinic.</p>
<p>Zabor emphasized that the success of R/Medicine 2025 owes much to the support of its financial sponsors, Genentech and Posit, alongside the invaluable backing of the R Consortium. Thanks are also due to the organizing committee, whose dedication and expertise have been instrumental in crafting this enriching experience. And, of course, to all participants who contribute time, knowledge and energy.</p>
</section>
<section id="recap-of-day-1-highlights" class="level2">
<h2 class="anchored" data-anchor-id="recap-of-day-1-highlights">Recap of Day 1 Highlights</h2>
<p>The first day of the conference was marked by an array of insightful presentations and discussions. Attendees were treated to a keynote by Ziad Obermeyer, who delved into the transformative potential of AI in the realm of medicine. The day unfolded with 9 regular talks and 2 lightning talks that spanned topics such as reproducibility, compliance, workflow automation, and clinical epidemiological applications.</p>
<p>A notable highlight was the presentation of competition winners in both the student and professional categories. Their analysis of vaccination and measles case rates for 2025 provided critical insights, despite the somewhat concerning data trends. The rigor and clarity of their presentations underscored the power of data-driven decision-making in public health.</p>
<section id="video-availability" class="level3">
<h3 class="anchored" data-anchor-id="video-availability">Video Availability</h3>
<p>For those who missed the live sessions, most of the presentations will be made available on the <a href="https://www.youtube.com/playlist?list=PL4IzsxWztPdmU2q31ZrTCASr78e0jpKux">R Consortium’s YouTube channel</a>. Participants are encouraged to watch for communications regarding the release of these videos, which will serve as a valuable resource for continued learning and inspiration.</p>
</section>
</section>
<section id="day-2-exciting-lineup-of-keynotes-and-talks" class="level2">
<h2 class="anchored" data-anchor-id="day-2-exciting-lineup-of-keynotes-and-talks">Day 2: Exciting Lineup of Keynotes and Talks</h2>
<p>Day Two promises another engaging lineup, starting with a keynote by Erin LeDell, focusing on model evaluation from machine learning to generative AI. This is followed by 6 regular talks and 11 lightning talks. Sessions will cover a broad spectrum of topics, including:</p>
<ul>
<li>Packages and tools for data management</li>
<li>Cohorts and APIs</li>
<li>Visualization and communication</li>
<li>Clinical and epidemiological applications</li>
<li>Statistical modeling, inference, and methodology</li>
</ul>
<p>This diversity of topics reflects the multifaceted nature of R’s applications in medicine, ensuring that there is something of interest for every attendee.</p>
</section>
<section id="community-and-networking" class="level2">
<h2 class="anchored" data-anchor-id="community-and-networking">Community and Networking</h2>
<p>The conference provides an excellent platform for networking and community building. With a few minutes between sessions, attendees are encouraged to engage in discussions and share ideas. This interaction fosters a sense of camaraderie and collaboration, essential for advancing the field.</p>
</section>
<section id="looking-ahead-future-webinars" class="level2">
<h2 class="anchored" data-anchor-id="looking-ahead-future-webinars">Looking Ahead: Future Webinars</h2>
<p>The R/Medicine conference is not limited to annual gatherings. Throughout the year, the <a href="https://r-consortium.org/webinars/webinars.html">R Consortium hosts webinars</a> that delve deeper into specific topics of interest. Last year saw two mid-year R/Medicine webinars, and this tradition of continuous learning and engagement is set to continue. Participants should stay tuned for announcements.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>R/Medicine 2025 stands as a vibrant celebration of the role of R in advancing medical research and practice. With an agenda packed with insightful talks, expert speakers, and a global community of attendees, the event embodies the spirit of innovation and collaboration. Participants are encouraged to immerse themselves in the wealth of knowledge shared and to continue engaging with the R community beyond the conference.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Events</category>
  <guid>https://r-consortium.org/posts/opening-remarks-day-2/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/opening-remarks-day-2/thumbnail-opening-remarks-day2.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Optimizing Public Healthcare Cost Recovery with R: A Use Case from Argentina</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/optimizing-public-healthcare-cost-recovery-with-r-a-use-case-from-argentina/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/BzYtr9dfHjQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="optimizing-argentinas-public-healthcare-system-with-r-a-case-study-in-efficiency-and-sustainability" class="level1">
<h1>Optimizing Argentina’s Public Healthcare System with R: A Case Study in Efficiency and Sustainability</h1>
<p>In the realm of public health, the need to do more with less is a constant challenge. This is particularly true in Argentina, where the healthcare system is fragmented into three subsystems: public, social security, and private sectors. Each of these subsystems serves different populations and has its unique funding mechanisms and challenges. However, in the spirit of equity and quality healthcare access, programs like SUMAR have emerged as vital tools for supporting the uninsured population by strengthening the public subsystem. SUMAR provides financial incentives to healthcare providers for each service rendered to individuals with exclusive public coverage, funded by external sources such as the World Bank.</p>
<p>In this context, the Ministry of Health of the City of Buenos Aires has undertaken an innovative approach to optimize cost recovery processes via the implementation of open-source tools, particularly R, showcasing a practical application of data science in public health settings. This effort is spearheaded by the Operational Management of Health Information and Statistics, a team that uses R to enhance healthcare efficiency and sustainability.</p>
<section id="understanding-argentinas-healthcare-landscape" class="level2">
<h2 class="anchored" data-anchor-id="understanding-argentinas-healthcare-landscape">Understanding Argentina’s Healthcare Landscape</h2>
<p>Argentina’s healthcare landscape is characterized by its division into three main sectors:</p>
<ol type="1">
<li><strong>Public Sector</strong>: Comprising national, provincial, and municipal hospitals offering free universal care regardless of a person’s insurance status.</li>
<li><strong>Social Security Sector</strong>: Funded through payroll and catering to workers, retirees, and individuals with disabilities, providing mandatory coverage linked to formal employment.</li>
<li><strong>Private Sector</strong>: Offering voluntary, prepaid plans for those seeking additional or alternative coverage.</li>
</ol>
<p>The SUMAR program comes into play as a national policy aimed at better financing public healthcare by tying financial incentives directly to the services provided to uninsured individuals. It operates on a results-based funding model, which allocates resources based on service provision and health outcomes, with funding sourced primarily from the World Bank.</p>
</section>
<section id="leveraging-technology-for-sustainable-healthcare" class="level2">
<h2 class="anchored" data-anchor-id="leveraging-technology-for-sustainable-healthcare">Leveraging Technology for Sustainable Healthcare</h2>
<p>The City of Buenos Aires is at the forefront of utilizing technology to optimize healthcare processes. The widespread adoption of the Electronic Health Record (EHR) system, known as the Hospital Management System (HMS), is a game-changer. HMS seamlessly integrates administrative and clinical workflows across public healthcare facilities, ensuring uniform data collection and management.</p>
<p>The data extracted from HMS is crucial for automating the SUMAR program. Each night, ETL (Extract, Transform, Load) jobs extract relevant data tables, such as patient encounters and diagnostics, which are then loaded into a central data warehouse. This setup forms the backbone of the automated SUMAR workflow in Buenos Aires.</p>
</section>
<section id="automating-the-sumar-process-with-r" class="level2">
<h2 class="anchored" data-anchor-id="automating-the-sumar-process-with-r">Automating the SUMAR Process with R</h2>
<p>The automation of the SUMAR program is a testament to the power of open-source tools like R. The data science team within the Ministry of Health employs R to automate processes and generate reports, ensuring a smooth and efficient workflow. Key outputs of this automated process include PDF invoices, weekly Excel reports, and performance indicator summaries.</p>
<p>The R-based workflow comprises three critical stages:</p>
<ol type="1">
<li><strong>Enrollment</strong>: Identifying individuals with public coverage using national registries.</li>
<li><strong>Detection of Basic Effective Coverage Services</strong>: Applying regex and logic rules across data sources to pinpoint eligible health services.</li>
<li><strong>Analysis of Health Performance Indicators</strong>: Generating comprehensive reports through R Markdown, ensuring data is transformed into actionable insights.</li>
</ol>
<p>The team’s dedicated R environment, which includes an R Studio server and GitLab for version control, facilitates collaborative development, ensuring the process is both reproducible and auditable.</p>
</section>
<section id="the-impact-of-automation-and-open-source" class="level2">
<h2 class="anchored" data-anchor-id="the-impact-of-automation-and-open-source">The Impact of Automation and Open Source</h2>
<p>The automation of the SUMAR program in Buenos Aires is not merely about efficiency—it represents a paradigm shift in how public healthcare can be managed. The steady increase in detected health services from January 2021 to April 2025 is a testament to the robustness and scalability of the automated processes. Each detected service translates into real funding, enhancing the capacity and accountability of the public healthcare system.</p>
<p>The use of R, with libraries like Tidyverse, StringR, WriteXL, and TinyTex, underscores the adaptability and sustainability of open-source solutions in public health. This approach not only saves time but also expands service coverage, ensuring that more individuals benefit from essential healthcare services.</p>
</section>
<section id="future-directions-and-opportunities" class="level2">
<h2 class="anchored" data-anchor-id="future-directions-and-opportunities">Future Directions and Opportunities</h2>
<p>The success of the SUMAR program’s automation opens up new possibilities for further innovation within the public healthcare sector. While the current focus is on generating tables and reports, there is potential for developing more interactive data visualizations and dashboards using tools like Shiny or R Markdown’s interactive capabilities. Such advancements could provide deeper insights and facilitate data-driven decision-making at various levels of government.</p>
<p>In conclusion, the use of R in automating the SUMAR program in Buenos Aires highlights the transformative potential of data science in public health. By optimizing administrative workflows and enhancing data traceability, this initiative not only improves cost recovery efforts but also sets a benchmark for other regions to follow. As we look to the future, the continued adoption of open-source tools in healthcare promises to drive innovation and sustainability, ultimately benefiting populations most in need.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Healthcare</category>
  <guid>https://r-consortium.org/posts/optimizing-public-healthcare-cost-recovery-with-r-a-use-case-from-argentina/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/optimizing-public-healthcare-cost-recovery-with-r-a-use-case-from-argentina/thumbnail-optimizing-buenos-aires.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Preprocessing Electronic Health Records for Analysis-Ready Data in an Asthma Cohort</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/preprocessing-electronic-health-records-for-analysis-ready-data-in-an-asthma-cohort/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/EytsZg9Rqkg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="pre-processing-electronic-health-records-for-asthma-cohort-analysis-with-r" class="level1">
<h1>Pre-processing Electronic Health Records for Asthma Cohort Analysis with R</h1>
<p>Electronic health record (EHR) data has emerged as a critical tool for conducting large-scale biomedical research. However, the complexity, inconsistencies, and potential inaccuracies within EHR data pose significant challenges for researchers. Kimberly Lactaoen, a staff scientist at the University of Pennsylvania, sheds light on these challenges and offers solutions through her presentation at R/Medicine 2025. By utilizing the tidyverse suite in R, she demonstrates how to transform perplexing EHR data into analysis-ready datasets, focusing on an asthma cohort study.</p>
<section id="understanding-the-intricacies-of-ehr-data" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-intricacies-of-ehr-data">Understanding the Intricacies of EHR Data</h2>
<p>EHR data encompasses vast amounts of patient information, providing a realistic representation of patient populations. Despite its potential, EHR data is riddled with complexities, including inconsistent data entries, diverse formats, and missing values, which can misrepresent a patient’s true health status. Lactaoen’s presentation focuses on the importance of understanding how this data is collected to streamline exploratory analyses and reduce scripting stages.</p>
<section id="key-challenges-in-ehr-data-cleaning" class="level3">
<h3 class="anchored" data-anchor-id="key-challenges-in-ehr-data-cleaning">Key Challenges in EHR Data Cleaning</h3>
<ol type="1">
<li><p><strong>Encounter-Level vs.&nbsp;Patient-Level Data</strong>: One of the initial hurdles is discerning between encounter-level and patient-level data. For instance, demographic details like sex, race, and ethnicity remain constant across encounters, while variables like insurance and BMI may vary. Lactaoen highlights the significance of understanding data collection methods to efficiently script for the most recent demographic entries using functions such as <code>as_date()</code> from the <code>lubridate</code> package.</p></li>
<li><p><strong>Conflicting Patient Information</strong>: EHR datasets often contain conflicting information, especially in demographic variables like race and ethnicity. Lactaoen’s approach involved using the <code>case_when()</code> function from <code>dplyr</code> to resolve conflicts, ensuring consistent and distinct race and ethnicity variables.</p></li>
<li><p><strong>Inconsistent Diagnostic Codes</strong>: Diagnostic code descriptions can vary, complicating the grouping of patients based on diagnoses. Lactaoen addresses this by joining diagnostic codes with descriptions from the Center for Medicare and Medicaid Services using <code>left_join()</code>, ensuring consistency across datasets.</p></li>
<li><p><strong>Medication Relevance for Asthma Treatment</strong>: Selecting relevant medications for asthma treatment from EHR data is another challenge. Lactaoen’s team collaborated with asthma specialists to filter relevant medications, leveraging Excel and the <code>map_dfr()</code> function from the <code>purrr</code> package to compile a comprehensive list for analysis.</p></li>
<li><p><strong>Laboratory Test Result Variability</strong>: Laboratory test results, such as eosinophil data, often feature varying units of measurement. Lactaoen utilized the <code>case_when()</code> function to standardize these units, though issues like missing unit information remain a work in progress.</p></li>
</ol>
</section>
</section>
<section id="strategies-for-effective-ehr-data-pre-processing" class="level2">
<h2 class="anchored" data-anchor-id="strategies-for-effective-ehr-data-pre-processing">Strategies for Effective EHR Data Pre-processing</h2>
<p>Lactaoen’s presentation offers several strategies for overcoming these challenges:</p>
<ul>
<li><strong>Understand Data Collection</strong>: Familiarizing oneself with the data collection process is crucial for reducing exploratory analysis and scripting efforts.</li>
<li><strong>Identify and Resolve Conflicts</strong>: Be vigilant about conflicting information, which may not accurately reflect a patient’s health status.</li>
<li><strong>Collaborate with Domain Experts</strong>: Engaging with specialists, such as asthma experts, ensures that the data used is relevant and accurate for the study.</li>
<li><strong>Simplify and Standardize Data</strong>: Wherever possible, simplify data entries and standardize units to facilitate easier analysis.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Lactaoen’s insights into EHR data pre-processing underscore the importance of meticulous data cleaning and transformation. By leveraging R’s tidyverse suite, researchers can effectively prepare EHR data for analysis, paving the way for impactful biomedical research. Her emphasis on understanding data collection, resolving conflicts, collaborating with experts, and standardizing data provides a robust framework for researchers embarking on EHR-based studies.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Healthcare</category>
  <category>Clinical Research</category>
  <guid>https://r-consortium.org/posts/preprocessing-electronic-health-records-for-analysis-ready-data-in-an-asthma-cohort/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/preprocessing-electronic-health-records-for-analysis-ready-data-in-an-asthma-cohort/thumbnail-reprocessing-ehs-lactaoen.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>rainbowR: A community for LGBTQ+ folks who code in R</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/rainbowr-a-community-for-lgbtq-folks-who-code-in-r/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/gLlRaqNfjys" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="embracing-inclusivity-in-r-the-journey-of-rainbowr" class="level1">
<h1>Embracing Inclusivity in R: The Journey of rainbowR</h1>
<p>In the ever-evolving world of data science and statistical computing, R has become a beloved language, not just for its robustness and versatility, but also for its vibrant and inclusive community. A shining example of this inclusivity is rainbowR, a community dedicated to connecting, supporting, and promoting LGBTQ+ individuals who code in R. Founded by Ella Kaye of the University of Warwick, rainbowR has transformed into a thriving hub for LGBTQ+ coders and allies alike, fostering a sense of belonging and camaraderie through innovative initiatives and data-driven activism.</p>
<section id="origin-and-growth-of-rainbowr" class="level2">
<h2 class="anchored" data-anchor-id="origin-and-growth-of-rainbowr">Origin and Growth of rainbowR</h2>
<p>The inception of rainbowR dates back to the useR! Conference in 2017, when Ella Kaye engaged in a conversation that highlighted the need for a dedicated LGBTQ+ space within the R community. Fast forward to today, and rainbowR has grown exponentially, boasting a membership of over 100 individuals. This growth is not just a testament to the need for such a community but also to the welcoming atmosphere and the meaningful connections it fosters.</p>
</section>
<section id="monthly-meetups-and-buddy-scheme" class="level2">
<h2 class="anchored" data-anchor-id="monthly-meetups-and-buddy-scheme">Monthly Meetups and Buddy Scheme</h2>
<p>Central to rainbowR’s mission are its monthly online meetups, typically held on the fourth Wednesday of each month. These gatherings provide a relaxed and supportive environment where participants can discuss R-related topics, share resources, and showcase their work. The focus on creating a friendly space encourages open dialogue and fosters learning among members.</p>
<p>Another key initiative is the buddy scheme, which aims to facilitate deeper connections within the community. Every three months, members can opt into the scheme, where they are randomly paired with another member. An R script processes these pairings and generates personalized emails to introduce the paired individuals. This innovative approach not only eases the anxiety of meeting new people but also enriches the community fabric by fostering one-on-one connections.</p>
</section>
<section id="data-driven-activism" class="level2">
<h2 class="anchored" data-anchor-id="data-driven-activism">Data-Driven Activism</h2>
<p>rainbowR’s commitment to raising awareness about LGBTQ+ issues is further exemplified through its GitHub repository, Tidy Rainbow. This repository hosts a collection of LGBTQ+ datasets, providing valuable resources for data visualization and analysis. These datasets can be utilized for educational purposes, blog posts, or simply to practice data skills, making them a valuable asset for both community members and allies.</p>
</section>
<section id="engaging-with-literature-the-rainbow-r-book-club" class="level2">
<h2 class="anchored" data-anchor-id="engaging-with-literature-the-rainbow-r-book-club">Engaging with Literature: The Rainbow R Book Club</h2>
<p>In addition to its technical initiatives, rainbowR has ventured into the literary world with its book club. The club recently completed its first session, where participants delved into “Queer Data: Using Gender, Sex, and Sexuality Data for Action” by Kevin Gian. The success of this book club highlights the community’s commitment to broadening its understanding of LGBTQ+ issues through various mediums. Future book club sessions are in the pipeline, promising more engaging discussions and insights.</p>
</section>
<section id="future-plans-and-the-role-of-allies" class="level2">
<h2 class="anchored" data-anchor-id="future-plans-and-the-role-of-allies">Future Plans and the Role of Allies</h2>
<p>As rainbowR looks to the future, exciting plans are underway to ensure the community’s sustainability and impact. Ella Kaye, now a fellow of the Software Sustainability Institute, is utilizing her fellowship to nurture and solidify rainbowR’s foundations. This includes establishing clear engagement pathways, developing governance policies, and potentially becoming a legal entity.</p>
<p>A flagship event in the pipeline is the inaugural Rainbow R Conference, designed to bring the community together to share knowledge and celebrate diversity. Allies play a crucial role in this journey, and their involvement is highly valued within rainbowR. The conference organizing committee welcomes allies, further emphasizing the inclusive nature of this vibrant community.</p>
</section>
<section id="call-for-collaboration-and-community-building" class="level2">
<h2 class="anchored" data-anchor-id="call-for-collaboration-and-community-building">Call for Collaboration and Community Building</h2>
<p>rainbowR’s mission extends beyond its own community. By sharing knowledge and experiences with other R or queer communities, rainbowR aims to build a network of support and shared learning. Community managers interested in exploring best practices for community building are encouraged to reach out to Ella Kaye and explore potential collaborations.</p>
</section>
<section id="join-the-movement" class="level2">
<h2 class="anchored" data-anchor-id="join-the-movement">Join the Movement</h2>
<p>For those interested in joining this vibrant community, rainbowR offers an open invitation to LGBTQ+ individuals and allies. By visiting <a href="https://rainbow.org">rainbow.org</a>, you can learn more about the community, sign up for newsletters, and become part of a supportive network that celebrates diversity and inclusivity in the R ecosystem.</p>
<p>rainbowR is more than just a community; it’s a movement towards a more inclusive and supportive future for all who code in R. As the community continues to grow and evolve, it stands as a testament to the power of connection and the impact of collective action in the world of data science.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>LGBTQ+</category>
  <guid>https://r-consortium.org/posts/rainbowr-a-community-for-lgbtq-folks-who-code-in-r/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/rainbowr-a-community-for-lgbtq-folks-who-code-in-r/thumbnail-rainbowr.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Refactor or Preserve? Challenging ‘If It Ain’t Broken, Don’t Fix It’ Mindset in Shiny App Lifecycle</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/refactor-or-preserve-challenging-if-it-aint-broken-dont-fix-it-mindset-in-shiny-app-lifecycle/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/9agO0o2gP68" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="refactor-or-preserve-challenging-the-if-it-aint-broken-dont-fix-it-mindset-in-shiny-app-lifecycle" class="level1">
<h1>Refactor or Preserve: Challenging the “If It Ain’t Broken, Don’t Fix It” Mindset in Shiny App Lifecycle</h1>
<p>In the ever-evolving landscape of technology, even well-crafted software solutions eventually face obsolescence. This reality brings forth a critical question for developers: to refactor or not? Dror Berel, an independent consultant with a diverse background in statistics, R, and the pharmaceutical industry, delves into this conundrum using the Pharmaverse/Teal framework as a case study. His insights shed light on the benefits and risks of transitioning a functional Shiny application to a modern framework.</p>
<section id="the-lifecycle-of-software-and-the-scope-creep-phenomenon" class="level2">
<h2 class="anchored" data-anchor-id="the-lifecycle-of-software-and-the-scope-creep-phenomenon">The Lifecycle of Software and the Scope Creep Phenomenon</h2>
<p>Throughout his career, Dror Berel has navigated through various programming environments—from SAS and S+ to R, becoming an early adopter of R web applications. His journey has led him through the intricate process of managing messy data and transforming it into meaningful insights. A recurring challenge in this process is the phenomenon of “scope creep,” where the project’s scope gradually expands beyond its original intent. This often results in unwieldy, spaghetti-like code that demands constant patching.</p>
<p>Dror defines scope creep as the uncontrolled or gradual expansion of project scope, adding features or requirements without corresponding adjustments in resources. This incremental approach can lead to inefficiencies and the need for a complete overhaul. Dror argues that instead of perpetually patching code, building a better solution from scratch can be a more sustainable approach.</p>
</section>
<section id="the-innovation-adoption-curve-in-r-frameworks" class="level2">
<h2 class="anchored" data-anchor-id="the-innovation-adoption-curve-in-r-frameworks">The Innovation Adoption Curve in R Frameworks</h2>
<p>Dror introduces the concept of the product adoption lifecycle, also known as the innovation adoption curve, to illustrate the evolution of R frameworks. Initially, solutions like R Base were created to address specific pain points. Over time, as these solutions gained popularity, they attracted early adopters, eventually reaching a broader audience. However, with the emergence of new challenges, frameworks like Tidyverse and Shiny were developed to address these issues.</p>
<p>Dror highlights the current landscape where modern frameworks, such as Teal and Rhino, are gaining traction, offering more advanced solutions for complex Shiny applications. He emphasizes the importance of understanding when to embrace these new frameworks and when to preserve existing solutions.</p>
</section>
<section id="the-case-for-refactoring-when-and-why" class="level2">
<h2 class="anchored" data-anchor-id="the-case-for-refactoring-when-and-why">The Case for Refactoring: When and Why?</h2>
<p>Refactoring is an integral part of the software lifecycle, but it requires careful consideration. Dror outlines scenarios when refactoring is justified:</p>
<ol type="1">
<li><p><strong>Deprecated Dependencies</strong>: When existing dependencies become outdated or unsupported, refactoring becomes necessary to ensure continued functionality.</p></li>
<li><p><strong>Scope Creep</strong>: If the current scope of the project exceeds its original limitations, refactoring can provide a more efficient and sustainable solution.</p></li>
<li><p><strong>Critical Pain Points</strong>: When the existing framework cannot adequately address a critical issue, adopting a new solution may be warranted.</p></li>
<li><p><strong>Trust in New Solutions</strong>: Refactoring should be pursued when there is confidence in the new solution’s longevity and support.</p></li>
</ol>
</section>
<section id="the-role-of-modern-frameworks-in-shiny-applications" class="level2">
<h2 class="anchored" data-anchor-id="the-role-of-modern-frameworks-in-shiny-applications">The Role of Modern Frameworks in Shiny Applications</h2>
<p>Dror delves into specific frameworks that have emerged to address the complexities of Shiny applications. He highlights the Pharmaverse open-source community’s ecosystem, particularly the Teal framework, which simplifies the development of Shiny apps by leveraging modules. This modular approach allows developers to focus on high-level objectives rather than getting bogged down in low-level details.</p>
<p>In addition to Teal, Dror mentions the Rhino framework, which provides recommendations for organizing files within an app, enhancing maintainability. Together with the Box package, these frameworks offer clear code quality, automation, and more, empowering developers to create robust Shiny applications.</p>
</section>
<section id="when-not-to-refactor-preserving-stability" class="level2">
<h2 class="anchored" data-anchor-id="when-not-to-refactor-preserving-stability">When Not to Refactor: Preserving Stability</h2>
<p>While refactoring offers numerous benefits, there are scenarios where it may not be necessary or advisable:</p>
<ol type="1">
<li><p><strong>Legacy Methodology</strong>: If replicating legacy methodologies is challenging and not well-understood, preservation may be the safer option.</p></li>
<li><p><strong>Overkill Features</strong>: Introducing features that do not significantly enhance functionality may lead to unnecessary complexity.</p></li>
<li><p><strong>Heavy Dependencies</strong>: Modern frameworks can introduce dependencies that may not align with project objectives, warranting a cautious approach.</p></li>
</ol>
</section>
<section id="conclusion-balancing-innovation-and-stability" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-balancing-innovation-and-stability">Conclusion: Balancing Innovation and Stability</h2>
<p>In conclusion, the decision to refactor or preserve an existing solution hinges on a variety of factors, including the project’s scope, the availability of modern frameworks, and the specific challenges faced. Dror Berel emphasizes the importance of being strategic and intentional in these decisions, weighing the risks and benefits to ensure long-term success.</p>
<p>For the R community, embracing new frameworks and technologies is essential for staying at the forefront of innovation. However, it is equally important to recognize when stability and preservation are the most prudent paths forward. By striking this balance, developers can ensure the longevity and effectiveness of their Shiny applications, ultimately delivering greater value to users and stakeholders.</p>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <guid>https://r-consortium.org/posts/refactor-or-preserve-challenging-if-it-aint-broken-dont-fix-it-mindset-in-shiny-app-lifecycle/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/refactor-or-preserve-challenging-if-it-aint-broken-dont-fix-it-mindset-in-shiny-app-lifecycle/thumbnail-refactoring-dror.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Retrospective clinical data harmonisation reporting using R and Quarto</title>
  <dc:creator>R Consortium</dc:creator>
  <link>https://r-consortium.org/posts/retrospective-clinical-data-harmonisation-reporting-using-r-and-quarto/</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0rUcKR4fylw" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="enhancing-clinical-data-harmonization-with-r-and-quarto" class="level1">
<h1>Enhancing Clinical Data Harmonization with R and Quarto</h1>
<p>Data harmonization has emerged as a crucial component in ensuring the integrity and utility of pooled data sets in clinical research. Jeremy Selva, a Research Officer at the National Heart Center Singapore, delivered a talk at R/Medicine 2025 on “Retrospective Clinical Data Harmonization Reporting.” His presentation delved into data harmonization, offering practical solutions and workflows for creating comprehensive reports using R and Quarto.</p>
<section id="the-challenge-of-data-harmonization" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge-of-data-harmonization">The Challenge of Data Harmonization</h2>
<p>Data harmonization is an indispensable part of the data cleaning process. It involves identifying similar variables across diverse data sets, grouping them based on generalized concepts, and transforming them into unified, harmonized variables for analysis. This process is particularly vital in clinical research, where data pooling from multiple sources can dramatically increase the statistical power to analyze rare outcomes.</p>
<p>However, as Jeremy highlighted, the path to effective data harmonization is fraught with challenges. Collaborators often send data in varying formats, making it difficult to match them against provided data dictionaries and input templates. This lack of uniformity necessitates a robust process to document and report the harmonization steps, serving as a bridge between data collaborators and the analysis team.</p>
</section>
<section id="the-importance-of-harmonization-reports" class="level2">
<h2 class="anchored" data-anchor-id="the-importance-of-harmonization-reports">The Importance of Harmonization Reports</h2>
<p>Jeremy emphasized the importance of transparency and accountability in data harmonization. A well-documented harmonization report can serve as a safeguard, providing clarity to collaborators and preempting potential issues that might arise from data mismanagement.</p>
<p>Despite the critical role of such reports, there is a dearth of resources providing detailed guidance on creating them, particularly using programming languages like R.</p>
</section>
<section id="leveraging-r-for-data-harmonization" class="level2">
<h2 class="anchored" data-anchor-id="leveraging-r-for-data-harmonization">Leveraging R for Data Harmonization</h2>
<p>In his quest to streamline the data harmonization process, Jeremy explored various R packages. Although packages like <code>retroharmonize</code>, <code>Rmonize</code> and <code>psHarmonize</code> offer some functionality, they come with limitations, such as handling categorical data better than continuous data or presenting complex harmonization processes in Excel, which can be cumbersome.</p>
<p>To address these gaps, Jeremy developed his own R project template for creating harmonization reports. This template draws inspiration from the <code>ourpackage</code> and <code>ourcompanion</code> packages, providing a structured layout for data storage, code management, and report generation.</p>
<section id="common-issues-and-solutions" class="level3">
<h3 class="anchored" data-anchor-id="common-issues-and-solutions">Common Issues and Solutions</h3>
<p>Jeremy shared some common pitfalls encountered during data harmonization and how they can be mitigated using R:</p>
<ol type="1">
<li><p><strong>Data Versioning Issues</strong>: Collaborators may send multiple versions of the same data set. Using the <code>readr</code> package and functions like <code>problems</code>, Jeremy demonstrated how to catch and address issues early in the data import process.</p></li>
<li><p><strong>Automating Checks</strong>: By employing packages like <code>testthat</code> and <code>pointblank</code>, Jeremy automated the validation of data inputs, ensuring robustness against changes in data versions.</p></li>
<li><p><strong>Variable Mapping and Validation</strong>: Jeremy outlined a workflow for mapping variables, validating mappings, and preparing data for merging. This included creating interactive tables for collaborators to review and using validation functions to ensure data integrity.</p></li>
</ol>
</section>
</section>
<section id="automating-report-generation-with-quarto" class="level2">
<h2 class="anchored" data-anchor-id="automating-report-generation-with-quarto">Automating Report Generation with Quarto</h2>
<p>To tackle the challenge of generating extensive harmonization reports, Jeremy turned to Quarto, a publishing system that allows for dynamic report generation. By creating a Quarto book project, he automated the creation of harmonization reports for each cohort, ensuring consistency and efficiency.</p>
<section id="creating-harmonization-reports" class="level3">
<h3 class="anchored" data-anchor-id="creating-harmonization-reports">Creating Harmonization Reports</h3>
<p>The process involves:</p>
<ul>
<li><strong>Index and Quarto Files</strong>: Setting up essential files like <code>index.qmd</code> and <code>quarto.yml</code> to control the book’s content and structure.</li>
<li><strong>Automating Scripts</strong>: Developing an R script to render Quarto documents for each cohort, facilitating the creation of both reference and how-to guide documents.</li>
</ul>
</section>
</section>
<section id="visualizing-harmonization-outcomes" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-harmonization-outcomes">Visualizing Harmonization Outcomes</h2>
<p>Jeremy also explored various visualization techniques to convey harmonization outcomes to collaborators and management. While traditional methods like Venn diagrams and Upset plots proved complex, he developed a heat map approach. This visualization provided a clear overview of cohort attributes, patient numbers, and variable availability—categorized by different colors and accompanied by legends for clarity.</p>
</section>
<section id="conclusion-and-future-directions" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-and-future-directions">Conclusion and Future Directions</h2>
<p>Jeremy’s presentation underscored the critical role of data harmonization in clinical research and the power of R and Quarto in streamlining this process. By creating a robust documentation and reporting framework, researchers can enhance transparency, accountability, and efficiency in their data harmonization efforts.</p>
<p>As Jeremy noted, while the current template offers a solid foundation, there is always room for improvement, such as unit testing and clearer documentation. His work serves as a valuable starting point for researchers facing similar challenges, fostering a more seamless integration of data across diverse clinical studies.</p>
<ul>
<li><a href="https://jauntyjjs.github.io/RMedicine2025_harmonisation/#/title-slide">Slides</a></li>
<li><a href="https://github.com/JauntyJJS/harmonisation/">Repo</a></li>
</ul>


</section>
</section>

 ]]></description>
  <category>R/Medicine 2025</category>
  <category>Software Development</category>
  <category>Clinical Research</category>
  <guid>https://r-consortium.org/posts/retrospective-clinical-data-harmonisation-reporting-using-r-and-quarto/</guid>
  <pubDate>Sun, 22 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://r-consortium.org/posts/retrospective-clinical-data-harmonisation-reporting-using-r-and-quarto/thumbnail-harmonisation-selva.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
